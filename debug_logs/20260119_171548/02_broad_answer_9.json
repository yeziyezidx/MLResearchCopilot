"{\"summary\": \"Answer generation from heterogeneous sources refers to the process of synthesizing accurate, fluent responses to user queries by leveraging multiple disparate types of information. In the context of product-related questions, these sources may include semi-structured attributes (e.g., feature tables), unstructured text descriptions, customer reviews, Q&A sections, and other user-generated content.  \\n\\nThe key challenge is that these sources differ in structure, style, and reliability. Effective systems must perform:  \\n1. **Evidence ranking** – Identifying the most relevant pieces of information from each source.  \\n2. **Source selection** – Choosing the optimal source(s) to answer a specific question.  \\n3. **Answer generation** – Producing natural-sounding, contextually appropriate answers from the selected evidence.  \\n\\nRecent research (Shen et al., Amazon Alexa AI; University of Cambridge) introduced a **large-scale benchmark** for product answer generation covering six heterogeneous sources. This benchmark provides annotated datasets for both evidence selection and answer generation, enabling standardized evaluation and model training.  \\n\\nBest practices identified include:  \\n- Training single models across all heterogeneous sources to achieve consistent confidence scoring.  \\n- Multi-source training, which improves performance even when sources have very different structures.  \\n- Using **pretrained Transformer-based models** (e.g., BERT, T5) for state-of-the-art performance.  \\n- Applying novel **data augmentation methods** that iteratively create synthetic training samples, achieving near-human performance with relatively few annotations.  \\n\\nThe research also provides an in-depth error analysis, highlighting persistent challenges such as handling conflicting information, generating answers from noisy or incomplete data, and adapting models to emerging product categories.\", \"problem\": null, \"key_concepts\": [\"- **Heterogeneous sources**: Different types of data (structured attributes, unstructured text, reviews, etc.) with varied formats and writing styles.\", \"- **Evidence ranking**: Determining the relevance of information within each source.\", \"- **Source selection**: Choosing the source(s) most likely to yield an accurate answer for a given query.\", \"- **Answer generation**: Producing a coherent, fluent, and contextually correct response from selected evidence.\", \"- **Benchmark datasets**: Standardized, annotated datasets enabling consistent evaluation and comparison of methods.\", \"- **Pretrained Transformer models**: Deep learning architectures like BERT, T5, and GPT that have achieved state-of-the-art performance in NLP tasks.\", \"- **Data augmentation**: Techniques to synthetically expand training datasets, improving model generalization.\"], \"recent_developments\": [\"- Release of a **large-scale benchmark** for product question answering from six heterogeneous sources.\", \"- Demonstrated effectiveness of **multi-source training** for improving accuracy and confidence calibration.\", \"- Introduction of **iterative data augmentation** methods to reduce reliance on large-scale human annotation.\", \"- Use of **pretrained Transformer-based architectures** in this domain, marking a shift from earlier single-source, shallow approaches.\", \"- Near-human performance achieved with relatively few training samples, indicating efficiency gains in model development.\"], \"authoritative_sources\": [\"- **Xiaoyu Shen, Gianni Barlacchi, Marco del Tredici, Weiwei Cheng, Adrià de Gispert, Bill Byrne** – Amazon Alexa AI and University of Cambridge.\", \"- **Published benchmark and best practices paper** – Available via Amazon Science, ACL Anthology, and OpenReview.\", \"- References to prior foundational work by Lai et al. (2018, 2020), Cui et al. (2017), Gao et al. (2019, 2021), McAuley and Yang (2016), Yu et al. (2018), Zhang et al. (2019, 2020).\", \"- Transformer model references: **Devlin et al. (2019)** for BERT, **Clark et al. (2020)** for ELECTRA.\"], \"search_results\": [{\"title\": \"https://assets.amazon.science/1e/26/8b0431dd425591086cfb6284c83f/product-answer-generation-from-heterogeneous-sources-a-new-benchmark-and-best-practices.pdf\", \"url\": \"https://assets.amazon.science/1e/26/8b0431dd425591086cfb6284c83f/product-answer-generation-from-heterogeneous-sources-a-new-benchmark-and-best-practices.pdf\", \"snippet\": \"<H1>Product Answer Generation from Heterogeneous Sources: A New Benchmark and Best Practices</H1><P>Xiaoyu Shen1, Gianni Barlacchi1, Marco del Tredici1, Weiwei Cheng1 Adrià de Gispert1 and Bill Byrne1,2</P><P>1Amazon Alexa AI 2Univeristy of Cambridge</P><P>{gyouu, gbarlac, mttredic, weiweic, agispert, willbyrn}@amazon.com</P><H3>Abstract</H3><P>It is of great value to answer product questions based on heterogeneous information sources available on web product pages, e.g., semi-structured attributes, text descriptions, user-provided contents, etc. However, these sources have different structures and writing styles, which poses challenges for (1) evidence rank-ing, (2) source selection, and (3) answer gen-eration. In this paper, we build a benchmark with annotations for both evidence selection and answer generation covering 6 information sources. Based on this benchmark, we con-duct a comprehensive study and present a set of best practices. We show that all sources are important and contribute to answering ques-tions. Handling all sources within one sin-gle model can produce comparable confidence scores across sources and combining multi-ple sources for training always helps, even for sources with totally different structures. We fur-ther propose a novel data augmentation method to iteratively create training samples for answer generation, which achieves close-to-human per-formance with only a few thousand annotations. Finally, we perform an in-depth error analysis of model predictions and highlight the chal-lenges for future research.</P><H3>1 Introduction</H3><P>Automatic answer generation for product-related questions is a hot topic in e-commerce applications. Previous approaches have leveraged information from sources like product specifications (Lai et al., 2018a, 2020), descriptions (Cui et al., 2017; Gao et al., 2019) or user reviews (McAuley and Yang, 2016; Yu et al., 2018; Zhang et al., 2019) to an-swer product questions. However, these works produce answers from only a single source. While a few works have utilized information from mul-tiple sources (Cui et al., 2017; Gao et al., 2019; Feng et al., 2021), they lack a reliable benchmark and have to resort to noisy labels or small-scaled human evaluation (Zhang et al., 2020; Gao et al.,</P><P>2021). Furthermore, almost none of them make use of pretrained Transformer-based models, which are the current state-of-the-art (SOTA) across NLP tasks (Devlin et al., 2019; Clark et al., 2020). In this work, we present a large-scale benchmark dataset for answering product questions from 6 het-erogeneous sources and study best practices to over-come three major challenges: (1) evidence ranking, which finds most relevant information from each of the heterogeneous sources; (2) source selection, which chooses the most appropriate data source to answer each question; and (3) answer generation, which produces a fluent, natural-sounding answer based on the relevant information. It is necessary since the sele\", \"source\": \"bing\"}, {\"title\": \"https://aclanthology.org/2022.ecnlp-1.13.pdf\", \"url\": \"https://aclanthology.org/2022.ecnlp-1.13.pdf\", \"snippet\": \"<H1>Product Answer Generation from Heterogeneous Sources: A New Benchmark and Best Practices</H1><P>Xiaoyu Shen1, Gianni Barlacchi1, Marco del Tredici1, Weiwei Cheng1 Adria de Gispert1 and Bill Birne1,2</P><H3>Abstract</H3><P>It is of great value to answer product questions based on heterogeneous information sources available on web product pages, e.g., semi- structured attributes, text descriptions, user- provided contents, etc. However, these sources have different structures and writing styles, which poses challenges for (1) evidence rank- ing, (2) source selection, and (3) answer gen- eration. In this paper, we build a benchmark with annotations for both evidence selection and answer generation covering 6 information sources. Based on this benchmark, we con- duct a comprehensive study and present a set of best practices. We show that all sources are important and contribute to answering ques- tions. Handling all sources within one sin- gle model can produce comparable confidence scores across sources and combining multi- ple sources for training always helps, even for sources with totally different structures. We fur- ther propose a novel data augmentation method to iteratively create training samples for answer generation, which achieves close-to-human per- formance with only a few thousand annotations. Finally, we perform an in-depth error analysis of model predictions and highlight the chal- lenges for future research.</P><H3>1 Introduction</H3><P>Automatic answer generation for product-related questions is a hot topic in e-commerce applications. Previous approaches have leveraged information from sources like product specifications (Lai et al., 2018a, 2020), descriptions (Cui et al., 2017; Gao et al., 2019) or user reviews (McAuley and Yang, 2016; Yu et al., 2018; Zhang et al., 2019) to an- swer product questions. However, these works produce answers from only a single source. While a few works have utilized information from mul- tiple sources (Cui et al., 2017; Gao et al., 2019; Feng et al., 2021), they lack a reliable benchmark and have to resort to noisy labels or small-scaled human evaluation (Zhang et al., 2020; Gao et al.,</P><P>2021). Furthermore, almost none of them make use of pretrained Transformer-based models, which are the current state-of-the-art (SOTA) across NLP tasks (Devlin et al., 2019; Clark et al., 2020).</P><P>In this work, we present a large-scale benchmark dataset for answering product questions from 6 het- erogeneous sources and study best practices to over- come three major challenges: (1) evidence ranking, which finds most relevant information from each of the heterogeneous sources; (2) source selection, which chooses the most appropriate data source to answer each question; and (3) answer generation, which produces a fluent, natural-sounding answer based on the relevant information. It is necessary since the selected relevant information may not be written to naturally answer a question, and there- fore not sui\", \"source\": \"bing\"}, {\"title\": \"https://openreview.net/pdf?id=PNUMO7p7SoC\", \"url\": \"https://openreview.net/pdf?id=PNUMO7p7SoC\", \"snippet\": \"<H1>Product Answer Generation from Heterogeneous Sources: A New Benchmark and Best Practices</H1><H3>Anonymous ACL submission</H3><P>001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027</P><P>028</P><P>029</P><P>030</P><P>031</P><P>032</P><P>033</P><P>034</P><P>035</P><P>036</P><P>037</P><P>038</P><P>039</P><P>040</P><P>041</P><P>042</P><H3>Abstract</H3><P>It is of great value to answer product questions based on heterogeneous information sources available on web product pages, e.g., semi-structured attributes, text descriptions, user-provided contents, etc. However, these sources have different structures and writing styles, which poses challenges for (1) evidence rank-ing, (2) source selection, and (3) answer gen-eration. In this paper, we build a benchmark with annotations for both evidence selection and answer generation covering 6 information sources. Based on this benchmark, we con-duct a comprehensive study and present a set of best practices. We show that all sources are important and contribute to answering ques-tions. Handling all sources within one sin-gle model can produce comparable confidence scores across sources and combining multi-ple sources for training always helps, even for sources with totally different structures. We fur-ther propose a novel data augmentation method to iteratively create training samples for answer generation, which achieves close-to-human per-formance with only a few thousand annotations. Finally, we perform an in-depth error analysis of model predictions and highlight the chal-lenges for future research.</P><H3>1 Introduction</H3><P>Automatic answer generation for product-related questions is a hot topic in e-commerce applications. Previous approaches have leveraged information from sources like product specifications (Lai et al., 2018a, 2020), descriptions (Cui et al., 2017; Gao et al., 2019) or user reviews (McAuley and Yang, 2016; Yu et al., 2018; Zhang et al., 2019) to an-swer product questions. However, these works produce answers from only a single source. While a few works have utilized information from mul-tiple sources (Cui et al., 2017; Gao et al., 2019; Feng et al., 2021), they lack a reliable benchmark and have to resort to noisy labels or small-scaled human evaluation (Zhang et al., 2020; Gao et al.,</P><P>2021). Furthermore, none of the above make use of pretrained Transformer-based models, which are the current state-of-the-art (SOTA) across NLP tasks (Devlin et al., 2019; Clark et al., 2020). In this work, we present a large-scale benchmark dataset for answering product questions from 6 het- erogeneous sources and study best practices to over- come three major challenges: (1) evidence ranking, which finds most relevant information from each of the heterogeneous sources; (2) source selection, which chooses the most appropriate data source to answer each question; and (3) answer generation, which produces a fluent, natural-sounding answe\", \"source\": \"bing\"}]}"