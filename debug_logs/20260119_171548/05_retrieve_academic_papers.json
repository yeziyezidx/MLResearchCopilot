"{\"deep-research 或者当前的AI联网搜索中，在拿到了目标网页之后，都是如何如何抽取有效信息组成答案的\": [{\"paper_id\": \"2507.17948v2\", \"title\": \"VERIRAG: A Post-Retrieval Auditing of Scientific Study Summaries\", \"authors\": [\"Shubham Mohole\", \"Hongjun Choi\", \"Shusen Liu\", \"Christine Klymko\", \"Shashank Kushwaha\", \"Derek Shi\", \"Wesam Sakla\", \"Sainyam Galhotra\", \"Ruben Glatt\"], \"abstract\": \"Can democratized information gatekeepers and community note writers effectively decide what scientific information to amplify? Lacking domain expertise, such gatekeepers rely on automated reasoning agents that use RAG to ground evidence to cited sources. But such standard RAG systems validate summaries via semantic grounding and suffer from \\\"methodological blindness,\\\" treating all cited evidence as equally valid regardless of rigor. To address this, we introduce VERIRAG, a post-retrieval auditing framework that shifts the task from classification to methodological vulnerability detection. Using private Small Language Models (SLMs), VERIRAG audits source papers against the Veritable taxonomy of statistical rigor. We contribute: (1) a benchmark of 1,730 summaries with realistic, non-obvious perturbations modeled after retracted papers; (2) the auditable Veritable taxonomy; and (3) an operational system that improves Macro F1 by at least 19 points over baselines using GPT-based SLMs, a result that replicates across MISTRAL and Gemma architectures. Given the complexity of detecting non-obvious flaws, we view VERIRAG as a \\\"vulnerability-detection copilot,\\\" providing structured audit trails for human editors. In our experiments, individual human testers found over 80% of the generated audit trails useful for decision-making. We plan to release the dataset and code to support responsible science advocacy.\", \"url\": \"https://arxiv.org/pdf/2507.17948v2\", \"source\": \"arxiv\", \"published_date\": \"2025-07-23T21:32:50Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2509.06472v2\", \"title\": \"Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for Dynamic Retrieval and Reranking\", \"authors\": [\"Haoxiang Jin\", \"Ronghan Li\", \"Zixiang Lu\", \"Qiguang Miao\"], \"abstract\": \"Large Language Models (LLMs) often generate inaccurate responses (hallucinations) when faced with questions beyond their knowledge scope. Retrieval-Augmented Generation (RAG) addresses this by leveraging external knowledge, but a critical challenge remains: determining whether retrieved contexts effectively enhance the model`s ability to answer specific queries. This challenge underscores the importance of knowledge boundary awareness, which current methods-relying on discrete labels or limited signals-fail to address adequately, as they overlook the rich information in LLMs` continuous internal hidden states. To tackle this, we propose a novel post-retrieval knowledge filtering approach. First, we construct a confidence detection model based on LLMs` internal hidden states to quantify how retrieved contexts enhance the model`s confidence. Using this model, we build a preference dataset (NQ_Rerank) to fine-tune a reranker, enabling it to prioritize contexts preferred by the downstream LLM during reranking. Additionally, we introduce Confidence-Based Dynamic Retrieval (CBDR), which adaptively triggers retrieval based on the LLM`s initial confidence in the original question, reducing knowledge conflicts and improving efficiency. Experimental results demonstrate significant improvements in accuracy for context screening and end-to-end RAG performance, along with a notable reduction in retrieval costs while maintaining competitive accuracy.\", \"url\": \"https://arxiv.org/pdf/2509.06472v2\", \"source\": \"arxiv\", \"published_date\": \"2025-09-08T09:37:20Z\", \"score_bm25\": 0.0}, {\"paper_id\": null, \"title\": \"Evidence for a dark matter particle\", \"authors\": [\"Yukio Tomozawa\"], \"abstract\": \"A prediction and observational evidence for the mass of a dark matter particle are presented..\", \"url\": \"https://arxiv.org/pdf/1002.1938v1\", \"source\": \"arxiv\", \"published_date\": \"2010-02-09T18:37:48Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2511.17118v1\", \"title\": \"Constant-Size Cryptographic Evidence Structures for Regulated AI Workflows\", \"authors\": [\"Leo Kao\"], \"abstract\": \"This paper introduces constant-size cryptographic evidence structures, a general abstraction for representing verifiable audit evidence for AI workflows in regulated environments. Each evidence item is a fixed-size tuple of cryptographic fields, designed to (i) provide strong binding to workflow events and configurations, (ii) support constant-size storage and uniform verification cost per event, and (iii) compose cleanly with hash-chain and Merkle-based audit constructions. We formalize a simple model of regulated AI workflows, define syntax and algorithms for evidence structures, and articulate security goals such as audit integrity and non-equivocation. We present a generic hash-and-sign construction that instantiates this abstraction using a collision-resistant hash function and a standard digital signature scheme. We then show how to integrate the construction with hash-chained logs, Merkle-tree anchoring, and optionally trusted execution environments, and we analyze the asymptotic complexity of evidence generation and verification. Finally, we implement a prototype library and report microbenchmark results on commodity hardware, demonstrating that the per-event overhead of constant-size evidence is small and predictable. The design is informed by industrial experience with regulated AI systems at Codebat Technologies Inc., while the paper focuses on the abstraction, algorithms, and their security and performance characteristics, with implications for clinical trial management, pharmaceutical compliance, and medical AI governance.\", \"url\": \"https://arxiv.org/pdf/2511.17118v1\", \"source\": \"arxiv\", \"published_date\": \"2025-11-21T10:28:07Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2509.08381v1\", \"title\": \"Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model\", \"authors\": [\"Yu Cheng Chih\", \"Yong Hao Hou\"], \"abstract\": \"Deploying large language models (LLMs) for structured data extraction in domains such as financial compliance reporting, legal document analytics, and multilingual knowledge base construction is often impractical for smaller teams due to the high cost of running large architectures and the difficulty of preparing large, high-quality datasets. Most recent instruction-tuning studies focus on seven-billion-parameter or larger models, leaving limited evidence on whether much smaller models can work reliably under low-resource, multi-task conditions. This work presents ETLCH, a billion-parameter LLaMA-based model fine-tuned with low-rank adaptation on only a few hundred to one thousand samples per task for JSON extraction, knowledge graph extraction, and named entity recognition. Despite its small scale, ETLCH outperforms strong baselines across most evaluation metrics, with substantial gains observed even at the lowest data scale. These findings demonstrate that well-tuned small models can deliver stable and accurate structured outputs at a fraction of the computational cost, enabling cost-effective and reliable information extraction pipelines in resource-constrained environments.\", \"url\": \"https://arxiv.org/pdf/2509.08381v1\", \"source\": \"arxiv\", \"published_date\": \"2025-09-10T08:19:07Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"1605.03033v1\", \"title\": \"Effect of Electrical Filtering on Level Dependent ASE Noise\", \"authors\": [\"Jeremy Witzens\"], \"abstract\": \"We derive an analytical model describing the effect of filtering on amplified spontaneous emission noise during or after opto-electronic conversion. In particular, we show that electrical filtering results in a further reduction of the signal quality factor associated with an effective increase of the noise levels and can lead to counter-intuitive dependencies of the measured signal quality on the characteristics of the test setup. Closed form equations are compared with numerical models and experiments, showing excellent agreement.\", \"url\": \"https://arxiv.org/pdf/1605.03033v1\", \"source\": \"arxiv\", \"published_date\": \"2016-05-10T14:31:02Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2512.20781v1\", \"title\": \"Soft Filtering: Guiding Zero-shot Composed Image Retrieval with Prescriptive and Proscriptive Constraints\", \"authors\": [\"Youjin Jung\", \"Seongwoo Cho\", \"Hyun-seok Min\", \"Sungchul Choi\"], \"abstract\": \"Composed Image Retrieval (CIR) aims to find a target image that aligns with user intent, expressed through a reference image and a modification text. While Zero-shot CIR (ZS-CIR) methods sidestep the need for labeled training data by leveraging pretrained vision-language models, they often rely on a single fused query that merges all descriptive cues of what the user wants, tending to dilute key information and failing to account for what they wish to avoid. Moreover, current CIR benchmarks assume a single correct target per query, overlooking the ambiguity in modification texts. To address these challenges, we propose Soft Filtering with Textual constraints (SoFT), a training-free, plug-and-play filtering module for ZS-CIR. SoFT leverages multimodal large language models (LLMs) to extract two complementary constraints from the reference-modification pair: prescriptive (must-have) and proscriptive (must-avoid) constraints. These serve as semantic filters that reward or penalize candidate images to re-rank results, without modifying the base retrieval model or adding supervision. In addition, we construct a two-stage dataset pipeline that refines CIR benchmarks. We first identify multiple plausible targets per query to construct multi-target triplets, capturing the open-ended nature of user intent. Then guide multimodal LLMs to rewrite the modification text to focus on one target, while referencing contrastive distractors to ensure precision. This enables more comprehensive and reliable evaluation under varying ambiguity levels. Applied on top of CIReVL, a ZS-CIR retriever, SoFT raises R@5 to 65.25 on CIRR (+12.94), mAP@50 to 27.93 on CIRCO (+6.13), and R@50 to 58.44 on FashionIQ (+4.59), demonstrating broad effectiveness.\", \"url\": \"https://arxiv.org/pdf/2512.20781v1\", \"source\": \"arxiv\", \"published_date\": \"2025-12-23T21:29:45Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"1601.04605v1\", \"title\": \"Dynamic Information Retrieval: Theoretical Framework and Application\", \"authors\": [\"Marc Sloan\", \"Jun Wang\"], \"abstract\": \"Theoretical frameworks like the Probability Ranking Principle and its more recent Interactive Information Retrieval variant have guided the development of ranking and retrieval algorithms for decades, yet they are not capable of helping us model problems in Dynamic Information Retrieval which exhibit the following three properties; an observable user signal, retrieval over multiple stages and an overall search intent. In this paper a new theoretical framework for retrieval in these scenarios is proposed. We derive a general dynamic utility function for optimizing over these types of tasks, that takes into account the utility of each stage and the probability of observing user feedback. We apply our framework to experiments over TREC data in the dynamic multi page search scenario as a practical demonstration of its effectiveness and to frame the discussion of its use, its limitations and to compare it against the existing frameworks.\", \"url\": \"https://arxiv.org/pdf/1601.04605v1\", \"source\": \"arxiv\", \"published_date\": \"2016-01-18T17:01:34Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2102.11903v2\", \"title\": \"Neural ranking models for document retrieval\", \"authors\": [\"Mohamed Trabelsi\", \"Zhiyu Chen\", \"Brian D. Davison\", \"Jeff Heflin\"], \"abstract\": \"Ranking models are the main components of information retrieval systems. Several approaches to ranking are based on traditional machine learning algorithms using a set of hand-crafted features. Recently, researchers have leveraged deep learning models in information retrieval. These models are trained end-to-end to extract features from the raw data for ranking tasks, so that they overcome the limitations of hand-crafted features. A variety of deep learning models have been proposed, and each model presents a set of neural network components to extract features that are used for ranking. In this paper, we compare the proposed models in the literature along different dimensions in order to understand the major contributions and limitations of each model. In our discussion of the literature, we analyze the promising neural components, and propose future research directions. We also show the analogy between document retrieval and other retrieval tasks where the items to be ranked are structured documents, answers, images and videos.\", \"url\": \"https://arxiv.org/pdf/2102.11903v2\", \"source\": \"arxiv\", \"published_date\": \"2021-02-23T19:30:37Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2503.01763v2\", \"title\": \"Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models\", \"authors\": [\"Zhengliang Shi\", \"Yuhan Wang\", \"Lingyong Yan\", \"Pengjie Ren\", \"Shuaiqiang Wang\", \"Dawei Yin\", \"Zhaochun Ren\"], \"abstract\": \"Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial step. However, the performance of IR models in tool retrieval tasks remains underexplored and unclear. Most tool-use benchmarks simplify this step by manually pre-annotating a small set of relevant tools for each task, which is far from the real-world scenarios. In this paper, we propose ToolRet, a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets. We benchmark six types of models on ToolRet. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on ToolRet. This low retrieval quality degrades the task pass rate of tool-use LLMs. As a further step, we contribute a large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models.\", \"url\": \"https://arxiv.org/pdf/2503.01763v2\", \"source\": \"arxiv\", \"published_date\": \"2025-03-03T17:37:16Z\", \"score_bm25\": 0.0}]}"