"{\"post-retrieval evidence extraction\": [{\"paper_id\": \"2507.17948v2\", \"title\": \"VERIRAG: A Post-Retrieval Auditing of Scientific Study Summaries\", \"authors\": [\"Shubham Mohole\", \"Hongjun Choi\", \"Shusen Liu\", \"Christine Klymko\", \"Shashank Kushwaha\", \"Derek Shi\", \"Wesam Sakla\", \"Sainyam Galhotra\", \"Ruben Glatt\"], \"abstract\": \"Can democratized information gatekeepers and community note writers effectively decide what scientific information to amplify? Lacking domain expertise, such gatekeepers rely on automated reasoning agents that use RAG to ground evidence to cited sources. But such standard RAG systems validate summaries via semantic grounding and suffer from \\\"methodological blindness,\\\" treating all cited evidence as equally valid regardless of rigor. To address this, we introduce VERIRAG, a post-retrieval auditing framework that shifts the task from classification to methodological vulnerability detection. Using private Small Language Models (SLMs), VERIRAG audits source papers against the Veritable taxonomy of statistical rigor. We contribute: (1) a benchmark of 1,730 summaries with realistic, non-obvious perturbations modeled after retracted papers; (2) the auditable Veritable taxonomy; and (3) an operational system that improves Macro F1 by at least 19 points over baselines using GPT-based SLMs, a result that replicates across MISTRAL and Gemma architectures. Given the complexity of detecting non-obvious flaws, we view VERIRAG as a \\\"vulnerability-detection copilot,\\\" providing structured audit trails for human editors. In our experiments, individual human testers found over 80% of the generated audit trails useful for decision-making. We plan to release the dataset and code to support responsible science advocacy.\", \"url\": \"https://arxiv.org/pdf/2507.17948v2\", \"source\": \"arxiv\", \"published_date\": \"2025-07-23T21:32:50Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2509.06472v2\", \"title\": \"Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for Dynamic Retrieval and Reranking\", \"authors\": [\"Haoxiang Jin\", \"Ronghan Li\", \"Zixiang Lu\", \"Qiguang Miao\"], \"abstract\": \"Large Language Models (LLMs) often generate inaccurate responses (hallucinations) when faced with questions beyond their knowledge scope. Retrieval-Augmented Generation (RAG) addresses this by leveraging external knowledge, but a critical challenge remains: determining whether retrieved contexts effectively enhance the model`s ability to answer specific queries. This challenge underscores the importance of knowledge boundary awareness, which current methods-relying on discrete labels or limited signals-fail to address adequately, as they overlook the rich information in LLMs` continuous internal hidden states. To tackle this, we propose a novel post-retrieval knowledge filtering approach. First, we construct a confidence detection model based on LLMs` internal hidden states to quantify how retrieved contexts enhance the model`s confidence. Using this model, we build a preference dataset (NQ_Rerank) to fine-tune a reranker, enabling it to prioritize contexts preferred by the downstream LLM during reranking. Additionally, we introduce Confidence-Based Dynamic Retrieval (CBDR), which adaptively triggers retrieval based on the LLM`s initial confidence in the original question, reducing knowledge conflicts and improving efficiency. Experimental results demonstrate significant improvements in accuracy for context screening and end-to-end RAG performance, along with a notable reduction in retrieval costs while maintaining competitive accuracy.\", \"url\": \"https://arxiv.org/pdf/2509.06472v2\", \"source\": \"arxiv\", \"published_date\": \"2025-09-08T09:37:20Z\", \"score_bm25\": 0.0}, {\"paper_id\": null, \"title\": \"Evidence for a dark matter particle\", \"authors\": [\"Yukio Tomozawa\"], \"abstract\": \"A prediction and observational evidence for the mass of a dark matter particle are presented..\", \"url\": \"https://arxiv.org/pdf/1002.1938v1\", \"source\": \"arxiv\", \"published_date\": \"2010-02-09T18:37:48Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2511.17118v1\", \"title\": \"Constant-Size Cryptographic Evidence Structures for Regulated AI Workflows\", \"authors\": [\"Leo Kao\"], \"abstract\": \"This paper introduces constant-size cryptographic evidence structures, a general abstraction for representing verifiable audit evidence for AI workflows in regulated environments. Each evidence item is a fixed-size tuple of cryptographic fields, designed to (i) provide strong binding to workflow events and configurations, (ii) support constant-size storage and uniform verification cost per event, and (iii) compose cleanly with hash-chain and Merkle-based audit constructions. We formalize a simple model of regulated AI workflows, define syntax and algorithms for evidence structures, and articulate security goals such as audit integrity and non-equivocation. We present a generic hash-and-sign construction that instantiates this abstraction using a collision-resistant hash function and a standard digital signature scheme. We then show how to integrate the construction with hash-chained logs, Merkle-tree anchoring, and optionally trusted execution environments, and we analyze the asymptotic complexity of evidence generation and verification. Finally, we implement a prototype library and report microbenchmark results on commodity hardware, demonstrating that the per-event overhead of constant-size evidence is small and predictable. The design is informed by industrial experience with regulated AI systems at Codebat Technologies Inc., while the paper focuses on the abstraction, algorithms, and their security and performance characteristics, with implications for clinical trial management, pharmaceutical compliance, and medical AI governance.\", \"url\": \"https://arxiv.org/pdf/2511.17118v1\", \"source\": \"arxiv\", \"published_date\": \"2025-11-21T10:28:07Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2509.08381v1\", \"title\": \"Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model\", \"authors\": [\"Yu Cheng Chih\", \"Yong Hao Hou\"], \"abstract\": \"Deploying large language models (LLMs) for structured data extraction in domains such as financial compliance reporting, legal document analytics, and multilingual knowledge base construction is often impractical for smaller teams due to the high cost of running large architectures and the difficulty of preparing large, high-quality datasets. Most recent instruction-tuning studies focus on seven-billion-parameter or larger models, leaving limited evidence on whether much smaller models can work reliably under low-resource, multi-task conditions. This work presents ETLCH, a billion-parameter LLaMA-based model fine-tuned with low-rank adaptation on only a few hundred to one thousand samples per task for JSON extraction, knowledge graph extraction, and named entity recognition. Despite its small scale, ETLCH outperforms strong baselines across most evaluation metrics, with substantial gains observed even at the lowest data scale. These findings demonstrate that well-tuned small models can deliver stable and accurate structured outputs at a fraction of the computational cost, enabling cost-effective and reliable information extraction pipelines in resource-constrained environments.\", \"url\": \"https://arxiv.org/pdf/2509.08381v1\", \"source\": \"arxiv\", \"published_date\": \"2025-09-10T08:19:07Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2405.17533v1\", \"title\": \"PAE: LLM-based Product Attribute Extraction for E-Commerce Fashion Trends\", \"authors\": [\"Apurva Sinha\", \"Ekta Gujral\"], \"abstract\": \"Product attribute extraction is an growing field in e-commerce business, with several applications including product ranking, product recommendation, future assortment planning and improving online shopping customer experiences. Understanding the customer needs is critical part of online business, specifically fashion products. Retailers uses assortment planning to determine the mix of products to offer in each store and channel, stay responsive to market dynamics and to manage inventory and catalogs. The goal is to offer the right styles, in the right sizes and colors, through the right channels. When shoppers find products that meet their needs and desires, they are more likely to return for future purchases, fostering customer loyalty. Product attributes are a key factor in assortment planning. In this paper we present PAE, a product attribute extraction algorithm for future trend reports consisting text and images in PDF format. Most existing methods focus on attribute extraction from titles or product descriptions or utilize visual information from existing product images. Compared to the prior works, our work focuses on attribute extraction from PDF files where upcoming fashion trends are explained. This work proposes a more comprehensive framework that fully utilizes the different modalities for attribute extraction and help retailers to plan the assortment in advance. Our contributions are three-fold: (a) We develop PAE, an efficient framework to extract attributes from unstructured data (text and images); (b) We provide catalog matching methodology based on BERT representations to discover the existing attributes using upcoming attribute values; (c) We conduct extensive experiments with several baselines and show that PAE is an effective, flexible and on par or superior (avg 92.5% F1-Score) framework to existing state-of-the-art for attribute value extraction task.\", \"url\": \"https://arxiv.org/pdf/2405.17533v1\", \"source\": \"arxiv\", \"published_date\": \"2024-05-27T17:50:25Z\", \"score_bm25\": 1.0893095234545045}, {\"paper_id\": \"2103.16929v1\", \"title\": \"Deep Neural Approaches to Relation Triplets Extraction: A Comprehensive Survey\", \"authors\": [\"Tapas Nayak\", \"Navonil Majumder\", \"Pawan Goyal\", \"Soujanya Poria\"], \"abstract\": \"Recently, with the advances made in continuous representation of words (word embeddings) and deep neural architectures, many research works are published in the area of relation extraction and it is very difficult to keep track of so many papers. To help future research, we present a comprehensive review of the recently published research works in relation extraction. We mostly focus on relation extraction using deep neural networks which have achieved state-of-the-art performance on publicly available datasets. In this survey, we cover sentence-level relation extraction to document-level relation extraction, pipeline-based approaches to joint extraction approaches, annotated datasets to distantly supervised datasets along with few very recent research directions such as zero-shot or few-shot relation extraction, noise mitigation in distantly supervised datasets. Regarding neural architectures, we cover convolutional models, recurrent network models, attention network models, and graph convolutional models in this survey.\", \"url\": \"https://arxiv.org/pdf/2103.16929v1\", \"source\": \"arxiv\", \"published_date\": \"2021-03-31T09:27:15Z\", \"score_bm25\": 1.074893948380721}, {\"paper_id\": \"2303.10659v2\", \"title\": \"COVID-19 event extraction from Twitter via extractive question answering with continuous prompts\", \"authors\": [\"Yuhang Jiang\", \"Ramakanth Kavuluru\"], \"abstract\": \"As COVID-19 ravages the world, social media analytics could augment traditional surveys in assessing how the pandemic evolves and capturing consumer chatter that could help healthcare agencies in addressing it. This typically involves mining disclosure events that mention testing positive for the disease or discussions surrounding perceptions and beliefs in preventative or treatment options. The 2020 shared task on COVID-19 event extraction (conducted as part of the W-NUT workshop during the EMNLP conference) introduced a new Twitter dataset for benchmarking event extraction from COVID-19 tweets. In this paper, we cast the problem of event extraction as extractive question answering using recent advances in continuous prompting in language models. On the shared task test dataset, our approach leads to over 5% absolute micro-averaged F1-score improvement over prior best results, across all COVID-19 event slots. Our ablation study shows that continuous prompts have a major impact on the eventual performance.\", \"url\": \"https://arxiv.org/pdf/2303.10659v2\", \"source\": \"arxiv\", \"published_date\": \"2023-03-19T13:47:56Z\", \"score_bm25\": 1.0613144347150463}, {\"paper_id\": \"2012.08105v2\", \"title\": \"Schema Extraction on Semi-structured Data\", \"authors\": [\"Panpan Li\", \"Yikun Gong\", \"Chen Wang\"], \"abstract\": \"With the continuous development of NoSQL databases, more and more developers choose to use semi-structured data for development and data management, which puts forward requirements for schema management of semi-structured data stored in NoSQL databases. Schema extraction plays an important role in understanding schemas, optimizing queries, and validating data consistency. Therefore, in this survey we investigate structural methods based on tree and graph and statistical methods based on distributed architecture and machine learning to extract schemas. The schemas obtained by the structural methods are more interpretable, and the statistical methods have better applicability and generalization ability. Moreover, we also investigate tools and systems for schemas extraction. Schema extraction tools are mainly used for spark or NoSQL databases, and are suitable for small datasets or simple application environments. The system mainly focuses on the extraction and management of schemas in large data sets and complex application scenarios. Furthermore, we also compare these techniques to facilitate data managers' choice.\", \"url\": \"https://arxiv.org/pdf/2012.08105v2\", \"source\": \"arxiv\", \"published_date\": \"2020-12-15T05:57:41Z\", \"score_bm25\": 1.0586396013595618}, {\"paper_id\": \"2207.01888v2\", \"title\": \"Keyword Extraction in Scientific Documents\", \"authors\": [\"Susie Xi Rao\", \"Piriyakorn Piriyatamwong\", \"Parijat Ghoshal\", \"Sara Nasirian\", \"Emmanuel de Salis\", \"Sandra MitroviÄ‡\", \"Michael Wechner\", \"Vanya Brucker\", \"Peter Egger\", \"Ce Zhang\"], \"abstract\": \"The scientific publication output grows exponentially. Therefore, it is increasingly challenging to keep track of trends and changes. Understanding scientific documents is an important step in downstream tasks such as knowledge graph building, text mining, and discipline classification. In this workshop, we provide a better understanding of keyword and keyphrase extraction from the abstract of scientific publications.\", \"url\": \"https://arxiv.org/pdf/2207.01888v2\", \"source\": \"arxiv\", \"published_date\": \"2022-07-05T08:33:47Z\", \"score_bm25\": 1.037545838306893}, {\"paper_id\": \"2505.17125v1\", \"title\": \"NEXT-EVAL: Next Evaluation of Traditional and LLM Web Data Record Extraction\", \"authors\": [\"Soyeon Kim\", \"Namhee Kim\", \"Yeonwoo Jeong\"], \"abstract\": \"Effective evaluation of web data record extraction methods is crucial, yet hampered by static, domain-specific benchmarks and opaque scoring practices. This makes fair comparison between traditional algorithmic techniques, which rely on structural heuristics, and Large Language Model (LLM)-based approaches, offering zero-shot extraction across diverse layouts, particularly challenging. To overcome these limitations, we introduce a concrete evaluation framework. Our framework systematically generates evaluation datasets from arbitrary MHTML snapshots, annotates XPath-based supervision labels, and employs structure-aware metrics for consistent scoring, specifically preventing text hallucination and allowing only for the assessment of positional hallucination. It also incorporates preprocessing strategies to optimize input for LLMs while preserving DOM semantics: HTML slimming, Hierarchical JSON, and Flat JSON. Additionally, we created a publicly available synthetic dataset by transforming DOM structures and modifying content. We benchmark deterministic heuristic algorithms and off-the-shelf LLMs across these multiple input formats. Our benchmarking shows that Flat JSON input enables LLMs to achieve superior extraction accuracy (F1 score of 0.9567) and minimal hallucination compared to other input formats like Slimmed HTML and Hierarchical JSON. We establish a standardized foundation for rigorous benchmarking, paving the way for the next principled advancements in web data record extraction.\", \"url\": \"https://arxiv.org/pdf/2505.17125v1\", \"source\": \"arxiv\", \"published_date\": \"2025-05-21T21:03:37Z\", \"score_bm25\": 1.0054260682061158}, {\"paper_id\": null, \"title\": \"Label-dependent Feature Extraction in Social Networks for Node Classification\", \"authors\": [\"Tomasz Kajdanowicz\", \"Przemyslaw Kazienko\", \"Piotr Doskocz\"], \"abstract\": \"A new method of feature extraction in the social network for within-network classification is proposed in the paper. The method provides new features calculated by combination of both: network structure information and class labels assigned to nodes. The influence of various features on classification performance has also been studied. The experiments on real-world data have shown that features created owing to the proposed method can lead to significant improvement of classification accuracy.\", \"url\": \"https://arxiv.org/pdf/1303.0095v1\", \"source\": \"arxiv\", \"published_date\": \"2013-03-01T06:31:02Z\", \"score_bm25\": 0.9910349752149622}, {\"paper_id\": \"2505.06284v1\", \"title\": \"DMRL: Data- and Model-aware Reward Learning for Data Extraction\", \"authors\": [\"Zhiqiang Wang\", \"Ruoxi Cheng\"], \"abstract\": \"Large language models (LLMs) are inherently vulnerable to unintended privacy breaches. Consequently, systematic red-teaming research is essential for developing robust defense mechanisms. However, current data extraction methods suffer from several limitations: (1) rely on dataset duplicates (addressable via deduplication), (2) depend on prompt engineering (now countered by detection and defense), and (3) rely on random-search adversarial generation. To address these challenges, we propose DMRL, a Data- and Model-aware Reward Learning approach for data extraction. This technique leverages inverse reinforcement learning to extract sensitive data from LLMs. Our method consists of two main components: (1) constructing an introspective reasoning dataset that captures leakage mindsets to guide model behavior, and (2) training reward models with Group Relative Policy Optimization (GRPO), dynamically tuning optimization based on task difficulty at both the data and model levels. Comprehensive experiments across various LLMs demonstrate that DMRL outperforms all baseline methods in data extraction performance.\", \"url\": \"https://arxiv.org/pdf/2505.06284v1\", \"source\": \"arxiv\", \"published_date\": \"2025-05-07T07:21:37Z\", \"score_bm25\": 0.9780611653873008}, {\"paper_id\": \"1904.09765v1\", \"title\": \"hf0: A hybrid pitch extraction method for multimodal voice\", \"authors\": [\"Pradeep Rengaswamy\", \"Gurunath Reddy M\", \"Krothapalli Sreenivasa Rao\"], \"abstract\": \"Pitch or fundamental frequency (f0) extraction is a fundamental problem studied extensively for its potential applications in speech and clinical applications. In literature, explicit mode specific (modal speech or singing voice or emotional/ expressive speech or noisy speech) signal processing and deep learning f0 extraction methods that exploit the quasi periodic nature of the signal in time, harmonic property in spectral or combined form to extract the pitch is developed. Hence, there is no single unified method which can reliably extract the pitch from various modes of the acoustic signal. In this work, we propose a hybrid f0 extraction method which seamlessly extracts the pitch across modes of speech production with very high accuracy required for many applications. The proposed hybrid model exploits the advantages of deep learning and signal processing methods to minimize the pitch detection error and adopts to various modes of acoustic signal. Specifically, we propose an ordinal regression convolutional neural networks to map the periodicity rich input representation to obtain the nominal pitch classes which drastically reduces the number of classes required for pitch detection unlike other deep learning approaches. Further, the accurate f0 is estimated from the nominal pitch class labels by filtering and autocorrelation. We show that the proposed method generalizes to the unseen modes of voice production and various noises for large scale datasets. Also, the proposed hybrid model significantly reduces the learning parameters required to train the deep model compared to other methods. Furthermore,the evaluation measures showed that the proposed method is significantly better than the state-of-the-art signal processing and deep learning approaches.\", \"url\": \"https://arxiv.org/pdf/1904.09765v1\", \"source\": \"arxiv\", \"published_date\": \"2019-04-22T08:08:12Z\", \"score_bm25\": 0.9318220400431342}, {\"paper_id\": \"2411.00469v1\", \"title\": \"MIRFLEX: Music Information Retrieval Feature Library for Extraction\", \"authors\": [\"Anuradha Chopra\", \"Abhinaba Roy\", \"Dorien Herremans\"], \"abstract\": \"This paper introduces an extendable modular system that compiles a range of music feature extraction models to aid music information retrieval research. The features include musical elements like key, downbeats, and genre, as well as audio characteristics like instrument recognition, vocals/instrumental classification, and vocals gender detection. The integrated models are state-of-the-art or latest open-source. The features can be extracted as latent or post-processed labels, enabling integration into music applications such as generative music, recommendation, and playlist generation. The modular design allows easy integration of newly developed systems, making it a good benchmarking and comparison tool. This versatile toolkit supports the research community in developing innovative solutions by providing concrete musical features.\", \"url\": \"https://arxiv.org/pdf/2411.00469v1\", \"source\": \"arxiv\", \"published_date\": \"2024-11-01T09:34:36Z\", \"score_bm25\": 0.909493821020785}, {\"paper_id\": \"1605.08857v1\", \"title\": \"Video Key Frame Extraction using Entropy value as Global and Local Feature\", \"authors\": [\"Siddu P Algur\", \"Vivek R\"], \"abstract\": \"Key frames play an important role in video annotation. It is one of the widely used methods for video abstraction as this will help us for processing a large set of video data with sufficient content representation in faster way. In this paper a novel approach for key-frame extraction using entropy value is proposed. The proposed approach classifies frames based on entropy values as global feature and selects frame from each class as representative key-frame. It also eliminates redundant frames from selected key-frames using entropy value as local feature. Evaluation of the approach on several video clips has been presented. Results show that the algorithm is successful in helping annotators automatically identify video key-frames.\", \"url\": \"https://arxiv.org/pdf/1605.08857v1\", \"source\": \"arxiv\", \"published_date\": \"2016-05-28T07:22:55Z\", \"score_bm25\": 0.8959147825516625}, {\"paper_id\": \"2011.00577v3\", \"title\": \"FusiformNet: Extracting Discriminative Facial Features on Different Levels\", \"authors\": [\"Kyo Takano\"], \"abstract\": \"Over the last several years, research on facial recognition based on Deep Neural Network has evolved with approaches like task-specific loss functions, image normalization and augmentation, network architectures, etc. However, there have been few approaches with attention to how human faces differ from person to person. Premising that inter-personal differences are found both generally and locally on the human face, I propose FusiformNet, a novel framework for feature extraction that leverages the nature of discriminative facial features. Tested on Image-Unrestricted setting of Labeled Faces in the Wild benchmark, this method achieved a state-of-the-art accuracy of 96.67% without labeled outside data, image augmentation, normalization, or special loss functions. Likewise, the method also performed on a par with previous state-of-the-arts when pre-trained on CASIA-WebFace dataset. Considering its ability to extract both general and local facial features, the utility of FusiformNet may not be limited to facial recognition but also extend to other DNN-based tasks.\", \"url\": \"https://arxiv.org/pdf/2011.00577v3\", \"source\": \"arxiv\", \"published_date\": \"2020-11-01T18:00:59Z\", \"score_bm25\": 0.5858545153197054}, {\"paper_id\": \"1707.06112v1\", \"title\": \"Microblog Retrieval for Post-Disaster Relief: Applying and Comparing Neural IR Models\", \"authors\": [\"Prannay Khosla\", \"Moumita Basu\", \"Kripabandhu Ghosh\", \"Saptarshi Ghosh\"], \"abstract\": \"Microblogging sites like Twitter and Weibo have emerged as important sourcesof real-time information on ongoing events, including socio-political events, emergency events, and so on. For instance, during emergency events (such as earthquakes, floods, terror attacks), microblogging sites are very useful for gathering situational information in real-time. During such an event, typically only a small fraction of the microblogs (tweets) posted are relevant to the information need. Hence, it is necessary to design effective methodologies for microblog retrieval, so that the relevant tweets can be automatically extracted from large sets of documents (tweets).\\n  In this work, we apply and compare various neural network-based IR models for microblog retrieval for a specific application, as follows. In a disaster situation, one of the primary and practical challenges in coordinating the post-disaster relief operations is to know about what resources are needed and what resources are available in the disaster-affected area. Thus, in this study, we focus on extracting these two specific types of microblogs or tweets namely need tweets and avail tweets, which are tweets which define some needs of the people and the tweets which offer some solutions or aid for the people, respectively.\", \"url\": \"https://arxiv.org/pdf/1707.06112v1\", \"source\": \"arxiv\", \"published_date\": \"2017-07-19T14:28:05Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2005.12739v1\", \"title\": \"An Effective Pipeline for a Real-world Clothes Retrieval System\", \"authors\": [\"Yang-Ho Ji\", \"HeeJae Jun\", \"Insik Kim\", \"Jongtack Kim\", \"Youngjoon Kim\", \"Byungsoo Ko\", \"Hyong-Keun Kook\", \"Jingeun Lee\", \"Sangwon Lee\", \"Sanghyuk Park\"], \"abstract\": \"In this paper, we propose an effective pipeline for clothes retrieval system which has sturdiness on large-scale real-world fashion data. Our proposed method consists of three components: detection, retrieval, and post-processing. We firstly conduct a detection task for precise retrieval on target clothes, then retrieve the corresponding items with the metric learning-based model. To improve the retrieval robustness against noise and misleading bounding boxes, we apply post-processing methods such as weighted boxes fusion and feature concatenation. With the proposed methodology, we achieved 2nd place in the DeepFashion2 Clothes Retrieval 2020 challenge.\", \"url\": \"https://arxiv.org/pdf/2005.12739v1\", \"source\": \"arxiv\", \"published_date\": \"2020-05-26T14:08:49Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2108.05454v1\", \"title\": \"Extracting Semantics from Maintenance Records\", \"authors\": [\"Sharad Dixit\", \"Varish Mulwad\", \"Abhinav Saxena\"], \"abstract\": \"Rapid progress in natural language processing has led to its utilization in a variety of industrial and enterprise settings, including in its use for information extraction, specifically named entity recognition and relation extraction, from documents such as engineering manuals and field maintenance reports. While named entity recognition is a well-studied problem, existing state-of-the-art approaches require large labelled datasets which are hard to acquire for sensitive data such as maintenance records. Further, industrial domain experts tend to distrust results from black box machine learning models, especially when the extracted information is used in downstream predictive maintenance analytics. We overcome these challenges by developing three approaches built on the foundation of domain expert knowledge captured in dictionaries and ontologies. We develop a syntactic and semantic rules-based approach and an approach leveraging a pre-trained language model, fine-tuned for a question-answering task on top of our base dictionary lookup to extract entities of interest from maintenance records. We also develop a preliminary ontology to represent and capture the semantics of maintenance records. Our evaluations on a real-world aviation maintenance records dataset show promising results and help identify challenges specific to named entity recognition in the context of noisy industrial data.\", \"url\": \"https://arxiv.org/pdf/2108.05454v1\", \"source\": \"arxiv\", \"published_date\": \"2021-08-11T21:23:10Z\", \"score_bm25\": 0.0}], \"retrieval noise filtering algorithms\": [{\"paper_id\": \"1605.03033v1\", \"title\": \"Effect of Electrical Filtering on Level Dependent ASE Noise\", \"authors\": [\"Jeremy Witzens\"], \"abstract\": \"We derive an analytical model describing the effect of filtering on amplified spontaneous emission noise during or after opto-electronic conversion. In particular, we show that electrical filtering results in a further reduction of the signal quality factor associated with an effective increase of the noise levels and can lead to counter-intuitive dependencies of the measured signal quality on the characteristics of the test setup. Closed form equations are compared with numerical models and experiments, showing excellent agreement.\", \"url\": \"https://arxiv.org/pdf/1605.03033v1\", \"source\": \"arxiv\", \"published_date\": \"2016-05-10T14:31:02Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2512.20781v1\", \"title\": \"Soft Filtering: Guiding Zero-shot Composed Image Retrieval with Prescriptive and Proscriptive Constraints\", \"authors\": [\"Youjin Jung\", \"Seongwoo Cho\", \"Hyun-seok Min\", \"Sungchul Choi\"], \"abstract\": \"Composed Image Retrieval (CIR) aims to find a target image that aligns with user intent, expressed through a reference image and a modification text. While Zero-shot CIR (ZS-CIR) methods sidestep the need for labeled training data by leveraging pretrained vision-language models, they often rely on a single fused query that merges all descriptive cues of what the user wants, tending to dilute key information and failing to account for what they wish to avoid. Moreover, current CIR benchmarks assume a single correct target per query, overlooking the ambiguity in modification texts. To address these challenges, we propose Soft Filtering with Textual constraints (SoFT), a training-free, plug-and-play filtering module for ZS-CIR. SoFT leverages multimodal large language models (LLMs) to extract two complementary constraints from the reference-modification pair: prescriptive (must-have) and proscriptive (must-avoid) constraints. These serve as semantic filters that reward or penalize candidate images to re-rank results, without modifying the base retrieval model or adding supervision. In addition, we construct a two-stage dataset pipeline that refines CIR benchmarks. We first identify multiple plausible targets per query to construct multi-target triplets, capturing the open-ended nature of user intent. Then guide multimodal LLMs to rewrite the modification text to focus on one target, while referencing contrastive distractors to ensure precision. This enables more comprehensive and reliable evaluation under varying ambiguity levels. Applied on top of CIReVL, a ZS-CIR retriever, SoFT raises R@5 to 65.25 on CIRR (+12.94), mAP@50 to 27.93 on CIRCO (+6.13), and R@50 to 58.44 on FashionIQ (+4.59), demonstrating broad effectiveness.\", \"url\": \"https://arxiv.org/pdf/2512.20781v1\", \"source\": \"arxiv\", \"published_date\": \"2025-12-23T21:29:45Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"1601.04605v1\", \"title\": \"Dynamic Information Retrieval: Theoretical Framework and Application\", \"authors\": [\"Marc Sloan\", \"Jun Wang\"], \"abstract\": \"Theoretical frameworks like the Probability Ranking Principle and its more recent Interactive Information Retrieval variant have guided the development of ranking and retrieval algorithms for decades, yet they are not capable of helping us model problems in Dynamic Information Retrieval which exhibit the following three properties; an observable user signal, retrieval over multiple stages and an overall search intent. In this paper a new theoretical framework for retrieval in these scenarios is proposed. We derive a general dynamic utility function for optimizing over these types of tasks, that takes into account the utility of each stage and the probability of observing user feedback. We apply our framework to experiments over TREC data in the dynamic multi page search scenario as a practical demonstration of its effectiveness and to frame the discussion of its use, its limitations and to compare it against the existing frameworks.\", \"url\": \"https://arxiv.org/pdf/1601.04605v1\", \"source\": \"arxiv\", \"published_date\": \"2016-01-18T17:01:34Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2102.11903v2\", \"title\": \"Neural ranking models for document retrieval\", \"authors\": [\"Mohamed Trabelsi\", \"Zhiyu Chen\", \"Brian D. Davison\", \"Jeff Heflin\"], \"abstract\": \"Ranking models are the main components of information retrieval systems. Several approaches to ranking are based on traditional machine learning algorithms using a set of hand-crafted features. Recently, researchers have leveraged deep learning models in information retrieval. These models are trained end-to-end to extract features from the raw data for ranking tasks, so that they overcome the limitations of hand-crafted features. A variety of deep learning models have been proposed, and each model presents a set of neural network components to extract features that are used for ranking. In this paper, we compare the proposed models in the literature along different dimensions in order to understand the major contributions and limitations of each model. In our discussion of the literature, we analyze the promising neural components, and propose future research directions. We also show the analogy between document retrieval and other retrieval tasks where the items to be ranked are structured documents, answers, images and videos.\", \"url\": \"https://arxiv.org/pdf/2102.11903v2\", \"source\": \"arxiv\", \"published_date\": \"2021-02-23T19:30:37Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2503.01763v2\", \"title\": \"Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models\", \"authors\": [\"Zhengliang Shi\", \"Yuhan Wang\", \"Lingyong Yan\", \"Pengjie Ren\", \"Shuaiqiang Wang\", \"Dawei Yin\", \"Zhaochun Ren\"], \"abstract\": \"Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial step. However, the performance of IR models in tool retrieval tasks remains underexplored and unclear. Most tool-use benchmarks simplify this step by manually pre-annotating a small set of relevant tools for each task, which is far from the real-world scenarios. In this paper, we propose ToolRet, a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets. We benchmark six types of models on ToolRet. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on ToolRet. This low retrieval quality degrades the task pass rate of tool-use LLMs. As a further step, we contribute a large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models.\", \"url\": \"https://arxiv.org/pdf/2503.01763v2\", \"source\": \"arxiv\", \"published_date\": \"2025-03-03T17:37:16Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2005.12739v1\", \"title\": \"An Effective Pipeline for a Real-world Clothes Retrieval System\", \"authors\": [\"Yang-Ho Ji\", \"HeeJae Jun\", \"Insik Kim\", \"Jongtack Kim\", \"Youngjoon Kim\", \"Byungsoo Ko\", \"Hyong-Keun Kook\", \"Jingeun Lee\", \"Sangwon Lee\", \"Sanghyuk Park\"], \"abstract\": \"In this paper, we propose an effective pipeline for clothes retrieval system which has sturdiness on large-scale real-world fashion data. Our proposed method consists of three components: detection, retrieval, and post-processing. We firstly conduct a detection task for precise retrieval on target clothes, then retrieve the corresponding items with the metric learning-based model. To improve the retrieval robustness against noise and misleading bounding boxes, we apply post-processing methods such as weighted boxes fusion and feature concatenation. With the proposed methodology, we achieved 2nd place in the DeepFashion2 Clothes Retrieval 2020 challenge.\", \"url\": \"https://arxiv.org/pdf/2005.12739v1\", \"source\": \"arxiv\", \"published_date\": \"2020-05-26T14:08:49Z\", \"score_bm25\": 0.38649200548177315}, {\"paper_id\": \"2505.21439v1\", \"title\": \"Towards Better Instruction Following Retrieval Models\", \"authors\": [\"Yuchen Zhuang\", \"Aaron Trinh\", \"Rushi Qiang\", \"Haotian Sun\", \"Chao Zhang\", \"Hanjun Dai\", \"Bo Dai\"], \"abstract\": \"Modern information retrieval (IR) models, trained exclusively on standard <query, passage> pairs, struggle to effectively interpret and follow explicit user instructions. We introduce InF-IR, a large-scale, high-quality training corpus tailored for enhancing retrieval models in Instruction-Following IR. InF-IR expands traditional training pairs into over 38,000 expressive <instruction, query, passage> triplets as positive samples. In particular, for each positive triplet, we generate two additional hard negative examples by poisoning both instructions and queries, then rigorously validated by an advanced reasoning model (o3-mini) to ensure semantic plausibility while maintaining instructional incorrectness. Unlike existing corpora that primarily support computationally intensive reranking tasks for decoder-only language models, the highly contrastive positive-negative triplets in InF-IR further enable efficient representation learning for smaller encoder-only models, facilitating direct embedding-based retrieval. Using this corpus, we train InF-Embed, an instruction-aware Embedding model optimized through contrastive learning and instruction-query attention mechanisms to align retrieval outcomes precisely with user intents. Extensive experiments across five instruction-based retrieval benchmarks demonstrate that InF-Embed significantly surpasses competitive baselines by 8.1% in p-MRR, measuring the instruction-following capabilities.\", \"url\": \"https://arxiv.org/pdf/2505.21439v1\", \"source\": \"arxiv\", \"published_date\": \"2025-05-27T17:14:37Z\", \"score_bm25\": 0.3512903337377342}, {\"paper_id\": \"2407.20114v3\", \"title\": \"FiCo-ITR: bridging fine-grained and coarse-grained image-text retrieval for comparative performance analysis\", \"authors\": [\"Mikel Williams-Lekuona\", \"Georgina Cosma\"], \"abstract\": \"In the field of Image-Text Retrieval (ITR), recent advancements have leveraged large-scale Vision-Language Pretraining (VLP) for Fine-Grained (FG) instance-level retrieval, achieving high accuracy at the cost of increased computational complexity. For Coarse-Grained (CG) category-level retrieval, prominent approaches employ Cross-Modal Hashing (CMH) to prioritise efficiency, albeit at the cost of retrieval performance. Due to differences in methodologies, FG and CG models are rarely compared directly within evaluations in the literature, resulting in a lack of empirical data quantifying the retrieval performance-efficiency tradeoffs between the two. This paper addresses this gap by introducing the FiCo-ITR library, which standardises evaluation methodologies for both FG and CG models, facilitating direct comparisons. We conduct empirical evaluations of representative models from both subfields, analysing precision, recall, and computational complexity across varying data scales. Our findings offer new insights into the performance-efficiency trade-offs between recent representative FG and CG models, highlighting their respective strengths and limitations. These findings provide the foundation necessary to make more informed decisions regarding model selection for specific retrieval tasks and highlight avenues for future research into hybrid systems that leverage the strengths of both FG and CG approaches.\", \"url\": \"https://arxiv.org/pdf/2407.20114v3\", \"source\": \"arxiv\", \"published_date\": \"2024-07-29T15:44:22Z\", \"score_bm25\": 0.34317187236769914}, {\"paper_id\": \"1811.08772v1\", \"title\": \"Overcoming low-utility facets for complex answer retrieval\", \"authors\": [\"Sean MacAvaney\", \"Andrew Yates\", \"Arman Cohan\", \"Luca Soldaini\", \"Kai Hui\", \"Nazli Goharian\", \"Ophir Frieder\"], \"abstract\": \"Many questions cannot be answered simply; their answers must include numerous nuanced details and additional context. Complex Answer Retrieval (CAR) is the retrieval of answers to such questions. In their simplest form, these questions are constructed from a topic entity (e.g., `cheese') and a facet (e.g., `health effects'). While topic matching has been thoroughly explored, we observe that some facets use general language that is unlikely to appear verbatim in answers. We call these low-utility facets. In this work, we present an approach to CAR that identifies and addresses low-utility facets. We propose two estimators of facet utility. These include exploiting the hierarchical structure of CAR queries and using facet frequency information from training data. To improve the retrieval performance on low-utility headings, we also include entity similarity scores using knowledge graph embeddings. We apply our approaches to a leading neural ranking technique, and evaluate using the TREC CAR dataset. We find that our approach perform significantly better than the unmodified neural ranker and other leading CAR techniques. We also provide a detailed analysis of our results, and verify that low-utility facets are indeed more difficult to match, and that our approach improves the performance for these difficult queries.\", \"url\": \"https://arxiv.org/pdf/1811.08772v1\", \"source\": \"arxiv\", \"published_date\": \"2018-11-21T15:09:00Z\", \"score_bm25\": 0.3167602710685488}, {\"paper_id\": \"2512.24268v1\", \"title\": \"RAGPart & RAGMask: Retrieval-Stage Defenses Against Corpus Poisoning in Retrieval-Augmented Generation\", \"authors\": [\"Pankayaraj Pathmanathan\", \"Michael-Andrei Panaitescu-Liess\", \"Cho-Yu Jason Chiang\", \"Furong Huang\"], \"abstract\": \"Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to enhance large language models (LLMs) with external knowledge, reducing hallucinations and compensating for outdated information. However, recent studies have exposed a critical vulnerability in RAG pipelines corpus poisoning where adversaries inject malicious documents into the retrieval corpus to manipulate model outputs. In this work, we propose two complementary retrieval-stage defenses: RAGPart and RAGMask. Our defenses operate directly on the retriever, making them computationally lightweight and requiring no modification to the generation model. RAGPart leverages the inherent training dynamics of dense retrievers, exploiting document partitioning to mitigate the effect of poisoned points. In contrast, RAGMask identifies suspicious tokens based on significant similarity shifts under targeted token masking. Across two benchmarks, four poisoning strategies, and four state-of-the-art retrievers, our defenses consistently reduce attack success rates while preserving utility under benign conditions. We further introduce an interpretable attack to stress-test our defenses. Our findings highlight the potential and limitations of retrieval-stage defenses, providing practical insights for robust RAG deployments.\", \"url\": \"https://arxiv.org/pdf/2512.24268v1\", \"source\": \"arxiv\", \"published_date\": \"2025-12-30T14:43:57Z\", \"score_bm25\": 0.17068634134616678}, {\"paper_id\": null, \"title\": \"Noise and Bell's inequality\", \"authors\": [\"David K. Ferry\", \"Laszlo B. Kish\"], \"abstract\": \"From the beginning of quantum mechanics, there has been a discussion about the concept of reality, as exemplified by the EPR paradox. To many, the idea of the paradox and the possibility of local hidden variables was dismissed by the Bell inequality. Yet, there remains considerable evidence that this inequality can be violated even by classical systems, so that experiments showing quantum behavior and the violation of the inequality must be questioned. Here, we demonstrate that classical optical polarization experiments based upon noise in the system can be shown to violate the Bell inequality.\", \"url\": \"https://arxiv.org/pdf/1008.0667v2\", \"source\": \"arxiv\", \"published_date\": \"2010-08-03T22:35:34Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"1612.03430v1\", \"title\": \"Quantum and Classical Frontiers of Noise\", \"authors\": [\"X. Oriols\"], \"abstract\": \"This paper is an introduction to the eleven works of the special issue on Quantum and Classical Frontiers of Noise. The weather, and its butterfly effect, is the typical example that explain why many natural phenomena are, in fact, not predictable with certainty. Noise in classical or quantum phenomena, understood as the difference between the empirical output of an experiment and its statistical prediction, is a measure of such uncertainty. One of the great contributions of noise appeared in 1905 when Einstein showed how the noisy (Brownian) motion of a dust particle seen in a microscope provided uncontroversial evidences on the existence of the (unseen) atoms. The aim of this special issue is to provoke discussions on how noise can help us to better understand (the reality behind) classical and quantum theories, and the frontier between them.\", \"url\": \"https://arxiv.org/pdf/1612.03430v1\", \"source\": \"arxiv\", \"published_date\": \"2016-12-11T16:08:16Z\", \"score_bm25\": 0.0}, {\"paper_id\": null, \"title\": \"Noise based logic: why noise? A comparative study of the necessity of randomness out of orthogonality\", \"authors\": [\"He Wen\", \"Laszlo B. Kish\"], \"abstract\": \"Although noise-based logic shows potential advantages of reduced power dissipation and the ability of large parallel operations with low hardware and time complexity the question still persist: is randomness really needed out of orthogonality? In this Letter, after some general thermodynamical considerations, we show relevant examples where we compare the computational complexity of logic systems based on orthogonal noise and sinusoidal signals, respectively. The conclusion is that in certain special-purpose applications noise-based logic is exponentially better than its sinusoidal version: its computational complexity can be exponentially smaller to perform the same task.\", \"url\": \"https://arxiv.org/pdf/1204.2545v4\", \"source\": \"arxiv\", \"published_date\": \"2012-04-11T15:10:17Z\", \"score_bm25\": 0.0}, {\"paper_id\": null, \"title\": \"Distributing an Exact Algorithm for Maximum Clique: maximising the costup\", \"authors\": [\"Ciaran McCreesh\", \"Patrick Prosser\"], \"abstract\": \"We take an existing implementation of an algorithm for the maximum clique problem and modify it so that we can distribute it over an ad-hoc cluster of machines. Our goal was to achieve a significant speedup in performance with minimal development effort, i.e. a maximum costup. We present a simple modification to a state-of-the-art exact algorithm for maximum clique that allows us to distribute it across many machines. An empirical study over large hard benchmarks shows that speedups of an order of magnitude are routine for 25 or more machines.\", \"url\": \"https://arxiv.org/pdf/1209.4560v1\", \"source\": \"arxiv\", \"published_date\": \"2012-09-20T15:18:54Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"1605.07838v1\", \"title\": \"Decoherence and noise in open quantum system dynamics\", \"authors\": [\"Bassano Vacchini\"], \"abstract\": \"We consider the description of quantum noise within the framework of the standard Copenhagen interpretation of quantum mechanics applied to a composite system environment setting. Averaging over the environmental degrees of freedom leads to a stochastic quantum dynamics, described by equations complying with the constraints arising from the statistical structure of quantum mechanics. Simple examples are considered in the framework of open system dynamics described within a master equation approach, pointing in particular to the appearance of the phenomenon of decoherence and to the relevance of quantum correlation functions of the environment in the determination of the action of quantum noise.\", \"url\": \"https://arxiv.org/pdf/1605.07838v1\", \"source\": \"arxiv\", \"published_date\": \"2016-05-25T11:52:34Z\", \"score_bm25\": 0.0}, {\"paper_id\": null, \"title\": \"Instantaneous noise-based logic\", \"authors\": [\"Laszlo B. Kish\", \"Sunil Khatri\", \"Ferdinand Peper\"], \"abstract\": \"We show two universal, Boolean, deterministic logic schemes based on binary noise timefunctions that can be realized without time-averaging units. The first scheme is based on a new bipolar random telegraph wave scheme and the second one makes use of the recent noise-based logic which is conjectured to be the brain's method of logic operations [Physics Letters A 373 (2009) 2338-2342]. Error propagation and error removal issues are also addressed.\", \"url\": \"https://arxiv.org/pdf/1004.2652v2\", \"source\": \"arxiv\", \"published_date\": \"2010-04-13T20:30:52Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2311.17794v1\", \"title\": \"Noise Dynamics in the Quantum Regime\", \"authors\": [\"Clovis Farley\", \"Edouard Pinsolle\", \"Bertrand Reulet\"], \"abstract\": \"A time-dependent bias voltage on a tunnel junction generates a time-dependent modulation of its current fluctuations, and in particular of its variance. This translates into an excitation at frequency $\\\\tilde{f}$ generating correlations between current fluctuating at any frequency $f$ and at frequency $\\\\pm$ $\\\\tilde{f} -f$. We report the measurement of such a correlation in the fully quantum regime, i.e. when both frequencies are much greater than $k_BT/h$ with $T$ the temperature. Such a correlator, usually referred to as the noise susceptibility, is involved in corrections to the measurements of higher-order moments and in the squeezing of noise.\", \"url\": \"https://arxiv.org/pdf/2311.17794v1\", \"source\": \"arxiv\", \"published_date\": \"2023-11-29T16:39:58Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2308.03684v1\", \"title\": \"Active Noise Control based on the Momentum Multichannel Normalized Filtered-x Least Mean Square Algorithm\", \"authors\": [\"Dongyuan Shi\", \"Woon-Seng Gan\", \"Bhan Lam\", \"Shulin Wen\", \"Xiaoyi Shen\"], \"abstract\": \"Multichannel active noise control (MCANC) is widely utilized to achieve significant noise cancellation area in the complicated acoustic field. Meanwhile, the filter-x least mean square (FxLMS) algorithm gradually becomes the benchmark solution for the implementation of MCANC due to its low computational complexity. However, its slow convergence speed more or less undermines the performance of dealing with quickly varying disturbances, such as piling noise. Furthermore, the noise power variation also deteriorates the robustness of the algorithm when it adopts the fixed step size. To solve these issues, we integrated the normalized multichannel FxLMS with the momentum method, which hence, effectively avoids the interference of the primary noise power and accelerates the convergence of the algorithm. To validate its effectiveness, we deployed this algorithm in a multichannel noise control window to control the real machine noise.\", \"url\": \"https://arxiv.org/pdf/2308.03684v1\", \"source\": \"arxiv\", \"published_date\": \"2023-08-07T15:59:38Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2406.15494v3\", \"title\": \"Simple Cracking of (Noise-Based) Dynamic Watermarking in Smart Grids\", \"authors\": [\"Mehmet Yildirim\", \"Nasir Kenarangui\", \"Robert Balog\", \"Laszlo B. Kish\", \"Chanan Singh\"], \"abstract\": \"Previous research employing a conceptual approach with a digital twin has demonstrated that (noise-based) dynamic watermarking is incapable of providing unconditional security in smart electrical grid systems. However, the implementation of digital twins can be prohibitively costly or infeasible due to limited available data on critical infrastructure. In this study, we first analyze the spectral properties of dynamic watermarking and its associated protocol. Subsequently, we present a straightforward attack inspired by the digital twin method, which extracts and utilizes the grid noises and completely breaches the security of dynamic watermarking without requiring knowledge of the private watermarking signal. The attacker can fully expose the grid while evading detection by the controller. Our findings indicate that in the absence of secure and authenticated communications, dynamic watermarking offers neither conditional nor unconditional security. Conversely, when communication lines, sensors, and communicators are equipped with tamper-resistant and secure/authenticated links, dynamic watermarking becomes redundant for grid security.\", \"url\": \"https://arxiv.org/pdf/2406.15494v3\", \"source\": \"arxiv\", \"published_date\": \"2024-06-18T23:24:22Z\", \"score_bm25\": 0.0}, {\"paper_id\": null, \"title\": \"Stokes' Drift and Hypersensitive Response with Dichotomous Markov Noise\", \"authors\": [\"I. Bena\", \"R. Kawai\", \"C. Van den Broeck\", \"Katja Lindenberg\"], \"abstract\": \"Stochastic Stokes' drift and hypersensitive transport driven by dichotomous noise are theoretically investigated. Explicit mathematical expressions for the asymptotic probability density and drift velocity are derived including the situation in which particles cross unstable fixed points. The results are confirmed by numerical simulations.\", \"url\": \"https://arxiv.org/pdf/cond-mat/0501499v1\", \"source\": \"arxiv\", \"published_date\": \"2005-01-20T17:44:46Z\", \"score_bm25\": 0.0}], \"context-aware passage ranking\": [{\"paper_id\": \"2108.12139v2\", \"title\": \"Dealing with Typos for BERT-based Passage Retrieval and Ranking\", \"authors\": [\"Shengyao Zhuang\", \"Guido Zuccon\"], \"abstract\": \"Passage retrieval and ranking is a key task in open-domain question answering and information retrieval. Current effective approaches mostly rely on pre-trained deep language model-based retrievers and rankers. These methods have been shown to effectively model the semantic matching between queries and passages, also in presence of keyword mismatch, i.e. passages that are relevant to a query but do not contain important query keywords. In this paper we consider the Dense Retriever (DR), a passage retrieval method, and the BERT re-ranker, a popular passage re-ranking method. In this context, we formally investigate how these models respond and adapt to a specific type of keyword mismatch -- that caused by keyword typos occurring in queries. Through empirical investigation, we find that typos can lead to a significant drop in retrieval and ranking effectiveness. We then propose a simple typos-aware training framework for DR and BERT re-ranker to address this issue. Our experimental results on the MS MARCO passage ranking dataset show that, with our proposed typos-aware training, DR and BERT re-ranker can become robust to typos in queries, resulting in significantly improved effectiveness compared to models trained without appropriately accounting for typos.\", \"url\": \"https://arxiv.org/pdf/2108.12139v2\", \"source\": \"arxiv\", \"published_date\": \"2021-08-27T07:09:39Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"1910.10687v2\", \"title\": \"Context-Aware Sentence/Passage Term Importance Estimation For First Stage Retrieval\", \"authors\": [\"Zhuyun Dai\", \"Jamie Callan\"], \"abstract\": \"Term frequency is a common method for identifying the importance of a term in a query or document. But it is a weak signal, especially when the frequency distribution is flat, such as in long queries or short documents where the text is of sentence/passage-length. This paper proposes a Deep Contextualized Term Weighting framework that learns to map BERT's contextualized text representations to context-aware term weights for sentences and passages. When applied to passages, DeepCT-Index produces term weights that can be stored in an ordinary inverted index for passage retrieval. When applied to query text, DeepCT-Query generates a weighted bag-of-words query. Both types of term weight can be used directly by typical first-stage retrieval algorithms. This is novel because most deep neural network based ranking models have higher computational costs, and thus are restricted to later-stage rankers. Experiments on four datasets demonstrate that DeepCT's deep contextualized text understanding greatly improves the accuracy of first-stage retrieval algorithms.\", \"url\": \"https://arxiv.org/pdf/1910.10687v2\", \"source\": \"arxiv\", \"published_date\": \"2019-10-23T17:42:35Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2103.16669v3\", \"title\": \"An In-depth Analysis of Passage-Level Label Transfer for Contextual Document Ranking\", \"authors\": [\"Koustav Rudra\", \"Zeon Trevor Fernando\", \"Avishek Anand\"], \"abstract\": \"Pre-trained contextual language models such as BERT, GPT, and XLnet work quite well for document retrieval tasks. Such models are fine-tuned based on the query-document/query-passage level relevance labels to capture the ranking signals. However, the documents are longer than the passages and such document ranking models suffer from the token limitation (512) of BERT. Researchers proposed ranking strategies that either truncate the documents beyond the token limit or chunk the documents into units that can fit into the BERT. In the later case, the relevance labels are either directly transferred from the original query-document pair or learned through some external model. In this paper, we conduct a detailed study of the design decisions about splitting and label transfer on retrieval effectiveness and efficiency. We find that direct transfer of relevance labels from documents to passages introduces label noise that strongly affects retrieval effectiveness for large training datasets. We also find that query processing times are adversely affected by fine-grained splitting schemes. As a remedy, we propose a careful passage level labelling scheme using weak supervision that delivers improved performance (3-14% in terms of nDCG score) over most of the recently proposed models for ad-hoc retrieval while maintaining manageable computational complexity on four diverse document retrieval datasets.\", \"url\": \"https://arxiv.org/pdf/2103.16669v3\", \"source\": \"arxiv\", \"published_date\": \"2021-03-30T20:28:02Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"1511.05806v1\", \"title\": \"Ranking library materials\", \"authors\": [\"Dirk Lewandowski\"], \"abstract\": \"Purpose: This paper discusses ranking factors suitable for library materials and shows that ranking in general is a complex process and that ranking for library materials requires a variety of techniques. Design/methodology/approach: The relevant literature is reviewed to provide a systematic overview of suitable ranking factors. The discussion is based on an overview of ranking factors used in Web search engines. Findings: While there are a wide variety of ranking factors applicable to library materials, todays library systems use only some of them. When designing a ranking component for the library catalogue, an individual weighting of applicable factors is necessary. Research limitations/applications: While this article discusses different factors, no particular ranking formula is given. However, this article presents the argument that such a formula must always be individual to a certain use case. Practical implications: The factors presented can be considered when designing a ranking component for a librarys search system or when discussing such a project with an ILS vendor. Originality/value: This paper is original in that it is the first to systematically discuss ranking of library materials based on the main factors used by Web search engines.\", \"url\": \"https://arxiv.org/pdf/1511.05806v1\", \"source\": \"arxiv\", \"published_date\": \"2015-11-18T14:36:20Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2210.15133v1\", \"title\": \"Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval\", \"authors\": [\"Dingkun Long\", \"Yanzhao Zhang\", \"Guangwei Xu\", \"Pengjun Xie\"], \"abstract\": \"Pre-trained language model (PTM) has been shown to yield powerful text representations for dense passage retrieval task. The Masked Language Modeling (MLM) is a major sub-task of the pre-training process. However, we found that the conventional random masking strategy tend to select a large number of tokens that have limited effect on the passage retrieval task (e,g. stop-words and punctuation). By noticing the term importance weight can provide valuable information for passage retrieval, we hereby propose alternative retrieval oriented masking (dubbed as ROM) strategy where more important tokens will have a higher probability of being masked out, to capture this straightforward yet essential information to facilitate the language model pre-training process. Notably, the proposed new token masking method will not change the architecture and learning objective of original PTM. Our experiments verify that the proposed ROM enables term importance information to help language model pre-training thus achieving better performance on multiple passage retrieval benchmarks.\", \"url\": \"https://arxiv.org/pdf/2210.15133v1\", \"source\": \"arxiv\", \"published_date\": \"2022-10-27T02:43:48Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2204.07496v4\", \"title\": \"Improving Passage Retrieval with Zero-Shot Question Generation\", \"authors\": [\"Devendra Singh Sachan\", \"Mike Lewis\", \"Mandar Joshi\", \"Armen Aghajanyan\", \"Wen-tau Yih\", \"Joelle Pineau\", \"Luke Zettlemoyer\"], \"abstract\": \"We propose a simple and effective re-ranking method for improving passage retrieval in open question answering. The re-ranker re-scores retrieved passages with a zero-shot question generation model, which uses a pre-trained language model to compute the probability of the input question conditioned on a retrieved passage. This approach can be applied on top of any retrieval method (e.g. neural or keyword-based), does not require any domain- or task-specific training (and therefore is expected to generalize better to data distribution shifts), and provides rich cross-attention between query and passage (i.e. it must explain every token in the question). When evaluated on a number of open-domain retrieval datasets, our re-ranker improves strong unsupervised retrieval models by 6%-18% absolute and strong supervised models by up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new state-of-the-art results on full open-domain question answering by simply adding the new re-ranker to existing models with no further changes.\", \"url\": \"https://arxiv.org/pdf/2204.07496v4\", \"source\": \"arxiv\", \"published_date\": \"2022-04-15T14:51:41Z\", \"score_bm25\": 1.9440921662456239}, {\"paper_id\": \"1805.09012v1\", \"title\": \"An Ontology-Based Reasoning Framework for Context-Aware Applications\", \"authors\": [\"Christoph Anderson\", \"Isabel Suarez\", \"Yaqian Xu\", \"Klaus David\"], \"abstract\": \"Context-aware applications process context information to support users in their daily tasks and routines. These applications can adapt their functionalities by aggregating context information through machine-learning and data processing algorithms, supporting users with recommendations or services based on their current needs. In the last years, smartphones have been used in the field of context-awareness due to their embedded sensors and various communication interfaces such as Bluetooth, WiFi, NFC or cellular. However, building context-aware applications for smartphones can be a challenging and time-consuming task. In this paper, we describe an ontology-based reasoning framework to create context-aware applications. The framework is based on an ontology as well as micro-services to aggregate, process and represent context information.\", \"url\": \"https://arxiv.org/pdf/1805.09012v1\", \"source\": \"arxiv\", \"published_date\": \"2018-05-23T08:36:28Z\", \"score_bm25\": 1.1155438846389751}, {\"paper_id\": null, \"title\": \"Context-aware recommendations from implicit data via scalable tensor factorization\", \"authors\": [\"BalÃ¡zs Hidasi\", \"Domonkos Tikk\"], \"abstract\": \"Albeit the implicit feedback based recommendation problem - when only the user history is available but there are no ratings - is the most typical setting in real-world applications, it is much less researched than the explicit feedback case. State-of-the-art algorithms that are efficient on the explicit case cannot be automatically transformed to the implicit case if scalability should be maintained. There are few implicit feedback benchmark data sets, therefore new ideas are usually experimented on explicit benchmarks. In this paper, we propose a generic context-aware implicit feedback recommender algorithm, coined iTALS. iTALS applies a fast, ALS-based tensor factorization learning method that scales linearly with the number of non-zero elements in the tensor. We also present two approximate and faster variants of iTALS using coordinate descent and conjugate gradient methods at learning. The method also allows us to incorporate various contextual information into the model while maintaining its computational efficiency. We present two context-aware variants of iTALS incorporating seasonality and item purchase sequentiality into the model to distinguish user behavior at different time intervals, and product types with different repetitiveness. Experiments run on six data sets shows that iTALS clearly outperforms context-unaware models and context aware baselines, while it is on par with factorization machines (beats 7 times out of 12 cases) both in terms of recall and MAP.\", \"url\": \"https://arxiv.org/pdf/1309.7611v1\", \"source\": \"arxiv\", \"published_date\": \"2013-09-29T15:50:45Z\", \"score_bm25\": 0.9021962037611366}, {\"paper_id\": \"2005.02373v2\", \"title\": \"Context-Oriented Behavioral Programming\", \"authors\": [\"Achiya Elyasaf\"], \"abstract\": \"Modern systems require programmers to develop code that dynamically adapts to different contexts, leading to the evolution of new context-oriented programming languages. These languages introduce new software-engineering challenges, such as: how to maintain and keep the separation of concerns of the codebase? how to model the changing behaviors? how to verify the system behavior? and more.\\n  This paper introduces Context-Oriented Behavioral Programming(COBP) - a novel paradigm for developing context-aware systems, centered on natural and incremental specification of context-dependent behaviors. As the name suggests, we combine behavioral-programming(BP) - a scenario-based modeling paradigm - with context idioms that explicitly specify when scenarios are relevant and what information they need. The core idea is to connect the behavioral model with a data model that represents the context, allowing an intuitive connection between the models via update and select queries. Combining behavioral-programming with context-oriented programming brings the best of the two worlds, solving issues that arise when using each of the approaches in separation.\\n  We begin with providing abstract semantics for COBP, laying the foundations for applying reasoning algorithms to context-aware behavioral programs. We then exemplify the semantics with formal specifications of systems, including a variant of Conway's Game of Life. Finally, we present a JavaScript-based implementation of the paradigm and provide two case studies of real-life context-aware systems (one in robotics and another in IoT) that were developed using this tool. Throughout the examples and case studies, we provide design patterns and a methodology for coping with the above challenges.\", \"url\": \"https://arxiv.org/pdf/2005.02373v2\", \"source\": \"arxiv\", \"published_date\": \"2020-05-05T17:56:58Z\", \"score_bm25\": 0.8759457232007107}, {\"paper_id\": \"2509.19212v1\", \"title\": \"Steering Multimodal Large Language Models Decoding for Context-Aware Safety\", \"authors\": [\"Zheyuan Liu\", \"Zhangchen Xu\", \"Guangyao Dou\", \"Xiangchi Yuan\", \"Zhaoxuan Tan\", \"Radha Poovendran\", \"Meng Jiang\"], \"abstract\": \"Multimodal Large Language Models (MLLMs) are increasingly deployed in real-world applications, yet their ability to make context-aware safety decisions remains limited. Existing methods often fail to balance oversensitivity (unjustified refusals of benign queries) and undersensitivity (missed detection of visually grounded risks), leaving a persistent gap in safety alignment. To address this issue, we introduce Safety-aware Contrastive Decoding (SafeCoDe), a lightweight and model-agnostic decoding framework that dynamically adjusts token generation based on multimodal context. SafeCoDe operates in two stages: (1) a contrastive decoding mechanism that highlights tokens sensitive to visual context by contrasting real and Gaussian-noised images, and (2) a global-aware token modulation strategy that integrates scene-level reasoning with token-level adjustment to adapt refusals according to the predicted safety verdict. Extensive experiments across diverse MLLM architectures and safety benchmarks, covering undersensitivity, oversensitivity, and general safety evaluations, show that SafeCoDe consistently improves context-sensitive refusal behaviors while preserving model helpfulness.\", \"url\": \"https://arxiv.org/pdf/2509.19212v1\", \"source\": \"arxiv\", \"published_date\": \"2025-09-23T16:32:25Z\", \"score_bm25\": 0.861506340098496}, {\"paper_id\": \"1811.02615v1\", \"title\": \"Understanding the Role of Data-Centric Social Context in Personalized Mobile Applications\", \"authors\": [\"Iqbal H. Sarker\"], \"abstract\": \"Context-awareness in personalized mobile applications is a growing area of study. Social context is one of the most important sources of information in human-activity based applications. In this paper, we mainly focus on social relational context that represents the interpersonal relationship between individuals, and the role or influence of such context on users' diverse phone call activities in their real world life. Individuals different phone call activities such as making a phone call to a particular person or responding an incoming call may differ from person-to-person based on their interpersonal relationships such as family, friend, or colleague. However, it is very difficult to make the device understandable about such semantic relationships between individuals and the relevant context-aware applications. To address this issue, in this paper, we explore the data-centric social relational context that can play a significant role in building context-aware personalized mobile applications for various purposes in our real world life.\", \"url\": \"https://arxiv.org/pdf/1811.02615v1\", \"source\": \"arxiv\", \"published_date\": \"2018-10-15T14:43:16Z\", \"score_bm25\": 0.8508538637879192}, {\"paper_id\": \"1909.07705v2\", \"title\": \"Variational Bayesian Context-aware Representation for Grocery Recommendation\", \"authors\": [\"Zaiqiao Meng\", \"Richard McCreadie\", \"Craig Macdonald\", \"Iadh Ounis\"], \"abstract\": \"Grocery recommendation is an important recommendation use-case, which aims to predict which items a user might choose to buy in the future, based on their shopping history. However, existing methods only represent each user and item by single deterministic points in a low-dimensional continuous space. In addition, most of these methods are trained by maximizing the co-occurrence likelihood with a simple Skip-gram-based formulation, which limits the expressive ability of their embeddings and the resulting recommendation performance. In this paper, we propose the Variational Bayesian Context-Aware Representation (VBCAR) model for grocery recommendation, which is a novel variational Bayesian model that learns the user and item latent vectors by leveraging basket context information from past user-item interactions. We train our VBCAR model based on the Bayesian Skip-gram framework coupled with the amortized variational inference so that it can learn more expressive latent representations that integrate both the non-linearity and Bayesian behaviour. Experiments conducted on a large real-world grocery recommendation dataset show that our proposed VBCAR model can significantly outperform existing state-of-the-art grocery recommendation methods.\", \"url\": \"https://arxiv.org/pdf/1909.07705v2\", \"source\": \"arxiv\", \"published_date\": \"2019-09-17T10:38:19Z\", \"score_bm25\": 0.8274673802870058}, {\"paper_id\": \"2106.14133v1\", \"title\": \"Semi-supervised Semantic Segmentation with Directional Context-aware Consistency\", \"authors\": [\"Xin Lai\", \"Zhuotao Tian\", \"Li Jiang\", \"Shu Liu\", \"Hengshuang Zhao\", \"Liwei Wang\", \"Jiaya Jia\"], \"abstract\": \"Semantic segmentation has made tremendous progress in recent years. However, satisfying performance highly depends on a large number of pixel-level annotations. Therefore, in this paper, we focus on the semi-supervised segmentation problem where only a small set of labeled data is provided with a much larger collection of totally unlabeled images. Nevertheless, due to the limited annotations, models may overly rely on the contexts available in the training data, which causes poor generalization to the scenes unseen before. A preferred high-level representation should capture the contextual information while not losing self-awareness. Therefore, we propose to maintain the context-aware consistency between features of the same identity but with different contexts, making the representations robust to the varying environments. Moreover, we present the Directional Contrastive Loss (DC Loss) to accomplish the consistency in a pixel-to-pixel manner, only requiring the feature with lower quality to be aligned towards its counterpart. In addition, to avoid the false-negative samples and filter the uncertain positive samples, we put forward two sampling strategies. Extensive experiments show that our simple yet effective method surpasses current state-of-the-art methods by a large margin and also generalizes well with extra image-level annotations.\", \"url\": \"https://arxiv.org/pdf/2106.14133v1\", \"source\": \"arxiv\", \"published_date\": \"2021-06-27T03:42:40Z\", \"score_bm25\": 0.8013129229291112}, {\"paper_id\": \"2503.08393v2\", \"title\": \"Weighted Tensor Decompositions for Context-aware Collaborative Filtering\", \"authors\": [\"Joey De Pauw\", \"Bart Goethals\"], \"abstract\": \"Over recent years it has become well accepted that user interest is not static or immutable. There are a variety of contextual factors, such as time of day, the weather or the user's mood, that influence the current interests of the user. Modelling approaches need to take these factors into account if they want to succeed at finding the most relevant content to recommend given the situation.\\n  A popular method for context-aware recommendation is to encode context attributes as extra dimensions of the classic user-item interaction matrix, effectively turning it into a tensor, followed by applying the appropriate tensor decomposition methods to learn missing values. However, unlike with matrix factorization, where all decompositions are essentially a product of matrices, there exist many more options for decomposing tensors by combining vector, matrix and tensor products. We study the most successful decomposition methods that use weighted square loss and categorize them based on their tensor structure and regularization strategy. Additionally, we further extend the pool of methods by filling in the missing combinations.\\n  In this paper we provide an overview of the properties of the different decomposition methods, such as their complexity, scalability, and modelling capacity. These benefits are then contrasted with the performances achieved in offline experiments to gain more insight into which method to choose depending on a specific situation and constraints.\", \"url\": \"https://arxiv.org/pdf/2503.08393v2\", \"source\": \"arxiv\", \"published_date\": \"2025-03-11T12:57:24Z\", \"score_bm25\": 0.759611879814026}, {\"paper_id\": \"2309.04802v3\", \"title\": \"CPMR: Context-Aware Incremental Sequential Recommendation with Pseudo-Multi-Task Learning\", \"authors\": [\"Qingtian Bian\", \"Jiaxing Xu\", \"Hui Fang\", \"Yiping Ke\"], \"abstract\": \"The motivations of users to make interactions can be divided into static preference and dynamic interest. To accurately model user representations over time, recent studies in sequential recommendation utilize information propagation and evolution to mine from batches of arriving interactions. However, they ignore the fact that people are easily influenced by the recent actions of other users in the contextual scenario, and applying evolution across all historical interactions dilutes the importance of recent ones, thus failing to model the evolution of dynamic interest accurately. To address this issue, we propose a Context-Aware Pseudo-Multi-Task Recommender System (CPMR) to model the evolution in both historical and contextual scenarios by creating three representations for each user and item under different dynamics: static embedding, historical temporal states, and contextual temporal states. To dually improve the performance of temporal states evolution and incremental recommendation, we design a Pseudo-Multi-Task Learning (PMTL) paradigm by stacking the incremental single-target recommendations into one multi-target task for joint optimization. Within the PMTL paradigm, CPMR employs a shared-bottom network to conduct the evolution of temporal states across historical and contextual scenarios, as well as the fusion of them at the user-item level. In addition, CPMR incorporates one real tower for incremental predictions, and two pseudo towers dedicated to updating the respective temporal states based on new batches of interactions. Experimental results on four benchmark recommendation datasets show that CPMR consistently outperforms state-of-the-art baselines and achieves significant gains on three of them. The code is available at: https://github.com/DiMarzioBian/CPMR.\", \"url\": \"https://arxiv.org/pdf/2309.04802v3\", \"source\": \"arxiv\", \"published_date\": \"2023-09-09T14:07:11Z\", \"score_bm25\": 0.7330244043747748}, {\"paper_id\": \"1906.02365v1\", \"title\": \"Context-Aware Visual Policy Network for Fine-Grained Image Captioning\", \"authors\": [\"Zheng-Jun Zha\", \"Daqing Liu\", \"Hanwang Zhang\", \"Yongdong Zhang\", \"Feng Wu\"], \"abstract\": \"With the maturity of visual detection techniques, we are more ambitious in describing visual content with open-vocabulary, fine-grained and free-form language, i.e., the task of image captioning. In particular, we are interested in generating longer, richer and more fine-grained sentences and paragraphs as image descriptions. Image captioning can be translated to the task of sequential language prediction given visual content, where the output sequence forms natural language description with plausible grammar. However, existing image captioning methods focus only on language policy while not visual policy, and thus fail to capture visual context that are crucial for compositional reasoning such as object relationships (e.g., \\\"man riding horse\\\") and visual comparisons (e.g., \\\"small(er) cat\\\"). This issue is especially severe when generating longer sequences such as a paragraph. To fill the gap, we propose a Context-Aware Visual Policy network (CAVP) for fine-grained image-to-language generation: image sentence captioning and image paragraph captioning. During captioning, CAVP explicitly considers the previous visual attentions as context, and decides whether the context is used for the current word/sentence generation given the current visual attention. Compared against traditional visual attention mechanism that only fixes a single visual region at each step, CAVP can attend to complex visual compositions over time. The whole image captioning model -- CAVP and its subsequent language policy network -- can be efficiently optimized end-to-end by using an actor-critic policy gradient method. We have demonstrated the effectiveness of CAVP by state-of-the-art performances on MS-COCO and Stanford captioning datasets, using various metrics and sensible visualizations of qualitative visual context.\", \"url\": \"https://arxiv.org/pdf/1906.02365v1\", \"source\": \"arxiv\", \"published_date\": \"2019-06-06T00:37:33Z\", \"score_bm25\": 0.7252980949954638}, {\"paper_id\": \"1704.08598v1\", \"title\": \"Crowdsensing in Opportunistic Mobile Social Networks: A Context-aware and Human-centric Approach\", \"authors\": [\"Phuong Nguyen\", \"Klara Nahrstedt\"], \"abstract\": \"In recent years, there have been efforts to collect human contact traces during social events (e.g., conferences) using Bluetooth devices (e.g., mobile phones, iMotes). The results of these studies have enabled the ability to do the crowd-sourcing task from within the crowd, in order to answer questions, such as: what is the current density of the crowd, or how many people are attending the event? However, in those studies, the sensing devices are usually distributed and configured in a certain manner. For example, the number of devices is fixed, people register for the devices on a volunteering basis. In this paper, we treat the above problem as an optimization problem and draw the connection to the vertex cover problem in graph theory. Since finding the optimal solution for minimum vertex cover problem is NP-complete, approximation algorithms have to be used. However, we will show that the well-known approximation algorithms do not perform well with the crowd-sensing task. In this paper, we propose the notions of node observability and coverage utility score and design a new context-aware approximation algorithm to find vertex cover that is tailored for crowd-sensing task. In addition, we design human-centric bootstrapping strategies to make initial assignment of sensing devices based on meta information about the participants (e.g., interests, friendship). The motivation is to assign the sensing task to a more \\\"socialized\\\" device to obtain better sensing coverage. We perform comprehensive experiments on real-world data traces obtained from previous experimental studies in conference and academic social context. The results show that our proposed approach significantly outperforms the baseline approximation algorithms in terms of sensing coverage.\", \"url\": \"https://arxiv.org/pdf/1704.08598v1\", \"source\": \"arxiv\", \"published_date\": \"2017-04-27T14:28:28Z\", \"score_bm25\": 0.7071953701344421}, {\"paper_id\": \"1505.01071v1\", \"title\": \"A Spatiotemporal Context Definition for Service Adaptation Prediction in a Pervasive Computing Environment\", \"authors\": [\"Darine Ameyed\", \"Moeiz Miraoui\", \"Chakib Tadj\"], \"abstract\": \"Pervasive systems refers to context-aware systems that can sense their context, and adapt their behavior accordingly to provide adaptable services. Proactive adaptation of such systems allows changing the service and the context based on prediction. However, the definition of the context is still vague and not suitable to prediction. In this paper we discuss and classify previous definitions of context. Then, we present a new definition which allows pervasive systems to understand and predict their contexts. We analyze the essential lines that fall within the context definition, and propose some scenarios to make it clear our approach.\", \"url\": \"https://arxiv.org/pdf/1505.01071v1\", \"source\": \"arxiv\", \"published_date\": \"2015-05-05T16:34:23Z\", \"score_bm25\": 0.7013773040645598}, {\"paper_id\": \"1909.06076v2\", \"title\": \"Deep Joint Embeddings of Context and Content for Recommendation\", \"authors\": [\"Miklas S. Kristoffersen\", \"Jacob L. Wieland\", \"Sven E. Shepstone\", \"Zheng-Hua Tan\", \"Vinoba Vinayagamoorthy\"], \"abstract\": \"This paper proposes a deep learning-based method for learning joint context-content embeddings (JCCE) with a view to context-aware recommendations, and demonstrate its application in the television domain. JCCE builds on recent progress within latent representations for recommendation and deep metric learning. The model effectively groups viewing situations and associated consumed content, based on supervision from 2.7 million viewing events. Experiments confirm the recommendation ability of JCCE, achieving improvements when compared to state-of-the-art methods. Furthermore, the approach shows meaningful structures in the learned representations that can be used to gain valuable insights of underlying factors in the relationship between contextual settings and content properties.\", \"url\": \"https://arxiv.org/pdf/1909.06076v2\", \"source\": \"arxiv\", \"published_date\": \"2019-09-13T08:14:22Z\", \"score_bm25\": 0.6973158613927579}, {\"paper_id\": null, \"title\": \"Context Aware Adaptable Applications - A global approach\", \"authors\": [\"Marc Dalmau\", \"Philippe Roose\", \"Sophie Laplace\"], \"abstract\": \"Actual applications (mostly component based) requirements cannot be expressed without a ubiquitous and mobile part for end-users as well as for M2M applications (Machine to Machine). Such an evolution implies context management in order to evaluate the consequences of the mobility and corresponding mechanisms to adapt or to be adapted to the new environment. Applications are then qualified as context aware applications. This first part of this paper presents an overview of context and its management by application adaptation. This part starts by a definition and proposes a model for the context. It also presents various techniques to adapt applications to the context: from self-adaptation to supervised approached. The second part is an overview of architectures for adaptable applications. It focuses on platforms based solutions and shows information flows between application, platform and context. Finally it makes a synthesis proposition with a platform for adaptable context-aware applications called Kalimucho. Then we present implementations tools for software components and a dataflow models in order to implement the Kalimucho platform.\", \"url\": \"https://arxiv.org/pdf/0909.2090v1\", \"source\": \"arxiv\", \"published_date\": \"2009-09-11T06:19:01Z\", \"score_bm25\": 0.5883025931540171}], \"redundancy removal in RAG\": [{\"paper_id\": \"1503.06632v1\", \"title\": \"A Fast Heuristic Algorithm for Redundancy Removal\", \"authors\": [\"Maxim Teslenko\", \"Elena Dubrova\"], \"abstract\": \"Redundancy identification is an important step of the design flow that typically follows logic synthesis and optimization. In addition to reducing circuit area, power consumption, and delay, redundancy removal also improves testability. All commercially available synthesis tools include a redundancy removal engine which is often run multiple times on the same netlist during optimization. This paper presents a fast heuristic algorithm for redundancy removal in combinational circuits. Our idea is to provide a quick partial solution which can be used for the intermediate redundancy removal runs instead of exact ATPG or SAT-based approaches. The presented approach has a higher implication power than the traditional heuristic algorithms, such as FIRE, e.g. on average it removes 37% more redundancies than FIRE with no penalty in runtime.\", \"url\": \"https://arxiv.org/pdf/1503.06632v1\", \"source\": \"arxiv\", \"published_date\": \"2015-03-23T13:25:18Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2406.00065v1\", \"title\": \"Parallel Redundancy Removal in lrslib with Application to Projections\", \"authors\": [\"David Avis\", \"Charles Jordan\"], \"abstract\": \"We describe a parallel implementation in lrslib for removing redundant halfspaces and finding a minimum representation for an H-representation of a convex polyhedron. By a standard transformation, the same code works for V-representations. We use this approach to speed up the redundancy removal step in Fourier-Motzkin elimination. Computational results are given including a comparison with Clarkson's algorithm, which is particularly fast on highly redundant inputs.\", \"url\": \"https://arxiv.org/pdf/2406.00065v1\", \"source\": \"arxiv\", \"published_date\": \"2024-05-30T11:24:11Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2106.03998v1\", \"title\": \"Asynchronous Distributed Optimization with Redundancy in Cost Functions\", \"authors\": [\"Shuo Liu\", \"Nirupam Gupta\", \"Nitin H. Vaidya\"], \"abstract\": \"This paper considers the problem of asynchronous distributed multi-agent optimization on server-based system architecture. In this problem, each agent has a local cost, and the goal for the agents is to collectively find a minimum of their aggregate cost. A standard algorithm to solve this problem is the iterative distributed gradient-descent (DGD) method being implemented collaboratively by the server and the agents. In the synchronous setting, the algorithm proceeds from one iteration to the next only after all the agents complete their expected communication with the server. However, such synchrony can be expensive and even infeasible in real-world applications. We show that waiting for all the agents is unnecessary in many applications of distributed optimization, including distributed machine learning, due to redundancy in the cost functions (or {\\\\em data}). Specifically, we consider a generic notion of redundancy named $(r,Îµ)$-redundancy implying solvability of the original multi-agent optimization problem with $Îµ$ accuracy, despite the removal of up to $r$ (out of total $n$) agents from the system. We present an asynchronous DGD algorithm where in each iteration the server only waits for (any) $n-r$ agents, instead of all the $n$ agents. Assuming $(r,Îµ)$-redundancy, we show that our asynchronous algorithm converges to an approximate solution with error that is linear in $Îµ$ and $r$. Moreover, we also present a generalization of our algorithm to tolerate some Byzantine faulty agents in the system. Finally, we demonstrate the improved communication efficiency of our algorithm through experiments on MNIST and Fashion-MNIST using the benchmark neural network LeNet.\", \"url\": \"https://arxiv.org/pdf/2106.03998v1\", \"source\": \"arxiv\", \"published_date\": \"2021-06-07T22:40:18Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2511.00505v2\", \"title\": \"Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge\", \"authors\": [\"Qi Luo\", \"Xiaonan Li\", \"Junqi Dai\", \"Shuang Cheng\", \"Xipeng Qiu\"], \"abstract\": \"Retrieval-Augmented Generation has shown remarkable results to address Large Language Models' hallucinations, which usually uses a large external corpus to supplement knowledge to LLMs. However, with the development of LLMs, the internal knowledge of LLMs has expanded significantly, thus causing significant knowledge redundancy between the external corpus and LLMs. On the one hand, the indexing cost of dense retrieval is highly related to the corpus size and thus significant redundant knowledge intensifies the dense retrieval's workload. On the other hand, the redundant knowledge in the external corpus is not helpful to LLMs and our exploratory analysis shows that it instead hurts the RAG performance on those questions which the LLM can answer by itself. To address these issues, we propose Zero-RAG to tackle these challenges. Specifically, we first propose the Mastery-Score metric to identify redundant knowledge in the RAG corpus to prune it. After pruning, answers to \\\"mastered\\\" questions rely primarily on internal knowledge of the LLM. To better harness the internal capacity, we propose Query Router and Noise-Tolerant Tuning to avoid the irrelevant documents' distraction and thus further improve the LLM's utilization of internal knowledge with pruned corpus. Experimental results show that Zero-RAG prunes the Wikipedia corpus by 30\\\\% and accelerates the retrieval stage by 22\\\\%, without compromising RAG's performance.\", \"url\": \"https://arxiv.org/pdf/2511.00505v2\", \"source\": \"arxiv\", \"published_date\": \"2025-11-01T11:18:20Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2401.15391v1\", \"title\": \"MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries\", \"authors\": [\"Yixuan Tang\", \"Yi Yang\"], \"abstract\": \"Retrieval-augmented generation (RAG) augments large language models (LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, thereby facilitating the great adoption of LLMs in practice. However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying RAG knowledge base. We demonstrate the benchmarking utility of MultiHop-RAG in two experiments. The first experiment compares different embedding models for retrieving evidence for multi-hop queries. In the second experiment, we examine the capabilities of various state-of-the-art LLMs, including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop queries given the evidence. Both experiments reveal that existing RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable resource for the community in developing effective RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop-RAG and implemented RAG system is publicly available at https://github.com/yixuantt/MultiHop-RAG/.\", \"url\": \"https://arxiv.org/pdf/2401.15391v1\", \"source\": \"arxiv\", \"published_date\": \"2024-01-27T11:41:48Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2106.11517v1\", \"title\": \"Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering\", \"authors\": [\"Shamane Siriwardhana\", \"Rivindu Weerasekera\", \"Elliott Wen\", \"Suranga Nanayakkara\"], \"abstract\": \"In this paper, we illustrate how to fine-tune the entire Retrieval Augment Generation (RAG) architecture in an end-to-end manner. We highlighted the main engineering challenges that needed to be addressed to achieve this objective. We also compare how end-to-end RAG architecture outperforms the original RAG architecture for the task of question answering. We have open-sourced our implementation in the HuggingFace Transformers library.\", \"url\": \"https://arxiv.org/pdf/2106.11517v1\", \"source\": \"arxiv\", \"published_date\": \"2021-06-22T03:17:59Z\", \"score_bm25\": 2.2040224624084757}, {\"paper_id\": \"2404.15939v3\", \"title\": \"Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications\", \"authors\": [\"Andrei-Laurentiu Bornea\", \"Fadhel Ayed\", \"Antonio De Domenico\", \"Nicola Piovesan\", \"Ali Maatouk\"], \"abstract\": \"The application of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems in the telecommunication domain presents unique challenges, primarily due to the complex nature of telecom standard documents and the rapid evolution of the field. The paper introduces Telco-RAG, an open-source RAG framework designed to handle the specific needs of telecommunications standards, particularly 3rd Generation Partnership Project (3GPP) documents. Telco-RAG addresses the critical challenges of implementing a RAG pipeline on highly technical content, paving the way for applying LLMs in telecommunications and offering guidelines for RAG implementation in other technical domains.\", \"url\": \"https://arxiv.org/pdf/2404.15939v3\", \"source\": \"arxiv\", \"published_date\": \"2024-04-24T15:58:59Z\", \"score_bm25\": 2.0966924179766044}, {\"paper_id\": null, \"title\": \"A Comparative Study of Removal Noise from Remote Sensing Image\", \"authors\": [\"Salem Saleh Al-amri\", \"N. V. Kalyankar\", \"S. D. Khamitkar\"], \"abstract\": \"This paper attempts to undertake the study of three types of noise such as Salt and Pepper (SPN), Random variation Impulse Noise (RVIN), Speckle (SPKN). Different noise densities have been removed between 10% to 60% by using five types of filters as Mean Filter (MF), Adaptive Wiener Filter (AWF), Gaussian Filter (GF), Standard Median Filter (SMF) and Adaptive Median Filter (AMF). The same is applied to the Saturn remote sensing image and they are compared with one another. The comparative study is conducted with the help of Mean Square Errors (MSE) and Peak-Signal to Noise Ratio (PSNR). So as to choose the base method for removal of noise from remote sensing image.\", \"url\": \"https://arxiv.org/pdf/1002.1148v1\", \"source\": \"arxiv\", \"published_date\": \"2010-02-05T08:34:39Z\", \"score_bm25\": 2.0454757670526584}, {\"paper_id\": \"2409.03708v2\", \"title\": \"RAG based Question-Answering for Contextual Response Prediction System\", \"authors\": [\"Sriram Veturi\", \"Saurabh Vaichal\", \"Reshma Lal Jagadheesh\", \"Nafis Irtiza Tripto\", \"Nian Yan\"], \"abstract\": \"Large Language Models (LLMs) have shown versatility in various Natural Language Processing (NLP) tasks, including their potential as effective question-answering systems. However, to provide precise and relevant information in response to specific customer queries in industry settings, LLMs require access to a comprehensive knowledge base to avoid hallucinations. Retrieval Augmented Generation (RAG) emerges as a promising technique to address this challenge. Yet, developing an accurate question-answering framework for real-world applications using RAG entails several challenges: 1) data availability issues, 2) evaluating the quality of generated content, and 3) the costly nature of human evaluation. In this paper, we introduce an end-to-end framework that employs LLMs with RAG capabilities for industry use cases. Given a customer query, the proposed system retrieves relevant knowledge documents and leverages them, along with previous chat history, to generate response suggestions for customer service agents in the contact centers of a major retail company. Through comprehensive automated and human evaluations, we show that this solution outperforms the current BERT-based algorithms in accuracy and relevance. Our findings suggest that RAG-based LLMs can be an excellent support to human customer service representatives by lightening their workload.\", \"url\": \"https://arxiv.org/pdf/2409.03708v2\", \"source\": \"arxiv\", \"published_date\": \"2024-09-05T17:14:23Z\", \"score_bm25\": 2.0164038768276136}, {\"paper_id\": \"2510.25518v1\", \"title\": \"Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation\", \"authors\": [\"Thomas Cook\", \"Richard Osuagwu\", \"Liman Tsatiashvili\", \"Vrynsia Vrynsia\", \"Koustav Ghosal\", \"Maraim Masoud\", \"Riccardo Mattivi\"], \"abstract\": \"Retrieval-Augmented Generation (RAG) systems often face limitations in specialized domains such as fintech, where domain-specific ontologies, dense terminology, and acronyms complicate effective retrieval and synthesis. This paper introduces an agentic RAG architecture designed to address these challenges through a modular pipeline of specialized agents. The proposed system supports intelligent query reformulation, iterative sub-query decomposition guided by keyphrase extraction, contextual acronym resolution, and cross-encoder-based context re-ranking. We evaluate our approach against a standard RAG baseline using a curated dataset of 85 question--answer--reference triples derived from an enterprise fintech knowledge base. Experimental results demonstrate that the agentic RAG system outperforms the baseline in retrieval precision and relevance, albeit with increased latency. These findings suggest that structured, multi-agent methodologies offer a promising direction for enhancing retrieval robustness in complex, domain-specific settings.\", \"url\": \"https://arxiv.org/pdf/2510.25518v1\", \"source\": \"arxiv\", \"published_date\": \"2025-10-29T13:41:36Z\", \"score_bm25\": 1.9809330022153009}, {\"paper_id\": \"2402.01717v1\", \"title\": \"From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process\", \"authors\": [\"Jaewoong Kim\", \"Moohong Min\"], \"abstract\": \"Regulatory compliance in the pharmaceutical industry entails navigating through complex and voluminous guidelines, often requiring significant human resources. To address these challenges, our study introduces a chatbot model that utilizes generative AI and the Retrieval Augmented Generation (RAG) method. This chatbot is designed to search for guideline documents relevant to the user inquiries and provide answers based on the retrieved guidelines. Recognizing the inherent need for high reliability in this domain, we propose the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In comparative experiments, the QA-RAG model demonstrated a significant improvement in accuracy, outperforming all other baselines including conventional RAG methods. This paper details QA-RAG's structure and performance evaluation, emphasizing its potential for the regulatory compliance domain in the pharmaceutical industry and beyond. We have made our work publicly available for further research and development.\", \"url\": \"https://arxiv.org/pdf/2402.01717v1\", \"source\": \"arxiv\", \"published_date\": \"2024-01-26T08:23:29Z\", \"score_bm25\": 1.9581594352234433}, {\"paper_id\": \"2504.01346v4\", \"title\": \"RAG over Tables: Hierarchical Memory Index, Multi-Stage Retrieval, and Benchmarking\", \"authors\": [\"Jiaru Zou\", \"Dongqi Fu\", \"Sirui Chen\", \"Xinrui He\", \"Zihao Li\", \"Yada Zhu\", \"Jiawei Han\", \"Jingrui He\"], \"abstract\": \"Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating them with an external knowledge base to improve the answer relevance and accuracy. In real-world scenarios, beyond pure text, a substantial amount of knowledge is stored in tables, and user questions often require retrieving answers that are distributed across multiple tables. Retrieving knowledge from a table corpora (i.e., various individual tables) for a question remains nascent, at least, for (i) how to understand intra- and inter-table knowledge effectively, (ii) how to filter unnecessary tables and how to retrieve the most relevant tables efficiently, (iii) how to prompt LLMs to infer over the retrieval, (iv) how to evaluate the corresponding performance in a realistic setting. Facing the above challenges, in this paper, we first propose a table-corpora-aware RAG framework, named T-RAG, which consists of the hierarchical memory index, multi-stage retrieval, and graph-aware prompting for effective and efficient table knowledge retrieval and inference. Further, we first develop a multi-table question answering benchmark named MultiTableQA, which spans 3 different task types, 57,193 tables, and 23,758 questions in total, and the sources are all from real-world scenarios. Based on MultiTableQA, we did the holistic comparison over table retrieval methods, RAG methods, and table-to-graph representation learning methods, where T-RAG shows the leading accuracy, recall, and running time performance. Also, under T-RAG, we evaluate the inference ability upgrade of different LLMs. Code and Data are available at https://github.com/jiaruzouu/T-RAG\", \"url\": \"https://arxiv.org/pdf/2504.01346v4\", \"source\": \"arxiv\", \"published_date\": \"2025-04-02T04:24:41Z\", \"score_bm25\": 1.8690390561511048}, {\"paper_id\": \"2502.13957v2\", \"title\": \"RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation\", \"authors\": [\"Guangzhi Xiong\", \"Qiao Jin\", \"Xiao Wang\", \"Yin Fang\", \"Haolin Liu\", \"Yifan Yang\", \"Fangyuan Chen\", \"Zhixing Song\", \"Dengyu Wang\", \"Minjia Zhang\", \"Zhiyong Lu\", \"Aidong Zhang\"], \"abstract\": \"Retrieval-augmented generation (RAG) has shown great promise for knowledge-intensive tasks and recently advanced with agentic RAG, where language agents engage in multi-round interactions with external knowledge sources for adaptive information retrieval. However, existing agentic RAG methods often depend on ad-hoc prompt engineering and lack a unified optimization framework. We introduce RAG-Gym, a comprehensive platform that systematically explores three optimization dimensions: (1) prompt engineering, (2) actor tuning, and (3) critic training. For prompt engineering, we propose Re$^2$Search, a novel agent incorporating reasoning reflection that significantly outperforms standard prompts. In actor tuning, we evaluate three popular post-training algorithms with fine-grained process supervision and identify direct preference optimization as the most effective. We further demonstrate that a trained critic can enhance inference by selecting higher-quality intermediate reasoning steps. Together, these findings lead to the optimized Re$^2$Search++ agent, which surpasses most recent methods like Search-R1 by a relative increase of 3.2% to 11.6% in average F1. Finally, we examine the impact of different reward sources and analyze scaling properties in training and inference, offering practical insights for agentic RAG optimization. The project homepage is available at https://rag-gym.github.io.\", \"url\": \"https://arxiv.org/pdf/2502.13957v2\", \"source\": \"arxiv\", \"published_date\": \"2025-02-19T18:56:03Z\", \"score_bm25\": 1.7807473055013516}, {\"paper_id\": \"2511.22858v1\", \"title\": \"RAG System for Supporting Japanese Litigation Procedures: Faithful Response Generation Complying with Legal Norms\", \"authors\": [\"Yuya Ishihara\", \"Atsushi Keyaki\", \"Hiroaki Yamada\", \"Ryutaro Ohara\", \"Mihoko Sumida\"], \"abstract\": \"This study discusses the essential components that a Retrieval-Augmented Generation (RAG)-based LLM system should possess in order to support Japanese medical litigation procedures complying with legal norms. In litigation, expert commissioners, such as physicians, architects, accountants, and engineers, provide specialized knowledge to help judges clarify points of dispute. When considering the substitution of these expert roles with a RAG-based LLM system, the constraint of strict adherence to legal norms is imposed. Specifically, three requirements arise: (1) the retrieval module must retrieve appropriate external knowledge relevant to the disputed issues in accordance with the principle prohibiting the use of private knowledge, (2) the responses generated must originate from the context provided by the RAG and remain faithful to that context, and (3) the retrieval module must reference external knowledge with appropriate timestamps corresponding to the issues at hand. This paper discusses the design of a RAG-based LLM system that satisfies these requirements.\", \"url\": \"https://arxiv.org/pdf/2511.22858v1\", \"source\": \"arxiv\", \"published_date\": \"2025-11-28T03:28:27Z\", \"score_bm25\": 1.7684830346252907}, {\"paper_id\": \"2502.11228v2\", \"title\": \"Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs\", \"authors\": [\"Mohammad Reza Rezaei\", \"Adji Bousso Dieng\"], \"abstract\": \"Retrieval-augmented generation (RAG) enhances large language models (LLMs) for domain-specific question-answering (QA) tasks by leveraging external knowledge sources. However, traditional RAG systems primarily focus on relevance-based retrieval and often struggle with redundancy, especially when reasoning requires connecting information from multiple sources. This paper introduces Vendi-RAG, a framework based on an iterative process that jointly optimizes retrieval diversity and answer quality. This joint optimization leads to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to promote semantic diversity in document retrieval. It then uses an LLM judge that evaluates candidate answers, generated after a reasoning step, and outputs a score that the retriever uses to balance relevance and diversity among the retrieved documents during each iteration. Experiments on three challenging datasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's effectiveness in multi-hop reasoning tasks. The framework achieves significant accuracy improvements over traditional single-step and multi-step RAG approaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on 2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current best baseline. The benefits of Vendi-RAG are even more pronounced as the number of retrieved documents increases. Finally, we evaluated Vendi-RAG across different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and observed consistent improvements, demonstrating that the framework's advantages are model-agnostic.\", \"url\": \"https://arxiv.org/pdf/2502.11228v2\", \"source\": \"arxiv\", \"published_date\": \"2025-02-16T18:46:10Z\", \"score_bm25\": 1.4527671327204075}, {\"paper_id\": \"2412.12881v1\", \"title\": \"RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement\", \"authors\": [\"Jinhao Jiang\", \"Jiayi Chen\", \"Junyi Li\", \"Ruiyang Ren\", \"Shijie Wang\", \"Wayne Xin Zhao\", \"Yang Song\", \"Tao Zhang\"], \"abstract\": \"Existing large language models (LLMs) show exceptional problem-solving capabilities but might struggle with complex reasoning tasks. Despite the successes of chain-of-thought and tree-based search methods, they mainly depend on the internal knowledge of LLMs to search over intermediate reasoning steps, limited to dealing with simple tasks involving fewer reasoning steps. In this paper, we propose \\\\textbf{RAG-Star}, a novel RAG approach that integrates the retrieved information to guide the tree-based deliberative reasoning process that relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree Search, RAG-Star iteratively plans intermediate sub-queries and answers for reasoning based on the LLM itself. To consolidate internal and external knowledge, we propose an retrieval-augmented verification that utilizes query- and answer-aware reward modeling to provide feedback for the inherent reasoning of LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate that RAG-Star significantly outperforms previous RAG and reasoning methods.\", \"url\": \"https://arxiv.org/pdf/2412.12881v1\", \"source\": \"arxiv\", \"published_date\": \"2024-12-17T13:05:36Z\", \"score_bm25\": 1.4304431438277836}, {\"paper_id\": \"2402.07483v2\", \"title\": \"T-RAG: Lessons from the LLM Trenches\", \"authors\": [\"Masoomali Fatehkia\", \"Ji Kim Lucas\", \"Sanjay Chawla\"], \"abstract\": \"Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure to represent entity hierarchies within the organization. This is used to generate a textual description to augment the context when responding to user queries pertaining to entities within the organization's hierarchy. Our evaluations, including a Needle in a Haystack test, show that this combination performs better than a simple RAG or finetuning implementation. Finally, we share some lessons learned based on our experiences building an LLM application for real-world use.\", \"url\": \"https://arxiv.org/pdf/2402.07483v2\", \"source\": \"arxiv\", \"published_date\": \"2024-02-12T08:45:08Z\", \"score_bm25\": 1.3835061998960163}, {\"paper_id\": \"2601.05264v1\", \"title\": \"Engineering the RAG Stack: A Comprehensive Review of the Architecture and Trust Frameworks for Retrieval-Augmented Generation Systems\", \"authors\": [\"Dean Wampler\", \"Dave Nielson\", \"Alireza Seddighi\"], \"abstract\": \"This article provides a comprehensive systematic literature review of academic studies, industrial applications, and real-world deployments from 2018 to 2025, providing a practical guide and detailed overview of modern Retrieval-Augmented Generation (RAG) architectures. RAG offers a modular approach for integrating external knowledge without increasing the capacity of the model as LLM systems expand. Research and engineering practices have been fragmented as a result of the increasing diversity of RAG methodologies, which encompasses a variety of fusion mechanisms, retrieval strategies, and orchestration approaches. We provide quantitative assessment frameworks, analyze the implications for trust and alignment, and systematically consolidate existing RAG techniques into a unified taxonomy. This document is a practical framework for the deployment of resilient, secure, and domain-adaptable RAG systems, synthesizing insights from academic literature, industry reports, and technical implementation guides. It also functions as a technical reference.\", \"url\": \"https://arxiv.org/pdf/2601.05264v1\", \"source\": \"arxiv\", \"published_date\": \"2025-11-07T16:26:29Z\", \"score_bm25\": 1.10549468857403}, {\"paper_id\": \"2504.04915v1\", \"title\": \"Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration\", \"authors\": [\"Ran Xu\", \"Wenqi Shi\", \"Yuchen Zhuang\", \"Yue Yu\", \"Joyce C. Ho\", \"Haoyu Wang\", \"Carl Yang\"], \"abstract\": \"Retrieval-Augmented Generation (RAG) systems often struggle to handle multi-hop question-answering tasks accurately due to irrelevant context retrieval and limited complex reasoning capabilities. We introduce Collab-RAG, a collaborative training framework that leverages mutual enhancement between a white-box small language model (SLM) and a blackbox large language model (LLM) for RAG. Specifically, the SLM decomposes complex queries into simpler sub-questions, thus enhancing the accuracy of the retrieval and facilitating more effective reasoning by the black-box LLM. Concurrently, the black-box LLM provides feedback signals to improve the SLM's decomposition capability. We observe that Collab-RAG relies solely on supervision from an affordable black-box LLM without additional distillation from frontier LLMs, yet demonstrates strong generalization across multiple black-box LLMs. Experimental evaluations across five multi-hop QA datasets demonstrate that Collab-RAG substantially outperforms existing black-box-only and SLM fine-tuning baselines by 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a frozen 32B LLM in question decomposition, highlighting the efficiency of Collab-RAG in improving reasoning and retrieval for complex questions. The code of Collab-RAG is available on https://github.com/ritaranx/Collab-RAG/.\", \"url\": \"https://arxiv.org/pdf/2504.04915v1\", \"source\": \"arxiv\", \"published_date\": \"2025-04-07T10:52:22Z\", \"score_bm25\": 0.923873373932113}, {\"paper_id\": \"2409.18313v5\", \"title\": \"Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation\", \"authors\": [\"Quanting Xie\", \"So Yeon Min\", \"Pengliang Ji\", \"Yue Yang\", \"Tianyi Zhang\", \"Kedi Xu\", \"Aarav Bajaj\", \"Ruslan Salakhutdinov\", \"Matthew Johnson-Roberson\", \"Yonatan Bisk\"], \"abstract\": \"There is no limit to how much a robot might explore and learn, but all of that knowledge needs to be searchable and actionable. Within language research, retrieval augmented generation (RAG) has become the workhorse of large-scale non-parametric knowledge; however, existing techniques do not directly transfer to the embodied domain, which is multimodal, where data is highly correlated, and perception requires abstraction. To address these challenges, we introduce Embodied-RAG, a framework that enhances the foundational model of an embodied agent with a non-parametric memory system capable of autonomously constructing hierarchical knowledge for both navigation and language generation. Embodied-RAG handles a full range of spatial and semantic resolutions across diverse environments and query types, whether for a specific object or a holistic description of ambiance. At its core, Embodied-RAG's memory is structured as a semantic forest, storing language descriptions at varying levels of detail. This hierarchical organization allows the system to efficiently generate context-sensitive outputs across different robotic platforms. We demonstrate that Embodied-RAG effectively bridges RAG to the robotics domain, successfully handling over 250 explanation and navigation queries across kilometer-level environments, highlighting its promise as a general-purpose non-parametric system for embodied agents.\", \"url\": \"https://arxiv.org/pdf/2409.18313v5\", \"source\": \"arxiv\", \"published_date\": \"2024-09-26T21:44:11Z\", \"score_bm25\": 0.5258018769126913}], \"web data preprocessing NLP\": [{\"paper_id\": \"2312.01678v6\", \"title\": \"Jellyfish: A Large Language Model for Data Preprocessing\", \"authors\": [\"Haochen Zhang\", \"Yuyang Dong\", \"Chuan Xiao\", \"Masafumi Oyamada\"], \"abstract\": \"This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format conducive to easy processing. Whereas the use of LLMs has sparked interest in devising universal solutions to DP, recent initiatives in this domain typically rely on GPT APIs, raising inevitable data breach concerns. Unlike these approaches, we consider instruction-tuning local LLMs (7 -- 13B models) as universal DP task solvers that operate on a local, single, and low-priced GPU, ensuring data security and enabling further customization. We select a collection of datasets across four representative DP tasks and construct instruction tuning data using data configuration, knowledge injection, and reasoning data distillation techniques tailored to DP. By tuning Mistral-7B, Llama 3-8B, and OpenOrca-Platypus2-13B, our models, namely, Jellyfish-7B/8B/13B, deliver competitiveness compared to GPT-3.5/4 models and strong generalizability to unseen tasks while barely compromising the base models' abilities in NLP tasks. Meanwhile, Jellyfish offers enhanced reasoning capabilities compared to GPT-3.5.\\n  Our models are available at: https://huggingface.co/NECOUDBFM/Jellyfish .\\n  Our instruction dataset is available at: https://huggingface.co/datasets/NECOUDBFM/Jellyfish-Instruct .\", \"url\": \"https://arxiv.org/pdf/2312.01678v6\", \"source\": \"arxiv\", \"published_date\": \"2023-12-04T07:01:54Z\", \"score_bm25\": 0.0}, {\"paper_id\": null, \"title\": \"A Survey on Preprocessing Methods for Web Usage Data\", \"authors\": [\"V. Chitraa\", \"Dr. Antony Selvdoss Davamani\"], \"abstract\": \"World Wide Web is a huge repository of web pages and links. It provides abundance of information for the Internet users. The growth of web is tremendous as approximately one million pages are added daily. Users' accesses are recorded in web logs. Because of the tremendous usage of web, the web log files are growing at a faster rate and the size is becoming huge. Web data mining is the application of data mining techniques in web data. Web Usage Mining applies mining techniques in log data to extract the behavior of users which is used in various applications like personalized services, adaptive web sites, customer profiling, prefetching, creating attractive web sites etc., Web usage mining consists of three phases preprocessing, pattern discovery and pattern analysis. Web log data is usually noisy and ambiguous and preprocessing is an important process before mining. For discovering patterns sessions are to be constructed efficiently. This paper reviews existing work done in the preprocessing stage. A brief overview of various data mining techniques for discovering patterns, and pattern analysis are discussed. Finally a glimpse of various applications of web usage mining is also presented.\", \"url\": \"https://arxiv.org/pdf/1004.1257v1\", \"source\": \"arxiv\", \"published_date\": \"2010-04-08T07:07:16Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2104.12405v2\", \"title\": \"A dissemination workshop for introducing young Italian students to NLP\", \"authors\": [\"Lucio Messina\", \"Lucia Busso\", \"Claudia Roberta Combei\", \"Ludovica Pannitto\", \"Alessio Miaschi\", \"Gabriele Sarti\", \"Malvina Nissim\"], \"abstract\": \"We describe and make available the game-based material developed for a laboratory run at several Italian science festivals to popularize NLP among young students.\", \"url\": \"https://arxiv.org/pdf/2104.12405v2\", \"source\": \"arxiv\", \"published_date\": \"2021-04-26T09:00:56Z\", \"score_bm25\": 0.0}, {\"paper_id\": null, \"title\": \"Multiple Criteria Decision-Making Preprocessing Using Data Mining Tools\", \"authors\": [\"A. Mosavi\"], \"abstract\": \"Real-life engineering optimization problems need Multiobjective Optimization (MOO) tools. These problems are highly nonlinear. As the process of Multiple Criteria Decision-Making (MCDM) is much expanded most MOO problems in different disciplines can be classified on the basis of it. Thus MCDM methods have gained wide popularity in different sciences and applications. Meanwhile the increasing number of involved components, variables, parameters, constraints and objectives in the process, has made the process very complicated. However the new generation of MOO tools has made the optimization process more automated, but still initializing the process and setting the initial value of simulation tools and also identifying the effective input variables and objectives in order to reach the smaller design space are still complicated. In this situation adding a preprocessing step into the MCDM procedure could make a huge difference in terms of organizing the input variables according to their effects on the optimization objectives of the system. The aim of this paper is to introduce the classification task of data mining as an effective option for identifying the most effective variables of the MCDM systems. To evaluate the effectiveness of the proposed method an example has been given for 3D wing design.\", \"url\": \"https://arxiv.org/pdf/1004.3258v1\", \"source\": \"arxiv\", \"published_date\": \"2010-04-19T17:53:38Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2203.13553v1\", \"title\": \"Preprocessing Reward Functions for Interpretability\", \"authors\": [\"Erik Jenner\", \"Adam Gleave\"], \"abstract\": \"In many real-world applications, the reward function is too complex to be manually specified. In such cases, reward functions must instead be learned from human feedback. Since the learned reward may fail to represent user preferences, it is important to be able to validate the learned reward function prior to deployment. One promising approach is to apply interpretability tools to the reward function to spot potential deviations from the user's intention. Existing work has applied general-purpose interpretability tools to understand learned reward functions. We propose exploiting the intrinsic structure of reward functions by first preprocessing them into simpler but equivalent reward functions, which are then visualized. We introduce a general framework for such reward preprocessing and propose concrete preprocessing algorithms. Our empirical evaluation shows that preprocessed rewards are often significantly easier to understand than the original reward.\", \"url\": \"https://arxiv.org/pdf/2203.13553v1\", \"source\": \"arxiv\", \"published_date\": \"2022-03-25T10:19:35Z\", \"score_bm25\": 0.0}, {\"paper_id\": \"2104.12422v2\", \"title\": \"Teaching NLP with Bracelets and Restaurant Menus: An Interactive Workshop for Italian Students\", \"authors\": [\"Ludovica Pannitto\", \"Lucia Busso\", \"Claudia Roberta Combei\", \"Lucio Messina\", \"Alessio Miaschi\", \"Gabriele Sarti\", \"Malvina Nissim\"], \"abstract\": \"Although Natural Language Processing (NLP) is at the core of many tools young people use in their everyday life, high school curricula (in Italy) do not include any computational linguistics education. This lack of exposure makes the use of such tools less responsible than it could be and makes choosing computational linguistics as a university degree unlikely. To raise awareness, curiosity, and longer-term interest in young people, we have developed an interactive workshop designed to illustrate the basic principles of NLP and computational linguistics to high school Italian students aged between 13 and 18 years. The workshop takes the form of a game in which participants play the role of machines needing to solve some of the most common problems a computer faces in understanding language: from voice recognition to Markov chains to syntactic parsing. Participants are guided through the workshop with the help of instructors, who present the activities and explain core concepts from computational linguistics. The workshop was presented at numerous outlets in Italy between 2019 and 2021, both face-to-face and online.\", \"url\": \"https://arxiv.org/pdf/2104.12422v2\", \"source\": \"arxiv\", \"published_date\": \"2021-04-26T09:23:52Z\", \"score_bm25\": 2.235619609930292}, {\"paper_id\": null, \"title\": \"U-Sem: Semantic Enrichment, User Modeling and Mining of Usage Data on the Social Web\", \"authors\": [\"Fabian Abel\", \"Ilknur Celik\", \"Claudia Hauff\", \"Laura Hollink\", \"Geert-Jan Houben\"], \"abstract\": \"With the growing popularity of Social Web applications, more and more user data is published on the Web everyday. Our research focuses on investigating ways of mining data from such platforms that can be used for modeling users and for semantically augmenting user profiles. This process can enhance adaptation and personalization in various adaptive Web-based systems. In this paper, we present the U-Sem people modeling service, a framework for the semantic enrichment and mining of people's profiles from usage data on the Social Web. We explain the architecture of our people modeling service and describe its application in an adult e-learning context as an example. Versions: Mar 21, 10:10, Mar 25, 09:37\", \"url\": \"https://arxiv.org/pdf/1104.0126v1\", \"source\": \"arxiv\", \"published_date\": \"2011-04-01T09:59:10Z\", \"score_bm25\": 1.8152836822367873}, {\"paper_id\": null, \"title\": \"Adding eScience Assets to the Data Web\", \"authors\": [\"Herbert Van de Sompel\", \"Carl Lagoze\", \"Michael L. Nelson\", \"Simeon Warner\", \"Robert Sanderson\", \"Pete Johnston\"], \"abstract\": \"Aggregations of Web resources are increasingly important in scholarship as it adopts new methods that are data-centric, collaborative, and networked-based. The same notion of aggregations of resources is common to the mashed-up, socially networked information environment of Web 2.0. We present a mechanism to identify and describe aggregations of Web resources that has resulted from the Open Archives Initiative - Object Reuse and Exchange (OAI-ORE) project. The OAI-ORE specifications are based on the principles of the Architecture of the World Wide Web, the Semantic Web, and the Linked Data effort. Therefore, their incorporation into the cyberinfrastructure that supports eScholarship will ensure the integration of the products of scholarly research into the Data Web.\", \"url\": \"https://arxiv.org/pdf/0906.2135v1\", \"source\": \"arxiv\", \"published_date\": \"2009-06-11T15:33:37Z\", \"score_bm25\": 1.8033092840624105}, {\"paper_id\": \"1601.08059v1\", \"title\": \"Exploration and Visualization in the Web of Big Linked Data: A Survey of the State of the Art\", \"authors\": [\"Nikos Bikakis\", \"Timos Sellis\"], \"abstract\": \"Data exploration and visualization systems are of great importance in the Big Data era. Exploring and visualizing very large datasets has become a major research challenge, of which scalability is a vital requirement. In this survey, we describe the major prerequisites and challenges that should be addressed by the modern exploration and visualization systems. Considering these challenges, we present how state-of-the-art approaches from the Database and Information Visualization communities attempt to handle them. Finally, we survey the systems developed by Semantic Web community in the context of the Web of Linked Data, and discuss to which extent these satisfy the contemporary requirements.\", \"url\": \"https://arxiv.org/pdf/1601.08059v1\", \"source\": \"arxiv\", \"published_date\": \"2016-01-29T11:30:44Z\", \"score_bm25\": 1.621915097641644}, {\"paper_id\": \"2005.05623v1\", \"title\": \"Unsupervised Multi-label Dataset Generation from Web Data\", \"authors\": [\"Carlos Roig\", \"David Varas\", \"Issey Masuda\", \"Juan Carlos Riveiro\", \"Elisenda Bou-Balust\"], \"abstract\": \"This paper presents a system towards the generation of multi-label datasets from web data in an unsupervised manner. To achieve this objective, this work comprises two main contributions, namely: a) the generation of a low-noise unsupervised single-label dataset from web-data, and b) the augmentation of labels in such dataset (from single label to multi label). The generation of a single-label dataset uses an unsupervised noise reduction phase (clustering and selection of clusters using anchors) obtaining a 85% of correctly labeled images. An unsupervised label augmentation process is then performed to assign new labels to the images in the dataset using the class activation maps and the uncertainty associated with each class. This process is applied to the dataset generated in this paper and a public dataset (Places365) achieving a 9.5% and 27% of extra labels in each dataset respectively, therefore demonstrating that the presented system can robustly enrich the initial dataset.\", \"url\": \"https://arxiv.org/pdf/2005.05623v1\", \"source\": \"arxiv\", \"published_date\": \"2020-05-12T08:57:59Z\", \"score_bm25\": 1.424657773896932}, {\"paper_id\": \"1902.04790v1\", \"title\": \"SaGe: Web Preemption for Public SPARQL Query Services\", \"authors\": [\"Thomas Minier\", \"Hala Skaf-Molli\", \"Pascal Molli\"], \"abstract\": \"To provide stable and responsive public SPARQL query services, data providers enforce quotas on server usage. Queries which exceed these quotas are interrupted and deliver partial results. Such interruption is not an issue if it is possible to resume queries execution afterward. Unfortunately, there is no preemption model for the Web that allows for suspending and resuming SPARQL queries. In this paper, we propose SaGe: a SPARQL query engine based on Web preemption. SaGe allows SPARQL queries to be suspended by the Web server after a fixed time quantum and resumed upon client request. Web preemption is tractable only if its cost in time is negligible compared to the time quantum. The challenge is to support the full SPARQL query language while keeping the cost of preemption negligible. Experimental results demonstrate that SaGe outperforms existing SPARQL query processing approaches by several orders of magnitude in term of the average total query execution time and the time for first results.\", \"url\": \"https://arxiv.org/pdf/1902.04790v1\", \"source\": \"arxiv\", \"published_date\": \"2019-02-13T08:53:59Z\", \"score_bm25\": 1.338571633393558}, {\"paper_id\": \"1806.07158v2\", \"title\": \"You, the Web and Your Device: Longitudinal Characterization of Browsing Habits\", \"authors\": [\"Luca Vassio\", \"Idilio Drago\", \"Marco Mellia\", \"Zied Ben Houidi\", \"Mohamed Lamine Lamali\"], \"abstract\": \"Understanding how people interact with the web is key for a variety of applications, e.g., from the design of effective web pages to the definition of successful online marketing campaigns. Browsing behavior has been traditionally represented and studied by means of clickstreams, i.e., graphs whose vertices are web pages, and edges are the paths followed by users. Obtaining large and representative data to extract clickstreams is however challenging. The evolution of the web questions whether browsing behavior is changing and, by consequence, whether properties of clickstreams are changing. This paper presents a longitudinal study of clickstreams in from 2013 to 2016. We evaluate an anonymized dataset of HTTP traces captured in a large ISP, where thousands of households are connected. We first propose a methodology to identify actual URLs requested by users from the massive set of requests automatically fired by browsers when rendering web pages. Then, we characterize web usage patterns and clickstreams, taking into account both the temporal evolution and the impact of the device used to explore the web. Our analyses precisely quantify various aspects of clickstreams and uncover interesting patterns, such as the typical short paths followed by people while navigating the web, the fast increasing trend in browsing from mobile devices and the different roles of search engines and social networks in promoting content. Finally, we contribute a dataset of anonymized clickstreams to the community to foster new studies (anonymized clickstreams are available to the public at http://bigdata.polito.it/clickstream).\", \"url\": \"https://arxiv.org/pdf/1806.07158v2\", \"source\": \"arxiv\", \"published_date\": \"2018-06-19T11:25:32Z\", \"score_bm25\": 1.2287822998902131}, {\"paper_id\": \"2507.11773v1\", \"title\": \"Small Data Explainer -- The impact of small data methods in everyday life\", \"authors\": [\"Maren Hackenberg\", \"Sophia G. Connor\", \"Fabian Kabus\", \"June Brawner\", \"Ella Markham\", \"Mahi Hardalupas\", \"Areeq Chowdhury\", \"Rolf Backofen\", \"Anna KÃ¶ttgen\", \"Angelika Rohde\", \"Nadine Binder\", \"Harald Binder\", \"the Collaborative Research Center 1597 Small Data\"], \"abstract\": \"The emergence of breakthrough artificial intelligence (AI) techniques has led to a renewed focus on how small data settings, i.e., settings with limited information, can benefit from such developments. This includes societal issues such as how best to include under-represented groups in data-driven policy and decision making, or the health benefits of assistive technologies such as wearables. We provide a conceptual overview, in particular contrasting small data with big data, and identify common themes from exemplary case studies and application areas. Potential solutions are described in a more detailed technical overview of current data analysis and modelling techniques, highlighting contributions from different disciplines, such as knowledge-driven modelling from statistics and data-driven modelling from computer science. By linking application settings, conceptual contributions and specific techniques, we highlight what is already feasible and suggest what an agenda for fully leveraging small data might look like.\", \"url\": \"https://arxiv.org/pdf/2507.11773v1\", \"source\": \"arxiv\", \"published_date\": \"2025-07-15T22:24:17Z\", \"score_bm25\": 1.1880890904491064}, {\"paper_id\": \"2411.15497v3\", \"title\": \"AeroGen: Enhancing Remote Sensing Object Detection with Diffusion-Driven Data Generation\", \"authors\": [\"Datao Tang\", \"Xiangyong Cao\", \"Xuan Wu\", \"Jialin Li\", \"Jing Yao\", \"Xueru Bai\", \"Dongsheng Jiang\", \"Yin Li\", \"Deyu Meng\"], \"abstract\": \"Remote sensing image object detection (RSIOD) aims to identify and locate specific objects within satellite or aerial imagery. However, there is a scarcity of labeled data in current RSIOD datasets, which significantly limits the performance of current detection algorithms. Although existing techniques, e.g., data augmentation and semi-supervised learning, can mitigate this scarcity issue to some extent, they are heavily dependent on high-quality labeled data and perform worse in rare object classes. To address this issue, this paper proposes a layout-controllable diffusion generative model (i.e. AeroGen) tailored for RSIOD. To our knowledge, AeroGen is the first model to simultaneously support horizontal and rotated bounding box condition generation, thus enabling the generation of high-quality synthetic images that meet specific layout and object category requirements. Additionally, we propose an end-to-end data augmentation framework that integrates a diversity-conditioned generator and a filtering mechanism to enhance both the diversity and quality of generated data. Experimental results demonstrate that the synthetic data produced by our method are of high quality and diversity. Furthermore, the synthetic RSIOD data can significantly improve the detection performance of existing RSIOD models, i.e., the mAP metrics on DIOR, DIOR-R, and HRSC datasets are improved by 3.7%, 4.3%, and 2.43%, respectively. The code is available at https://github.com/Sonettoo/AeroGen.\", \"url\": \"https://arxiv.org/pdf/2411.15497v3\", \"source\": \"arxiv\", \"published_date\": \"2024-11-23T09:04:33Z\", \"score_bm25\": 1.1673119011892312}, {\"paper_id\": null, \"title\": \"Constraints on dark energy from H II starburst galaxy apparent magnitude versus redshift data\", \"authors\": [\"Data Mania\", \"Bharat Ratra\"], \"abstract\": \"In this paper we use H II starburst galaxy apparent magnitude versus redshift data from Siegel et al. (2005) to constrain dark energy cosmological model parameters. These constraints are generally consistent with those derived using other data sets, but are not as restrictive as the tightest currently available constraints.\", \"url\": \"https://arxiv.org/pdf/1110.5626v1\", \"source\": \"arxiv\", \"published_date\": \"2011-10-25T19:44:22Z\", \"score_bm25\": 1.16008491582983}, {\"paper_id\": \"2010.16064v1\", \"title\": \"A 20 Gbps Data Transmitting ASIC with PAM4 for Particle Physics Experiments\", \"authors\": [\"Li Zhang\", \"Datao Gong\", \"Suen Hou\", \"Guanming Huang\", \"Xing Huang\", \"Chonghan Liu\", \"Tiankuan Liu\", \"Hanhan Sun\", \"Quan Sun\", \"Xiangming Sun\", \"Wei Zhang\", \"Jingbo Ye\"], \"abstract\": \"We present the design principle and test results of a data transmitting ASIC, GBS20, for particle physics experiments. The goal of GBS20 will be an ASIC that employs two serializers each from the 10.24 Gbps lpGBT SerDes, sharing the PLL also from lpGBT. A PAM4 encoder plus a VCSEL driver will be implemented in the same die to use the same clock system, eliminating the need of CDRs in the PAM4 encoder. This way the transmitter module, GBT20, developed using the GBS20 ASIC, will have the exact lpGBT data interface and transmission protocol, with an output up to 20.48 Gbps over one fiber. With PAM4 embedded FPGAs at the receiving end, GBT20 will halve the fibers needed in a system and better use the input bandwidth of the FPGA. A prototype, GBS20v0 is fabricated using a commercial 65 nm CMOS technology. This prototype has two serializers and a PAM4 encoder sharing the lpGBT PLL, but no user data input. An internal PRBS generator provides data to the serializers. GBS20v0 is tested barely up to 20.48 Gbps. With lessons learned from this prototype, we are designing the second prototype, GBS20v1, that will have 16 user data input channels each at 1.28 Gbps. We present the design concept of the GBS20 ASIC and the GBT20 module, the preliminary test results, and lessons learned from GBS20v0 and the design of GBS20v1 which will be not only a test chip but also a user chip with 16 input data channels.\", \"url\": \"https://arxiv.org/pdf/2010.16064v1\", \"source\": \"arxiv\", \"published_date\": \"2020-10-30T04:24:49Z\", \"score_bm25\": 1.1310506671618248}, {\"paper_id\": \"1907.02664v2\", \"title\": \"Data Encoding for Byzantine-Resilient Distributed Optimization\", \"authors\": [\"Deepesh Data\", \"Linqi Song\", \"Suhas Diggavi\"], \"abstract\": \"We study distributed optimization in the presence of Byzantine adversaries, where both data and computation are distributed among $m$ worker machines, $t$ of which may be corrupt. The compromised nodes may collaboratively and arbitrarily deviate from their pre-specified programs, and a designated (master) node iteratively computes the model/parameter vector for generalized linear models. In this work, we primarily focus on two iterative algorithms: Proximal Gradient Descent (PGD) and Coordinate Descent (CD). Gradient descent (GD) is a special case of these algorithms. PGD is typically used in the data-parallel setting, where data is partitioned across different samples, whereas, CD is used in the model-parallelism setting, where data is partitioned across the parameter space.\\n  In this paper, we propose a method based on data encoding and error correction over real numbers to combat adversarial attacks. We can tolerate up to $t\\\\leq \\\\lfloor\\\\frac{m-1}{2}\\\\rfloor$ corrupt worker nodes, which is information-theoretically optimal. We give deterministic guarantees, and our method does not assume any probability distribution on the data. We develop a {\\\\em sparse} encoding scheme which enables computationally efficient data encoding and decoding. We demonstrate a trade-off between the corruption threshold and the resource requirements (storage, computational, and communication complexity). As an example, for $t\\\\leq\\\\frac{m}{3}$, our scheme incurs only a {\\\\em constant} overhead on these resources, over that required by the plain distributed PGD/CD algorithms which provide no adversarial protection. To the best of our knowledge, ours is the first paper that makes CD secure against adversarial attacks.\\n  Our encoding scheme extends efficiently to the data streaming model and for stochastic gradient descent (SGD). We also give experimental results to show the efficacy of our proposed schemes.\", \"url\": \"https://arxiv.org/pdf/1907.02664v2\", \"source\": \"arxiv\", \"published_date\": \"2019-07-05T03:31:43Z\", \"score_bm25\": 1.1133559379304567}, {\"paper_id\": \"2005.07866v1\", \"title\": \"Byzantine-Resilient SGD in High Dimensions on Heterogeneous Data\", \"authors\": [\"Deepesh Data\", \"Suhas Diggavi\"], \"abstract\": \"We study distributed stochastic gradient descent (SGD) in the master-worker architecture under Byzantine attacks. We consider the heterogeneous data model, where different workers may have different local datasets, and we do not make any probabilistic assumptions on data generation. At the core of our algorithm, we use the polynomial-time outlier-filtering procedure for robust mean estimation proposed by Steinhardt et al. (ITCS 2018) to filter-out corrupt gradients. In order to be able to apply their filtering procedure in our {\\\\em heterogeneous} data setting where workers compute {\\\\em stochastic} gradients, we derive a new matrix concentration result, which may be of independent interest.\\n  We provide convergence analyses for smooth strongly-convex and non-convex objectives. We derive our results under the bounded variance assumption on local stochastic gradients and a {\\\\em deterministic} condition on datasets, namely, gradient dissimilarity; and for both these quantities, we provide concrete bounds in the statistical heterogeneous data model. We give a trade-off between the mini-batch size for stochastic gradients and the approximation error. Our algorithm can tolerate up to $\\\\frac{1}{4}$ fraction Byzantine workers. It can find approximate optimal parameters in the strongly-convex setting exponentially fast and reach to an approximate stationary point in the non-convex setting with a linear speed, thus, matching the convergence rates of vanilla SGD in the Byzantine-free setting.\\n  We also propose and analyze a Byzantine-resilient SGD algorithm with gradient compression, where workers send $k$ random coordinates of their gradients. Under mild conditions, we show a $\\\\frac{d}{k}$-factor saving in communication bits as well as decoding complexity over our compression-free algorithm without affecting its convergence rate (order-wise) and the approximation error.\", \"url\": \"https://arxiv.org/pdf/2005.07866v1\", \"source\": \"arxiv\", \"published_date\": \"2020-05-16T04:15:27Z\", \"score_bm25\": 1.0214643402013117}, {\"paper_id\": \"2510.21391v1\", \"title\": \"TerraGen: A Unified Multi-Task Layout Generation Framework for Remote Sensing Data Augmentation\", \"authors\": [\"Datao Tang\", \"Hao Wang\", \"Yudeng Xin\", \"Hui Qiao\", \"Dongsheng Jiang\", \"Yin Li\", \"Zhiheng Yu\", \"Xiangyong Cao\"], \"abstract\": \"Remote sensing vision tasks require extensive labeled data across multiple, interconnected domains. However, current generative data augmentation frameworks are task-isolated, i.e., each vision task requires training an independent generative model, and ignores the modeling of geographical information and spatial constraints. To address these issues, we propose \\\\textbf{TerraGen}, a unified layout-to-image generation framework that enables flexible, spatially controllable synthesis of remote sensing imagery for various high-level vision tasks, e.g., detection, segmentation, and extraction. Specifically, TerraGen introduces a geographic-spatial layout encoder that unifies bounding box and segmentation mask inputs, combined with a multi-scale injection scheme and mask-weighted loss to explicitly encode spatial constraints, from global structures to fine details. Also, we construct the first large-scale multi-task remote sensing layout generation dataset containing 45k images and establish a standardized evaluation protocol for this task. Experimental results show that our TerraGen can achieve the best generation image quality across diverse tasks. Additionally, TerraGen can be used as a universal data-augmentation generator, enhancing downstream task performance significantly and demonstrating robust cross-task generalisation in both full-data and few-shot scenarios.\", \"url\": \"https://arxiv.org/pdf/2510.21391v1\", \"source\": \"arxiv\", \"published_date\": \"2025-10-24T12:29:12Z\", \"score_bm25\": 0.9580141566475923}, {\"paper_id\": \"1612.05413v1\", \"title\": \"Analyzing Web Archives Through Topic and Event Focused Sub-collections\", \"authors\": [\"Gerhard Gossen\", \"Elena Demidova\", \"Thomas Risse\"], \"abstract\": \"Web archives capture the history of the Web and are therefore an important source to study how societal developments have been reflected on the Web. However, the large size of Web archives and their temporal nature pose many challenges to researchers interested in working with these collections. In this work, we describe the challenges of working with Web archives and propose the research methodology of extracting and studying sub-collections of the archive focused on specific topics and events. We discuss the opportunities and challenges of this approach and suggest a framework for creating sub-collections.\", \"url\": \"https://arxiv.org/pdf/1612.05413v1\", \"source\": \"arxiv\", \"published_date\": \"2016-12-16T10:10:16Z\", \"score_bm25\": 0.7968923097697849}]}"