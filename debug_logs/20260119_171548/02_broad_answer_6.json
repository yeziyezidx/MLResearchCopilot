"{\"summary\": \"**Context-aware passage ranking** refers to advanced methods in information retrieval that not only assess the relevance of individual passages to a query but also incorporate contextual relationships between passages, such as cross-document dependencies, coreference resolution, and evidence aggregation. This is particularly important in modern **retrieval-augmented generation (RAG)** systems, where retrieved passages feed into downstream generation tasks like question answering or fact verification.\\n\\nTraditional passage ranking often treats each passage independently, using either BM25-style lexical matching or dense vector similarity from embedding models. However, these approaches can fail when the answer or relevant context is distributed across multiple passages, or when understanding requires resolving references between them.\\n\\nTwo recent approaches exemplify cutting-edge progress in this field:\\n\\n1. **Multi-View-guided Passage Reranking (MVP)** – Introduced by Na et al. (EMNLP 2025), MVP is a **non-generative LLM-based reranker** that reduces computational overhead compared to autoregressive LLM methods. It creates multiple \\\"views\\\" of query-passage embeddings, each producing an anchor vector for direct relevance scoring in a single decoding step. An orthogonal loss ensures that each view captures distinct aspects of relevance. MVP achieves state-of-the-art performance with far fewer parameters (220M–3B) and drastically reduced latency (up to 100× faster than generative LLM rerankers), while mitigating position and selection biases.\\n\\n2. **Embedding-Based Context-Aware Reranker (EBCAR)** – Proposed by Yuan, Shabani, and Liu (arXiv 2510.13329), EBCAR is a lightweight embedding-based framework designed to handle **cross-passage inference** challenges, such as coreference, entity disambiguation, and multi-source evidence aggregation. It leverages the structural information of passages and a **hybrid attention mechanism** that captures both global relationships across documents and local relationships within documents. Evaluated on the **ConTEB benchmark**, EBCAR outperforms many SOTA rerankers in tasks requiring contextual integration, with strong efficiency gains.\\n\\nThe general trend in context-aware passage ranking is toward **efficient, lightweight models** that still capture complex inter-passage relationships—moving away from heavy generative models toward embedding-based and hybrid strategies that can scale better in production environments.\", \"problem\": null, \"key_concepts\": [\"- **Context-aware passage ranking**: Ranking passages based on both their direct relevance to a query and their contextual relationships with other passages.\", \"- **Retrieval-Augmented Generation (RAG)**: A pipeline where retrieved passages feed into a language model to improve factual accuracy and reasoning.\", \"- **Passage reranking**: A second-stage process after initial retrieval that reorders candidate passages to maximize relevance and usefulness.\", \"- **Cross-passage inference**: The ability to integrate information spread across multiple passages, requiring reasoning about entity references, timelines, or scattered evidence.\", \"- **Multi-view embeddings**: Representations from different perspectives or \\\"views\\\" of query-passage pairs to capture diverse relevance signals.\", \"- **Hybrid attention mechanism**: A model component that jointly attends to global cross-document relationships and fine-grained intra-document details.\"], \"recent_developments\": [\"- Shift from **autoregressive LLM rerankers** to **non-generative, embedding-based methods** for efficiency.\", \"- Introduction of **multi-view guided embeddings (MVP)** to mitigate position bias and input-order sensitivity.\", \"- Development of **structurally-aware hybrid attention (EBCAR)** to handle complex cross-passage reasoning.\", \"- Benchmarks like **ConTEB** emerging to evaluate rerankers in multi-passage context-sensitive scenarios.\", \"- Demonstrated ability of smaller models (220M–3B parameters) to match or exceed larger fine-tuned LLMs in performance while drastically reducing latency.\"], \"authoritative_sources\": [\"- **Jeongwoo Na et al., EMNLP 2025** – \\\"Multi-View-guided Passage Reranking with Large Language Models\\\" ([ACL Anthology](https://aclanthology.org/2025.emnlp-main.1459.pdf))\", \"- **Ye Yuan, Mohammad Amin Shabani, Siqi Liu, 2025** – \\\"Embedding-Based Context-Aware Reranker\\\" ([arXiv:2510.13329](https://arxiv.org/abs/2510.13329))\", \"- Benchmarks: **ConTEB** for cross-passage evaluation\", \"- Foundational works: Lewis et al. (2020) on RAG, Karpukhin et al. (2020) on dense retrieval, Robertson et al. (1994) on BM25\"], \"search_results\": [{\"title\": \"Multi-view-guided Passage Reranking with Large Language Models\", \"url\": \"https://aclanthology.org/2025.emnlp-main.1459.pdf\", \"snippet\": \"<P>Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 28693–28706 November 4-9, 2025 ©2025 Association for Computational Linguistics</P><P>Multi-view-guided Passage Reranking with Large Language Models</P><P>Jeongwoo Na*, Jun Kwon*, Eunseong Choi, Jongwuk Lee</P><P>Sungkyunkwan University, Republic of Korea {wjddn7946, kwon04210, eunseong, jongwuklee}@skku.edu Abstract</P><P>Recent advances in large language models (LLMs) have shown impressive performance in passage reranking tasks. Despite their suc- cess, LLM-based methods still face challenges in efciency and sensitivity to external biases. (i) Existing models rely mostly on autoregres- sive generation and sliding window strategies to rank passages, which incurs heavy compu- tational overhead as the number of passages increases. (ii) External biases, such as posi- tionorselectionbias,hinderthemodel'sability to accurately represent passages and the input- order sensitivity. To address these limitations, we introduce a novel passage reranking model, called Multi-View-guidedPassage Reranking (MVP). MVP is a non-generative LLM-based reranking method that encodes querypassage informationintodiverseviewembeddingswith- out being inuenced by external biases. For each view, it combines query-aware passage embeddings to produce a distinct anchor vec- tor, used to directly compute relevance scores in a single decoding step. Besides, it employs an orthogonal loss to make the views more distinctive. Extensive experiments demonstrate that MVP, with just 220M parameters, matches the performance of much larger 7B-scale ne- tuned models while achieving a 100× reduction in inference latency. Notably, the 3B-parameter variant of MVP achieves state-of-the-art perfor- mance on both in-domain and out-of-domain benchmarks. The source code is available at https://github.com/bulbna/MVP .</P><P>1 Introduction</P><P>Passage reranking aims to assign ne-grained rel- evance scores to candidate passages  typically retrieved by a rst-stage retriever (Robertson et al., 1994;Karpukhin et al.,2020)  by harnessing the language understanding capabilities of large lan- guage models (LLMs), in both zero-shot and ne-</P><P>*Equal contribution.</P><P>Corresponding author.</P><P>Figure 1: Comparison of latency and nDCG@10 across various reranking models. Latency refers to the time required to rerank for a single query and nDCG@10 is averaged over DL19 and DL20.</P><P>tuned settings. Recent studies (Sun et al.,2023; Liang et al.,2023) formulate a prompt that consists of a query and candidate passages and generate an ordered list of passage identiers in a zero-shot set- ting. Subsequent work has ne-tuned open-source LLMs by distilling knowledge from the teacher model (Pradeep et al.,2023a,b), achieving compet- itive performance. Despite their success, LLM-based reranking methods still face challenges in efciency and sen- sitivity to input order. Specically, we address two key issues for de\", \"source\": \"bing\"}, {\"title\": \"Embedding-Based Context-Aware Reranker\", \"url\": \"https://arxiv.org/pdf/2510.13329v1\", \"snippet\": \"<H1>EMBEDDING-BASED CONTEXT-AWARE RERANKER</H1><H3>Ye Yuan1, 2∗, Mohammad Amin Shabani3, Siqi Liu3</H3><P>1McGill, 2Mila - Quebec AI Institute, 3RBC Borealis</P><H3>ABSTRACT</H3><P>Retrieval-Augmented Generation (RAG) systems rely on retrieving relevant evi-dence from a corpus to support downstream generation. The common practice of splitting a long document into multiple shorter passages enables finer-grained and targeted information retrieval. However, it also introduces challenges when a cor-rect retrieval would require inference across passages, such as resolving corefer-ence, disambiguating entities, and aggregating evidence scattered across multiple sources. Many state-of-the-art (SOTA) reranking methods, despite utilizing pow-erful large pretrained language models with potentially high inference costs, still neglect the aforementioned challenges. Therefore, we propose Embedding-Based Context-Aware Reranker (EBCAR), a lightweight reranking framework operat-ing directly on embeddings of retrieved passages with enhanced cross-passage understandings through the structural information of the passages and a hybrid at-tention mechanism, which captures both high-level interactions across documents and low-level relationships within each document. We evaluate EBCAR against SOTA rerankers on the ConTEB benchmark, demonstrating its effectiveness for information retrieval requiring cross-passage inference and its advantages in both accuracy and efficiency.</P><H3>1 INTRODUCTION</H3><P>Retrieval-Augmented Generation (RAG) systems (Lewis et al., 2020; Wu et al., 2024) have become</P><P>a cornerstone for enabling language models to incorporate external knowledge in complex reasoning tasks such as question answering (Mao et al., 2021; Xu et al., 2024b), fact verification (Adjali, 2024; Yue et al., 2024), and dialogue generation (Huang et al., 2023; Wang et al., 2024). A typical RAG pipeline consists of a retriever, identifying relevant passages from a large corpus, and a reranker, reordering these candidates to surface the most useful evidence for downstream generation. The reranker plays a critical role in filtering out noisy retrieval results and promoting passages that are more faithful, complete, and relevant to the input query (Glass et al., 2022; de Souza P. Moreira et al., 2024). To support fine-grained retrieval, modern pipelines often rely on passage-level indexing, where a long document is split into shorter fixed-size passages for retrieval. This chunking process refines the granularity of retrieval and improves the readability of retrieved content for downstream models (Xu et al., 2024a; Jiang et al., 2024). While long-context encoders and embedding models have recently emerged (Zhang et al., 2024; Warner et al., 2024; Boizard et al., 2025; Zhu et al., 2024), passage-level retrieval remains the dominant design choice in many deployed systems due to its effectiveness and efficiency (Wu et al., 2024; Conti et al., 2025).</P><P>Existing reranking\", \"source\": \"bing\"}, {\"title\": \"[2510.13329] Embedding-Based Context-Aware Reranker\", \"url\": \"https://arxiv.org/abs/2510.13329\", \"snippet\": \"<H1>Computer Science > Computation and Language</H1><H1>Embedding-Based Context-Aware Reranker</H1><P>Retrieval-Augmented Generation (RAG) systems rely on retrieving relevant evidence from a corpus to support downstream generation. The common practice of splitting a long document into multiple shorter passages enables finer-grained and targeted information retrieval. However, it also introduces challenges when a correct retrieval would require inference across passages, such as resolving coreference, disambiguating entities, and aggregating evidence scattered across multiple sources. Many state-of-the-art (SOTA) reranking methods, despite utilizing powerful large pretrained language models with potentially high inference costs, still neglect the aforementioned challenges. Therefore, we propose Embedding-Based Context-Aware Reranker (EBCAR), a lightweight reranking framework operating directly on embeddings of retrieved passages with enhanced cross-passage understandings through the structural information of the passages and a hybrid attention mechanism, which captures both high-level interactions across documents and low-level relationships within each document. We evaluate EBCAR against SOTA rerankers on the ConTEB benchmark, demonstrating its effectiveness for information retrieval requiring cross-passage inference and its advantages in both accuracy and efficiency.</P><TABLE><TR><TD>Comments:</TD><TD>Under Review</TD></TR><TR><TD>Subjects:</TD><TD>Computation and Language (cs.CL)</TD></TR><TR><TD>Cite as:</TD><TD>arXiv:2510.13329 [cs.CL]</TD></TR><TR><TD> </TD><TD>(or arXiv:2510.13329v1 [cs.CL] for this version)</TD></TR><TR><TD> </TD><TD>https://doi.org/10.48550/arXiv.2510.13329\\nFocus to learn more\\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer (What is the Explorer?)</P><P>Connected Papers (What is Connected Papers?)</P><P>Litmaps (What is Litmaps?)</P><P>scite Smart Citations (What are Smart Citations?)</P>\", \"source\": \"bing\"}]}"