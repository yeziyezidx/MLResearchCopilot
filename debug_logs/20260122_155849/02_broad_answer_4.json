{"summary": "Passage ranking for answer synthesis in open-domain question answering (OpenQA) and other knowledge-intensive language tasks has evolved beyond purely topical relevance toward approaches that explicitly model answerability and the utility of a passage for generating correct answers.  \n\nTraditional retrievers often rank passages based solely on lexical or semantic similarity to the query, which can result in retrieving passages that are relevant but do not contain the answer, or passages containing answer entities that are unrelated to the question. This mismatch between retrieval and the needs of downstream answer generation has motivated several novel approaches:  \n\n1. **PReGAN** introduces *answer-oriented passage ranking* using a generative adversarial network (GAN) with dual discriminators — one for topical relevance and one for answerability. The generator learns to rank passages that are both relevant and contain the answer in context. This approach mitigates noise in weakly supervised data where answer entities may appear out of context, improving QA accuracy without external data.  \n\n2. **GripRank** addresses the *retrieval–generation discrepancy* by distilling knowledge from a generative passage estimator (GPE) — a language model that evaluates how likely a passage could generate the correct answer — into a ranking model. A curriculum knowledge distillation process gradually teaches the ranker to prioritize passages that not only match the query but also serve as better input for answer generation.  \n\n3. **Passage-specific Prompt Tuning (PSPT)** leverages large language models (LLMs) for reranking by introducing *learnable soft prompts* tailored to each passage. This parameter-efficient method fine-tunes prompts using limited question–passage relevance pairs, enabling LLMs to incorporate passage-specific knowledge without the computational overhead of full fine-tuning. PSPT ranks passages based on the log-likelihood of generating the query conditioned on the passage and its tuned prompt.  \n\nThese methods reflect a shift toward integrating answer synthesis potential directly into the ranking stage, bridging retrieval with generation and optimizing for downstream QA performance.", "problem": null, "key_concepts": ["- **Passage Ranking**: Ordering retrieved candidate passages by relevance and utility for answering a query.", "- **Answerability**: A measure of whether a passage contains the correct answer in context to the query.", "- **Topical Relevance**: Semantic alignment between the query and passage subject matter.", "- **Generative Passage Estimator (GPE)**: A generative model used to assess the likelihood of a passage producing the correct answer.", "- **Knowledge Distillation**: Teaching a smaller or simpler model to replicate the behavior of a more complex model.", "- **Curriculum Learning**: Training strategy that presents easier tasks first, gradually progressing to harder ones.", "- **Soft Prompt Tuning**: Parameter-efficient adaptation of LLMs via learnable prompts associated with specific passages.", "- **Retrieval–Generation Discrepancy**: The gap between passages that are retrieved for relevance and those that are most useful for answer generation."], "recent_developments": ["- **Integration of Answerability into Ranking** (PReGAN): Dual-discriminator GANs that jointly optimize topical relevance and contextual answer presence.", "- **Generative Knowledge for Ranking** (GripRank): Distilling ranking knowledge from generative models, improving alignment between retrieval outputs and generation needs.", "- **LLM-based Passage-specific Prompt Tuning** (PSPT, 2024): Parameter-efficient reranking that adapts prompts per passage, reducing fine-tuning costs and enhancing relevance scoring.", "- **Focus on Bridging Retrieval and Generation**: All three approaches target the gap between retrieving relevant text and enabling effective answer synthesis.", "- **Curriculum Distillation**: Progressive training strategies to improve ranker robustness and answer provenance recognition."], "authoritative_sources": ["- **PReGAN**: Pan Du, Jian-Yun Nie, Yutao Zhu, Hao Jiang, Lixin Zou, Xiaohui Yan — University of Montreal, Thomson Reuters Labs, Huawei Poisson Lab, Tsinghua University ([arXiv:2207.01762](https://arxiv.org/pdf/2207.01762.pdf)).", "- **GripRank**: Jiaqi Bai, Hongcheng Guo, Jiaheng Liu, Jian Yang, Xinnian Liang, Zhao Yan, Zhoujun Li — Beihang University, Tencent Cloud AI ([arXiv:2305.18144](https://arxiv.org/pdf/2305.18144v1)).", "- **Passage-specific Prompt Tuning (PSPT)**: Xuyang Wu, Zhiyuan Peng, Krishna Sravanthi Rajanala Sai, Hsin-Tai Wu, Yi Fang — Accepted at Gen-IR@SIGIR 2024 ([arXiv:2405.20654](https://arxiv.org/abs/2405.20654))."], "search_results": [{"title": "PReGAN: Answer Oriented Passage Ranking with Weakly Supervised GAN", "url": "https://arxiv.org/pdf/2207.01762.pdf", "snippet": "<TITLE>PReGAN: Answer Oriented Passage Ranking with Weakly Supervised GAN</TITLE><H1>PReGAN: Answer Oriented Passage Ranking with Weakly Supervised GAN</H1><H3>Pan Du∗ Jian-Yun Nie</H3><P>du@youark.com</P><H3>Yutao Zhu</H3><P>University of Montreal Thomson Reuters Labs University of Montreal Montreal, Canada Toronto, Canada Montreal, Canada nie@iro.umontreal.ca</P><H3>Hao Jiang</H3><P>yutaozhu94@gmail.com</P><H3>Lixin Zou</H3><P>Huawei Poisson Lab.</P><H3>Xiaohui Yan</H3><P>Tshinghua Univeristy Shenzhen, China Huawei Poisson Lab. Beijing, China jianghao66@huawei.com Shenzhen, China zoulx15@mails.tsinghua.edu.cn</P><H3>ABSTRACT</H3><P>Beyond topical relevance, passage ranking for open-domain factoid question answering also requires a passage to contain an answer (answerability). While a few recent studies have incorporated some reading capability into a ranker to account for answerability, the ranker is still hindered by the noisy nature of the training data typi-cally available in this area, which considers any passage containing an answer entity as a positive sample. However, the answer entity in a passage is not necessarily mentioned in relation with the given question. To address the problem, we propose an approach called</P><H3>PReGAN</H3><P>for Passage Reranking based on Generative Adversarial Neural networks, which incorporates a discriminator on answer-ability, in addition to a discriminator on topical relevance. The goal is to force the generator to rank higher a passage that is topically rel-evant and contains an answer. Experiments on five public datasets</P><H3>PReGAN</H3><P>show that can better rank appropriate passages, which in turn, boosts the efectiveness of QA systems, and outperforms the existing approaches without using external data.</P><P>yanxiaohui2@huawei.com</P><P>right</P><P>when reading the passage. However, when reading the pas-</P><P>retrieved</P><P>sages with the question, the performance drops dramati-cally [19, 27], showing the critical importance of retrieving good candidate passages. Many studies have been devoted to improving the topical relevance of retrieved candidate passages [19, 22, 25, 41],</P><P>answerability</P><P>but few studies have investigated the problem of ,</P><H3>i.e.</H3><P>, whether a retrieved passage may contain an answer. Both crite-ria - topical relevance and answerability, are critical in the context of OpenQA. In fact, a passage highly relevant to the question may not necessarily contain an answer, and a passage that contains an answer entity may not be relevant to the question. In both cases, the reader may be misled by these passages to select a wrong answer, as the reader highly relies on the few passages provided to it. It is thus important that the selected passages for the reader should both be relevant and contain an asnwer.</P><P>In this paper, we propose an approach to refine the list of candi-date passages according to both criteria through an answer-oriented passage (re-)ranking", "source": "bing"}, {"title": "GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved  Passage Ranking", "url": "https://arxiv.org/pdf/2305.18144v1", "snippet": "<TITLE>GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved  Passage Ranking</TITLE><H1>GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking</H1><P>Jiaqi Bai1,2, Hongcheng Guo2, Jiaheng Liu2, Jian Yang2, Xinnian Liang2, Zhao Yan3and Zhoujun Li2</P><P>1School of Cyber Science and Technology, Beihang University 2State Key Lab of Software Development Environment, Beihang University 3Tencent Cloud AI</P><P>Beijing, China</P><P>{bjq,hongchengguo,liujiaheng,jiaya,xnliang,lizj}@buaa.edu.cn</P><P>zhaoyan@tencent.com</P><H3>ABSTRACT</H3><P>Retrieval-enhanced text generation, which aims to leverage pas-sages retrieved from a large passage corpus for delivering a proper answer given the input query, has shown remarkable progress on knowledge-intensive language tasks such as open-domain question answering and knowledge-enhanced dialogue generation. How-ever, the retrieved passages are not ideal for guiding answer gener-ation because of the discrepancy between retrieval and generation, i.e., the candidate passages are all treated equally during the re-trieval procedure without considering their potential to generate the proper answers. This discrepancy makes a passage retriever deliver a sub-optimal collection of candidate passages to gener-ate answers. In this paper, we propose the GeneRative Knowledge Improved Passage Ranking (GripRank) approach, addressing the above challenge by distilling knowledge from a generative passage estimator (GPE) to a passage ranker, where the GPE is a generative language model used to measure how likely the candidate passages can generate the proper answer. We realize the distillation proce-dure by teaching the passage ranker learning to rank the passages ordered by the GPE. Furthermore, we improve the distillation qual-ity by devising a curriculum knowledge distillation mechanism, which allows the knowledge provided by the GPE can be progres-sively distilled to the ranker through an easy-to-hard curriculum, enabling the passage ranker to correctly recognize the provenance of the answer from many plausible candidates. We conduct exten-sive experiments on four datasets across three knowledge-intensive language tasks. Experimental results show advantages over the state-of-the-art methods for both passage ranking and answer gen-eration on the KILT benchmark.</P><H3>CCS CONCEPTS</H3><P>• Information systems → Retrieval models and ranking.</P><H3>KEYWORDS</H3><P>Knowledge-intensive language tasks, Retrieval-enhanced text gen-eration, Passage ranking, Knowledge distillation</P><H3>1 INTRODUCTION</H3><P>Knowledge-intensive language tasks, including open-domain ques-tion answering, knowledge-grounded conversation, and fact verifi-cation, pose a challenge for retrieving passages most likely to be the provenance of the target answer from a large passage corpus (e.g., Wikipedia). One of the most successful paradigms to deal with these", "source": "bing"}, {"title": "[2405.20654] Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models", "url": "https://arxiv.org/abs/2405.20654", "snippet": "<TITLE>[2405.20654] Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models</TITLE><H1>Computer Science > Computation and Language</H1><DIV>[Submitted on 31 May 2024 (</DIV><A>v1</A><DIV>), last revised 21 Jun 2024 (this version, v2)]</DIV><H1>Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models</H1><A>Xuyang Wu</A><DIV>,</DIV><A>Zhiyuan Peng</A><DIV>,</DIV><A>Krishna Sravanthi Rajanala Sai</A><DIV>,</DIV><A>Hsin-Tai Wu</A><DIV>,</DIV><A>Yi Fang</A><DIV>View a PDF of the paper titled Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models, by Xuyang Wu and 4 other authors</DIV><P>Effective passage retrieval and reranking methods have been widely utilized to identify suitable candidates in open-domain question answering tasks, recent studies have resorted to LLMs for reranking the retrieved passages by the log-likelihood of the question conditioned on each passage. Although these methods have demonstrated promising results, the performance is notably sensitive to the human-written prompt (or hard prompt), and fine-tuning LLMs can be computationally intensive and time-consuming. Furthermore, this approach limits the leverage of question-passage relevance pairs and passage-specific knowledge to enhance the ranking capabilities of LLMs. In this paper, we propose passage-specific prompt tuning for reranking in open-domain question answering (PSPT): a parameter-efficient method that fine-tunes learnable passage-specific soft prompts, incorporating passage-specific knowledge from a limited set of question-passage relevance pairs. The method involves ranking retrieved passages based on the log-likelihood of the model generating the question conditioned on each passage and the learned soft prompt. We conducted extensive experiments utilizing the Llama-2-chat-7B model across three publicly available open-domain question answering datasets and the results demonstrate the effectiveness of the proposed approach.</P><TABLE><TR><TD>Comments:</TD>\n<TD>Accepted at Gen-IR@SIGIR24</TD></TR><TR><TD>Subjects:</TD>\n<TD>Computation and Language (cs.CL); Information Retrieval (cs.IR)</TD></TR><TR><TD>Cite as:</TD>\n<TD><A>arXiv:2405.20654</A> [cs.CL]</TD></TR><TR><TD> </TD>\n<TD>(or <A>arXiv:2405.20654v2</A> [cs.CL] for this version)</TD></TR><TR><TD> </TD>\n<TD><A>https://doi.org/10.48550/arXiv.2405.20654</A>\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><DIV>From: Xuyang Wu [</DIV><A>view email</A><DIV>]</DIV><B><A>[v1]</A></B><DIV>Fri, 31 May 2024 07:43:42 UTC (174 KB)</DIV><B>[v2]</B><DIV>Fri, 21 Jun 2024 03:52:30 UTC (172 KB)</DIV><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer <I>(<A>What is the Explorer?</A>)</I></P><P>Connected Papers <I>(<A>What is Connected Papers?</A>)</I></P><P>Litmaps <I>(<A>What is Litmaps?</A>)</I></P><P>scite Smart Citations <I>(<A>What", "source": "bing"}]}