{"deep-research 或者当前的AI联网搜索中，在拿到了目标网页之后，都是如何如何抽取有效信息组成答案的": [{"paper_id": "2410.11315v1", "title": "[2410.11315v1] SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation", "authors": [], "abstract": "<TITLE>[2410.11315v1] SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation</TITLE><H1>Computer Science > Computation and Language</H1><DIV>[Submitted on 15 Oct 2024]</DIV><H1>SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation</H1><A>Xinping Zhao</A><DIV>,</DIV><A>Dongfang Li</A><DIV>,</DIV><A>Yan Zhong</A><DIV>,</DIV><A>Boren Hu</A><DIV>,</DIV><A>Yibin Chen</A><DIV>,</DIV><A>Baotian Hu</A><DIV>,</DIV><A>Min Zhang</A><DIV>View a PDF of the paper titled SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation, by Xinping Zhao and 6 other authors</DIV><P>Recent studies in Retrieval-Augmented Generation (RAG) have investigated extracting evidence from retrieved passages to reduce computational costs and enhance the final RAG performance, yet it remains challenging. Existing methods heavily rely on heuristic-based augmentation, encountering several issues: (1) Poor generalization due to hand-crafted context filtering; (2) Semantics deficiency due to rule-based context chunking; (3) Skewed length due to sentence-wise filter learning. To address these issues, we propose a model-based evidence extraction learning framework, SEER, optimizing a vanilla model as an evidence extractor with desired properties through self-aligned learning. Extensive experiments show that our method largely improves the final RAG performance, enhances the faithfulness, helpfulness, and conciseness of the extracted evidence, and reduces the evidence length by 9.25 times. The code will be available at <A>this https URL</A>.</P><TABLE><TR><TD>Comments:</TD>\n<TD>15 pages, 6 figures, 5 tables. Accepted by EMNLP 2024 (main)</TD></TR><TR><TD>Subjects:</TD>\n<TD>Computation and Language (cs.CL)</TD></TR><TR><TD>Cite as:</TD>\n<TD><A>arXiv:2410.11315</A> [cs.CL]</TD></TR><TR>\n<TD>(or <A>arXiv:2410.11315v1</A> [cs.CL] for this version)</TD></TR><TR>\n<TD><A>https://doi.org/10.48550/arXiv.2410.11315</A>\nFocus to learn more\narXiv-issued DOI via DataCite (pending registration)</TD></TR></TABLE><H2>Submission history</H2><DIV>From: Xinping Zhao [</DIV><A>view email</A><DIV>]</DIV><B>[v1]</B><DIV>Tue, 15 Oct 2024 06:26:24 UTC (4,382 KB)</DIV><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer <I>(<A>What is the Explorer?</A>)</I></P><P>Litmaps <I>(<A>What is Litmaps?</A>)</I></P><P>scite Smart Citations <I>(<A>What are Smart Citations?</A>)</I></P><META description=\"Abstract page for arXiv paper 2410.11315v1: SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation\"></META>", "url": "https://arxiv.org/abs/2410.11315v1", "pdf_url": "https://arxiv.org/pdf/2410.11315v1.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2401.15391v1", "title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries", "authors": ["Yixuan Tang", "Yi Yang"], "abstract": "Retrieval-augmented generation (RAG) augments large language models (LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, thereby facilitating the great adoption of LLMs in practice. However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying RAG knowledge base. We demonstrate the benchmarking utility of MultiHop-RAG in two experiments. The first experiment compares different embedding models for retrieving evidence for multi-hop queries. In the second experiment, we examine the capabilities of various state-of-the-art LLMs, including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop queries given the evidence. Both experiments reveal that existing RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable resource for the community in developing effective RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop-RAG and implemented RAG system is publicly available at https://github.com/yixuantt/MultiHop-RAG/.", "url": "http://arxiv.org/abs/2401.15391v1", "pdf_url": "https://arxiv.org/pdf/2401.15391v1", "source": "arxiv", "published_date": "2024-01-27T11:41:48Z", "score_bm25": 0.0}, {"paper_id": "2510.25518v1", "title": "Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation", "authors": ["Thomas Cook", "Richard Osuagwu", "Liman Tsatiashvili", "Vrynsia Vrynsia", "Koustav Ghosal", "Maraim Masoud", "Riccardo Mattivi"], "abstract": "Retrieval-Augmented Generation (RAG) systems often face limitations in specialized domains such as fintech, where domain-specific ontologies, dense terminology, and acronyms complicate effective retrieval and synthesis. This paper introduces an agentic RAG architecture designed to address these challenges through a modular pipeline of specialized agents. The proposed system supports intelligent query reformulation, iterative sub-query decomposition guided by keyphrase extraction, contextual acronym resolution, and cross-encoder-based context re-ranking. We evaluate our approach against a standard RAG baseline using a curated dataset of 85 question--answer--reference triples derived from an enterprise fintech knowledge base. Experimental results demonstrate that the agentic RAG system outperforms the baseline in retrieval precision and relevance, albeit with increased latency. These findings suggest that structured, multi-agent methodologies offer a promising direction for enhancing retrieval robustness in complex, domain-specific settings.", "url": "http://arxiv.org/abs/2510.25518v1", "pdf_url": "https://arxiv.org/pdf/2510.25518v1", "source": "arxiv", "published_date": "2025-10-29T13:41:36Z", "score_bm25": 0.0}, {"paper_id": "2402.07483v2", "title": "T-RAG: Lessons from the LLM Trenches", "authors": ["Masoomali Fatehkia", "Ji Kim Lucas", "Sanjay Chawla"], "abstract": "Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure to represent entity hierarchies within the organization. This is used to generate a textual description to augment the context when responding to user queries pertaining to entities within the organization's hierarchy. Our evaluations, including a Needle in a Haystack test, show that this combination performs better than a simple RAG or finetuning implementation. Finally, we share some lessons learned based on our experiences building an LLM application for real-world use.", "url": "http://arxiv.org/abs/2402.07483v2", "pdf_url": "https://arxiv.org/pdf/2402.07483v2", "source": "arxiv", "published_date": "2024-02-12T08:45:08Z", "score_bm25": 0.0}, {"paper_id": "2412.12881v1", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "abstract": "Existing large language models (LLMs) show exceptional problem-solving capabilities but might struggle with complex reasoning tasks. Despite the successes of chain-of-thought and tree-based search methods, they mainly depend on the internal knowledge of LLMs to search over intermediate reasoning steps, limited to dealing with simple tasks involving fewer reasoning steps. In this paper, we propose \\textbf{RAG-Star}, a novel RAG approach that integrates the retrieved information to guide the tree-based deliberative reasoning process that relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree Search, RAG-Star iteratively plans intermediate sub-queries and answers for reasoning based on the LLM itself. To consolidate internal and external knowledge, we propose an retrieval-augmented verification that utilizes query- and answer-aware reward modeling to provide feedback for the inherent reasoning of LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate that RAG-Star significantly outperforms previous RAG and reasoning methods.", "url": "http://arxiv.org/abs/2412.12881v1", "pdf_url": "https://arxiv.org/pdf/2412.12881v1", "source": "arxiv", "published_date": "2024-12-17T13:05:36Z", "score_bm25": 0.0}, {"paper_id": "2502.13957v2", "title": "RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation", "authors": ["Guangzhi Xiong", "Qiao Jin", "Xiao Wang", "Yin Fang", "Haolin Liu", "Yifan Yang", "Fangyuan Chen", "Zhixing Song", "Dengyu Wang", "Minjia Zhang", "Zhiyong Lu", "Aidong Zhang"], "abstract": "Retrieval-augmented generation (RAG) has shown great promise for knowledge-intensive tasks and recently advanced with agentic RAG, where language agents engage in multi-round interactions with external knowledge sources for adaptive information retrieval. However, existing agentic RAG methods often depend on ad-hoc prompt engineering and lack a unified optimization framework. We introduce RAG-Gym, a comprehensive platform that systematically explores three optimization dimensions: (1) prompt engineering, (2) actor tuning, and (3) critic training. For prompt engineering, we propose Re$^2$Search, a novel agent incorporating reasoning reflection that significantly outperforms standard prompts. In actor tuning, we evaluate three popular post-training algorithms with fine-grained process supervision and identify direct preference optimization as the most effective. We further demonstrate that a trained critic can enhance inference by selecting higher-quality intermediate reasoning steps. Together, these findings lead to the optimized Re$^2$Search++ agent, which surpasses most recent methods like Search-R1 by a relative increase of 3.2% to 11.6% in average F1. Finally, we examine the impact of different reward sources and analyze scaling properties in training and inference, offering practical insights for agentic RAG optimization. The project homepage is available at https://rag-gym.github.io.", "url": "http://arxiv.org/abs/2502.13957v2", "pdf_url": "https://arxiv.org/pdf/2502.13957v2", "source": "arxiv", "published_date": "2025-02-19T18:56:03Z", "score_bm25": 0.0}, {"paper_id": "2207.01762v1", "title": "PReGAN: Answer Oriented Passage Ranking with Weakly Supervised GAN", "authors": ["Pan Du", "Jian-Yun Nie", "Yutao Zhu", "Hao Jiang", "Lixin Zou", "Xiaohui Yan"], "abstract": "Beyond topical relevance, passage ranking for open-domain factoid question answering also requires a passage to contain an answer (answerability). While a few recent studies have incorporated some reading capability into a ranker to account for answerability, the ranker is still hindered by the noisy nature of the training data typically available in this area, which considers any passage containing an answer entity as a positive sample. However, the answer entity in a passage is not necessarily mentioned in relation with the given question. To address the problem, we propose an approach called \\ttt{PReGAN} for Passage Reranking based on Generative Adversarial Neural networks, which incorporates a discriminator on answerability, in addition to a discriminator on topical relevance. The goal is to force the generator to rank higher a passage that is topically relevant and contains an answer. Experiments on five public datasets show that \\ttt{PReGAN} can better rank appropriate passages, which in turn, boosts the effectiveness of QA systems, and outperforms the existing approaches without using external data.", "url": "http://arxiv.org/abs/2207.01762v1", "pdf_url": "https://arxiv.org/pdf/2207.01762v1", "source": "arxiv", "published_date": "2022-07-05T01:43:35Z", "score_bm25": 0.0}, {"paper_id": "2405.20654", "title": "[2405.20654] Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models", "authors": [], "abstract": "<TITLE>[2405.20654] Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models</TITLE><H1>Computer Science > Computation and Language</H1><DIV>[Submitted on 31 May 2024 (</DIV><A>v1</A><DIV>), last revised 21 Jun 2024 (this version, v2)]</DIV><H1>Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models</H1><A>Xuyang Wu</A><DIV>,</DIV><A>Zhiyuan Peng</A><DIV>,</DIV><A>Krishna Sravanthi Rajanala Sai</A><DIV>,</DIV><A>Hsin-Tai Wu</A><DIV>,</DIV><A>Yi Fang</A><DIV>View a PDF of the paper titled Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models, by Xuyang Wu and 4 other authors</DIV><P>Effective passage retrieval and reranking methods have been widely utilized to identify suitable candidates in open-domain question answering tasks, recent studies have resorted to LLMs for reranking the retrieved passages by the log-likelihood of the question conditioned on each passage. Although these methods have demonstrated promising results, the performance is notably sensitive to the human-written prompt (or hard prompt), and fine-tuning LLMs can be computationally intensive and time-consuming. Furthermore, this approach limits the leverage of question-passage relevance pairs and passage-specific knowledge to enhance the ranking capabilities of LLMs. In this paper, we propose passage-specific prompt tuning for reranking in open-domain question answering (PSPT): a parameter-efficient method that fine-tunes learnable passage-specific soft prompts, incorporating passage-specific knowledge from a limited set of question-passage relevance pairs. The method involves ranking retrieved passages based on the log-likelihood of the model generating the question conditioned on each passage and the learned soft prompt. We conducted extensive experiments utilizing the Llama-2-chat-7B model across three publicly available open-domain question answering datasets and the results demonstrate the effectiveness of the proposed approach.</P><TABLE><TR><TD>Comments:</TD>\n<TD>Accepted at Gen-IR@SIGIR24</TD></TR><TR><TD>Subjects:</TD>\n<TD>Computation and Language (cs.CL); Information Retrieval (cs.IR)</TD></TR><TR><TD>Cite as:</TD>\n<TD><A>arXiv:2405.20654</A> [cs.CL]</TD></TR><TR><TD> </TD>\n<TD>(or <A>arXiv:2405.20654v2</A> [cs.CL] for this version)</TD></TR><TR><TD> </TD>\n<TD><A>https://doi.org/10.48550/arXiv.2405.20654</A>\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><DIV>From: Xuyang Wu [</DIV><A>view email</A><DIV>]</DIV><B><A>[v1]</A></B><DIV>Fri, 31 May 2024 07:43:42 UTC (174 KB)</DIV><B>[v2]</B><DIV>Fri, 21 Jun 2024 03:52:30 UTC (172 KB)</DIV><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer <I>(<A>What is the Explorer?</A>)</I></P><P>Connected Papers <I>(<A>What is Connected Papers?</A>)</I></P><P>Litmaps <I>(<A>What is Litmaps?</A>)</I></P><P>scite Smart Citations <I>(<A>What", "url": "https://arxiv.org/abs/2405.20654", "pdf_url": "https://arxiv.org/pdf/2405.20654.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2405.20654v1", "title": "Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models", "authors": [], "abstract": "<TITLE>Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models</TITLE><H1>Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models</H1><TABLE><TR><TD><P>Xuyang Wu∗</P>\n</TD>\n<TD><P>Zhiyuan Peng∗</P>\n</TD>\n<TD><P>Sravanthi Rajanala</P>\n</TD></TR><TR><TD><P>Santa Clara University</P>\n</TD>\n<TD><P>Santa Clara University</P>\n</TD>\n<TD><P>Walmart Global Tech</P>\n</TD></TR><TR><TD><P>Santa Clara, USA</P>\n</TD>\n<TD><P>Santa Clara, USA</P>\n</TD>\n<TD><P>Sunnyvale, USA</P>\n</TD></TR><TR><TD><P>xwu5@scu.edu</P>\n</TD>\n<TD><P>zpeng@scu.edu</P>\n</TD>\n<TD><P>sravanthi.rajanala@walmart.com</P>\n</TD></TR></TABLE><H3>Hsin-Tai Wu</H3><P>Docomo Innovations Sunnyvale, USA hwu@docomoinnovations.com</P><H3>ABSTRACT</H3><P>Efective passage retrieval and reranking methods have been widely utilized to identify suitable candidates in open-domain question answering tasks, recent studies have resorted to LLMs for rerank-ing the retrieved passages by the log-likelihood of the question conditioned on each passage. Although these methods have demon-strated promising results, the performance is notably sensitive to the human-written prompt (or hard prompt), and fine-tuning LLMs can be computationally intensive and time-consuming. Further-more, this approach limits the leverage of question-passage rele-vance pairs and passage-specific knowledge to enhance the ranking capabilities of LLMs. In this paper, we propose passage-specific prompt tuning for reranking in open-domain question answer-ing (PSPT1): a parameter-eficient method that fine-tunes learn-able passage-specific soft prompts, incorporating passage-specific knowledge from a limited set of question-passage relevance pairs. The method involves ranking retrieved passages based on the log-likelihood of the model generating the question conditioned on each passage and the learned soft prompt. We conducted extensive experiments utilizing the Llama-2-chat-7B model across three pub-licly available open-domain question answering datasets and the results demonstrate the efectiveness of the proposed approach.</P><P>ACM Reference Format:</P><P>Xuyang Wu, Zhiyuan Peng, Sravanthi Rajanala, Hsin-Tai Wu, and Yi Fang. 2018. Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym ’XX). ACM, New York, NY, USA, 7 pages. https://doi.org/XXXXXX</P><P>X.XXXXXXX</P><P>∗Both authors contributed equally to this research.</P><P>†Yi Fang is the corresponding author.</P><P>1Our code can be found at https://github.com/elviswxy/Gen-IR_PSPT.</P><P>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full ci", "url": "https://arxiv.org/pdf/2405.20654v1", "pdf_url": "https://arxiv.org/pdf/2405.20654v1.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2103.16669v3", "title": "An In-depth Analysis of Passage-Level Label Transfer for Contextual Document Ranking", "authors": ["Koustav Rudra", "Zeon Trevor Fernando", "Avishek Anand"], "abstract": "Pre-trained contextual language models such as BERT, GPT, and XLnet work quite well for document retrieval tasks. Such models are fine-tuned based on the query-document/query-passage level relevance labels to capture the ranking signals. However, the documents are longer than the passages and such document ranking models suffer from the token limitation (512) of BERT. Researchers proposed ranking strategies that either truncate the documents beyond the token limit or chunk the documents into units that can fit into the BERT. In the later case, the relevance labels are either directly transferred from the original query-document pair or learned through some external model. In this paper, we conduct a detailed study of the design decisions about splitting and label transfer on retrieval effectiveness and efficiency. We find that direct transfer of relevance labels from documents to passages introduces label noise that strongly affects retrieval effectiveness for large training datasets. We also find that query processing times are adversely affected by fine-grained splitting schemes. As a remedy, we propose a careful passage level labelling scheme using weak supervision that delivers improved performance (3-14% in terms of nDCG score) over most of the recently proposed models for ad-hoc retrieval while maintaining manageable computational complexity on four diverse document retrieval datasets.", "url": "http://arxiv.org/abs/2103.16669v3", "pdf_url": "https://arxiv.org/pdf/2103.16669v3", "source": "arxiv", "published_date": "2021-03-30T20:28:02Z", "score_bm25": 0.0}, {"paper_id": "2210.15133v1", "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval", "authors": ["Dingkun Long", "Yanzhao Zhang", "Guangwei Xu", "Pengjun Xie"], "abstract": "Pre-trained language model (PTM) has been shown to yield powerful text representations for dense passage retrieval task. The Masked Language Modeling (MLM) is a major sub-task of the pre-training process. However, we found that the conventional random masking strategy tend to select a large number of tokens that have limited effect on the passage retrieval task (e,g. stop-words and punctuation). By noticing the term importance weight can provide valuable information for passage retrieval, we hereby propose alternative retrieval oriented masking (dubbed as ROM) strategy where more important tokens will have a higher probability of being masked out, to capture this straightforward yet essential information to facilitate the language model pre-training process. Notably, the proposed new token masking method will not change the architecture and learning objective of original PTM. Our experiments verify that the proposed ROM enables term importance information to help language model pre-training thus achieving better performance on multiple passage retrieval benchmarks.", "url": "http://arxiv.org/abs/2210.15133v1", "pdf_url": "https://arxiv.org/pdf/2210.15133v1", "source": "arxiv", "published_date": "2022-10-27T02:43:48Z", "score_bm25": 0.0}, {"paper_id": "2204.07496v4", "title": "Improving Passage Retrieval with Zero-Shot Question Generation", "authors": ["Devendra Singh Sachan", "Mike Lewis", "Mandar Joshi", "Armen Aghajanyan", "Wen-tau Yih", "Joelle Pineau", "Luke Zettlemoyer"], "abstract": "We propose a simple and effective re-ranking method for improving passage retrieval in open question answering. The re-ranker re-scores retrieved passages with a zero-shot question generation model, which uses a pre-trained language model to compute the probability of the input question conditioned on a retrieved passage. This approach can be applied on top of any retrieval method (e.g. neural or keyword-based), does not require any domain- or task-specific training (and therefore is expected to generalize better to data distribution shifts), and provides rich cross-attention between query and passage (i.e. it must explain every token in the question). When evaluated on a number of open-domain retrieval datasets, our re-ranker improves strong unsupervised retrieval models by 6%-18% absolute and strong supervised models by up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new state-of-the-art results on full open-domain question answering by simply adding the new re-ranker to existing models with no further changes.", "url": "http://arxiv.org/abs/2204.07496v4", "pdf_url": "https://arxiv.org/pdf/2204.07496v4", "source": "arxiv", "published_date": "2022-04-15T14:51:41Z", "score_bm25": 0.0}, {"paper_id": null, "title": "Multi-view-guided Passage Reranking with Large Language Models", "authors": [], "abstract": "<TITLE>Multi-view-guided Passage Reranking with Large Language Models</TITLE><P>Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 28693–28706 November 4-9, 2025 ©2025 Association for Computational Linguistics</P><P>Multi-view-guided Passage Reranking with Large Language Models</P><P>Jeongwoo Na*, Jun Kwon*, Eunseong Choi, Jongwuk Lee</P><P>Sungkyunkwan University, Republic of Korea {wjddn7946, kwon04210, eunseong, jongwuklee}@skku.edu Abstract</P><P>Recent advances in large language models (LLMs) have shown impressive performance in passage reranking tasks. Despite their suc- cess, LLM-based methods still face challenges in efciency and sensitivity to external biases. (i) Existing models rely mostly on autoregres- sive generation and sliding window strategies to rank passages, which incurs heavy compu- tational overhead as the number of passages increases. (ii) External biases, such as posi- tionorselectionbias,hinderthemodel'sability to accurately represent passages and the input- order sensitivity. To address these limitations, we introduce a novel passage reranking model, called Multi-View-guidedPassage Reranking (MVP). MVP is a non-generative LLM-based reranking method that encodes querypassage informationintodiverseviewembeddingswith- out being inuenced by external biases. For each view, it combines query-aware passage embeddings to produce a distinct anchor vec- tor, used to directly compute relevance scores in a single decoding step. Besides, it employs an orthogonal loss to make the views more distinctive. Extensive experiments demonstrate that MVP, with just 220M parameters, matches the performance of much larger 7B-scale ne- tuned models while achieving a 100× reduction in inference latency. Notably, the 3B-parameter variant of MVP achieves state-of-the-art perfor- mance on both in-domain and out-of-domain benchmarks. The source code is available at https://github.com/bulbna/MVP .</P><P>1 Introduction</P><P>Passage reranking aims to assign ne-grained rel- evance scores to candidate passages  typically retrieved by a rst-stage retriever (Robertson et al., 1994;Karpukhin et al.,2020)  by harnessing the language understanding capabilities of large lan- guage models (LLMs), in both zero-shot and ne-</P><P>*Equal contribution.</P><P>Corresponding author.</P><P>Figure 1: Comparison of latency and nDCG@10 across various reranking models. Latency refers to the time required to rerank for a single query and nDCG@10 is averaged over DL19 and DL20.</P><P>tuned settings. Recent studies (Sun et al.,2023; Liang et al.,2023) formulate a prompt that consists of a query and candidate passages and generate an ordered list of passage identiers in a zero-shot set- ting. Subsequent work has ne-tuned open-source LLMs by distilling knowledge from the teacher model (Pradeep et al.,2023a,b), achieving compet- itive performance. Despite their success, LLM-based reranking methods still face challenges in efciency a", "url": "https://aclanthology.org/2025.emnlp-main.1459.pdf", "pdf_url": "https://aclanthology.org/2025.emnlp-main.1459.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2408.07303v2", "title": "Enhancing Visual Question Answering through Ranking-Based Hybrid Training and Multimodal Fusion", "authors": ["Peiyuan Chen", "Zecheng Zhang", "Yiping Dong", "Li Zhou", "Han Wang"], "abstract": "Visual Question Answering (VQA) is a challenging task that requires systems to provide accurate answers to questions based on image content. Current VQA models struggle with complex questions due to limitations in capturing and integrating multimodal information effectively. To address these challenges, we propose the Rank VQA model, which leverages a ranking-inspired hybrid training strategy to enhance VQA performance. The Rank VQA model integrates high-quality visual features extracted using the Faster R-CNN model and rich semantic text features obtained from a pre-trained BERT model. These features are fused through a sophisticated multimodal fusion technique employing multi-head self-attention mechanisms. Additionally, a ranking learning module is incorporated to optimize the relative ranking of answers, thus improving answer accuracy. The hybrid training strategy combines classification and ranking losses, enhancing the model's generalization ability and robustness across diverse datasets. Experimental results demonstrate the effectiveness of the Rank VQA model. Our model significantly outperforms existing state-of-the-art models on standard VQA datasets, including VQA v2.0 and COCO-QA, in terms of both accuracy and Mean Reciprocal Rank (MRR). The superior performance of Rank VQA is evident in its ability to handle complex questions that require understanding nuanced details and making sophisticated inferences from the image and text. This work highlights the effectiveness of a ranking-based hybrid training strategy in improving VQA performance and lays the groundwork for further research in multimodal learning methods.", "url": "http://arxiv.org/abs/2408.07303v2", "pdf_url": "https://arxiv.org/pdf/2408.07303v2", "source": "arxiv", "published_date": "2024-08-14T05:18:43Z", "score_bm25": 0.0}, {"paper_id": "2204.00797v1", "title": "Improving the Factual Accuracy of Abstractive Clinical Text Summarization using Multi-Objective Optimization", "authors": ["Amanuel Alambo", "Tanvi Banerjee", "Krishnaprasad Thirunarayan", "Mia Cajita"], "abstract": "While there has been recent progress in abstractive summarization as applied to different domains including news articles, scientific articles, and blog posts, the application of these techniques to clinical text summarization has been limited. This is primarily due to the lack of large-scale training data and the messy/unstructured nature of clinical notes as opposed to other domains where massive training data come in structured or semi-structured form. Further, one of the least explored and critical components of clinical text summarization is factual accuracy of clinical summaries. This is specifically crucial in the healthcare domain, cardiology in particular, where an accurate summary generation that preserves the facts in the source notes is critical to the well-being of a patient. In this study, we propose a framework for improving the factual accuracy of abstractive summarization of clinical text using knowledge-guided multi-objective optimization. We propose to jointly optimize three cost functions in our proposed architecture during training: generative loss, entity loss and knowledge loss and evaluate the proposed architecture on 1) clinical notes of patients with heart failure (HF), which we collect for this study; and 2) two benchmark datasets, Indiana University Chest X-ray collection (IU X-Ray), and MIMIC-CXR, that are publicly available. We experiment with three transformer encoder-decoder architectures and demonstrate that optimizing different loss functions leads to improved performance in terms of entity-level factual accuracy.", "url": "http://arxiv.org/abs/2204.00797v1", "pdf_url": "https://arxiv.org/pdf/2204.00797v1", "source": "arxiv", "published_date": "2022-04-02T07:59:28Z", "score_bm25": 0.0}, {"paper_id": "2406.16167v1", "title": "FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy in Large Language Models", "authors": ["Harish Tayyar Madabushi"], "abstract": "We present a novel extension to Retrieval Augmented Generation with the goal of mitigating factual inaccuracies in the output of large language models. Specifically, our method draws on the cognitive linguistic theory of frame semantics for the indexing and retrieval of factual information relevant to helping large language models answer queries. We conduct experiments to demonstrate the effectiveness of this method both in terms of retrieval effectiveness and in terms of the relevance of the frames and frame relations automatically generated. Our results show that this novel mechanism of Frame Semantic-based retrieval, designed to improve Retrieval Augmented Generation (FS-RAG), is effective and offers potential for providing data-driven insights into frame semantics theory. We provide open access to our program code and prompts.", "url": "http://arxiv.org/abs/2406.16167v1", "pdf_url": "https://arxiv.org/pdf/2406.16167v1", "source": "arxiv", "published_date": "2024-06-23T17:18:19Z", "score_bm25": 0.0}, {"paper_id": "2212.09726", "title": "arXiv:2212.09726v2 [cs.CL] 18 Jan 2024", "authors": [], "abstract": "<H1>arXiv:2212.09726v2 [cs.CL] 18 Jan 2024\nImproving Faithfulness of Abstractive Summarization by Controlling Con<TITLE>arXiv:2212.09726v2 [cs.CL] 18 Jan 2024</TITLE>\nImproving Faithfulness of Abstractive Summarization by Controlling Con</H1>founding Effect of Irrelevant Sentences<P>Asish Ghoshal, Arash Einolghozati, Ankit Arun, Haoran Li, Lili Yu, Vera Gor, Yashar Mehdad, Scott Wen-tau Yih and Asli Celikyilmaz Meta AI, Seattle, Washington, USA</P><H3>Abstract</H3><P>Lack of factual correctness is an issue that still plagues state-of-the-art summarization systems despite their impressive progress on generat-ing seemingly fluent summaries. In this pa-per, we show that factual inconsistency can be caused by irrelevant parts of the input text, which act as confounders. To that end, we lever-age information-theoretic measures of causal effects to quantify the amount of confounding and precisely quantify how they affect the sum-marization performance. Based on insights de-rived from our theoretical results, we design a simple multi-task model to control such con-founding by leveraging human-annotated rele-vant sentences when available. Crucially, give a principled characterization of data distributions where such confounding can be large thereby necessitating obtaininig and using human an-notated relevant sentences to generate factual summaries. Our approach improves faithful-ness scores by 20% over strong baselines on ANSWERSUMM (Fabbri et al., 2021), a con-versation summarization dataset where lack of faithfulness is a significant issue due to the sub-jective nature of the task. Our best method achieves the highest faithfulness score while also achieving state-of-the-art results on stan-dard metrics like ROUGE and METEOR. We corroborate these improvements through hu-man evaluation.</P><H3>1 Introduction</H3><P>Large neural language models like BART (Lewis et al., 2020) and T5 (Raffel et al., 2019) have spurred rapid progress on the problem of abstrac-tive summarization. While such models can gen-erate fluent summaries, their practical deployment is hampered by lack of confidence in the factual correctness of generated summaries, where models can hallucinate facts that are not supported by the input document. Generating factual summaries is especially challenging in conversational domains</P><P>Question: Everyfallwhentheweathergetscooleritisagiventhatamouse will end up in my house. I understand that this happens probably to most home owners ... But if I wanted to take some action next summer/early fall to prevent mice from entering, what could I do?</P><P>Answers:</P><OL><LI><P>This is probably a futile exercise. Mice can squeeze through the smallest of gaps so you’d have to virtually hermetically seal your house to prevent any mouse coming in... What you can do is make your house a less welcoming environment. ...</P>\n</LI><LI><P>When I bought my house it had a big problem with mice. ... What I did was: make sure there was no accessible food I checke", "url": "https://arxiv.org/pdf/2212.09726.pdf", "pdf_url": "https://arxiv.org/pdf/2212.09726.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2204.06508", "title": "[2204.06508] FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations", "authors": [], "abstract": "<TITLE>[2204.06508] FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations</TITLE><H1>FactGraph: Evaluating Factuality in Summarization\nwith Semantic Graph Representations</H1><H1>FactGraph</H1><H1>: Evaluating Factuality in Summarization</H1><H1>with Semantic Graph Representations</H1><DIV>Leonardo F. R. Ribeiro</DIV><SUP>†</SUP><DIV>, Mengwen Liu</DIV><SUP>‡</SUP><DIV>, Iryna Gurevych</DIV><SUP>†</SUP><DIV>, Markus Dreyer</DIV><SUP>‡</SUP><DIV>, Mohit Bansal</DIV><SUP>‡,§</SUP><SUP>†</SUP><DIV>UKP Lab, Technical University of Darmstadt</DIV><SUP>‡</SUP><DIV>Amazon Alexa AI,</DIV><SUP>§</SUP><DIV>UNC Chapel Hill<BR>ribeiro@aiphes.tu-darmstadt.de, {mengwliu, mddreyer, mobansal}@amazon.com</DIV><P>gurevych@ukp.informatik.tu-darmstadt.de, mbansal@cs.unc.edu Work done as an intern at Amazon Alexa AI.</P><H6>Abstract</H6><P>Despite recent improvements in abstractive summarization, most current approaches generate summaries that are not <I>factually consistent</I> with the source document, severely restricting their trust and usage in real-world applications. Recent works have shown promising improvements in factuality error identification using text or dependency arc entailments; however, they do not consider the entire semantic graph simultaneously. To this end, we propose FactGraph, a method that decomposes the document and the summary into structured <I>meaning representations</I> (MR), which are more suitable for factuality evaluation. MRs describe core semantic concepts and their relations, aggregating the main content in both document and summary in a canonical form, and reducing data sparsity. FactGraph encodes such graphs using a graph encoder augmented with structure-aware adapters to capture interactions among the concepts based on the graph connectivity, along with text representations using an adapter-based text encoder. Experiments on different benchmarks for evaluating factuality show that FactGraph outperforms previous approaches by up to 15%. Furthermore, FactGraph improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies. <SUP>1</SUP></P><H2>1 Introduction</H2><P>Recent summarization approaches based on pretrained language models (LM) have established a new level of performance Zhang et al. (<A>2020</A>); Lewis et al. (<A>2020</A>), generating summaries that are grammatically fluent and capable of combining salient parts of the source document. However, current models suffer from a severe limitation, generating summaries that are <I>not factually consistent</I>, that is, the content of the summary does not meet the facts of the source document, an issue also known as <I>hallucination</I>. Previous studies Cao et al. (<A>2018</A>); Falke et al. (<A>2019</A>); Maynez et al. (<A>2020</A>); Dreyer et al. (<A>2021</A>) report rates of hallucinations in generated summaries ranging from 30% to over 70%. In the face of such a challenge, recent", "url": "https://ar5iv.labs.arxiv.org/html/2204.06508", "pdf_url": "https://arxiv.org/pdf/2204.06508.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2204.06508v2", "title": "FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations", "authors": ["Leonardo F. R. Ribeiro", "Mengwen Liu", "Iryna Gurevych", "Markus Dreyer", "Mohit Bansal"], "abstract": "Despite recent improvements in abstractive summarization, most current approaches generate summaries that are not factually consistent with the source document, severely restricting their trust and usage in real-world applications. Recent works have shown promising improvements in factuality error identification using text or dependency arc entailments; however, they do not consider the entire semantic graph simultaneously. To this end, we propose FactGraph, a method that decomposes the document and the summary into structured meaning representations (MR), which are more suitable for factuality evaluation. MRs describe core semantic concepts and their relations, aggregating the main content in both document and summary in a canonical form, and reducing data sparsity. FactGraph encodes such graphs using a graph encoder augmented with structure-aware adapters to capture interactions among the concepts based on the graph connectivity, along with text representations using an adapter-based text encoder. Experiments on different benchmarks for evaluating factuality show that FactGraph outperforms previous approaches by up to 15%. Furthermore, FactGraph improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies.", "url": "http://arxiv.org/abs/2204.06508v2", "pdf_url": "https://arxiv.org/pdf/2204.06508v2", "source": "arxiv", "published_date": "2022-04-13T16:45:33Z", "score_bm25": 0.0}, {"paper_id": "2310.10981v3", "title": "Instructive Dialogue Summarization with Query Aggregations", "authors": ["Bin Wang", "Zhengyuan Liu", "Nancy F. Chen"], "abstract": "Conventional dialogue summarization methods directly generate summaries and do not consider user's specific interests. This poses challenges in cases where the users are more focused on particular topics or aspects. With the advancement of instruction-finetuned language models, we introduce instruction-tuning to dialogues to expand the capability set of dialogue summarization models. To overcome the scarcity of instructive dialogue summarization data, we propose a three-step approach to synthesize high-quality query-based summarization triples. This process involves summary-anchored query generation, query filtering, and query-based summary generation. By training a unified model called InstructDS (Instructive Dialogue Summarization) on three summarization datasets with multi-purpose instructive triples, we expand the capability of dialogue summarization models. We evaluate our method on four datasets, including dialogue summarization and dialogue reading comprehension. Experimental results show that our approach outperforms the state-of-the-art models and even models with larger sizes. Additionally, our model exhibits higher generalizability and faithfulness, as confirmed by human subjective evaluations.", "url": "http://arxiv.org/abs/2310.10981v3", "pdf_url": "https://arxiv.org/pdf/2310.10981v3", "source": "arxiv", "published_date": "2023-10-17T04:03:00Z", "score_bm25": 0.0}, {"paper_id": null, "title": "https://assets.amazon.science/6f/89/402bcc6a468eb5eeab80078b9165/factgraph-evaluating-factuality-in-summarization-with-semantic-graph-representations.pdf", "authors": [], "abstract": "<H1>FACTGRAPH: Evaluating Factuality in Summarization with Semantic Graph Representations</H1><P>Leonardo F. R. Ribeiro†∗ , Mengwen Liu‡, Iryna Gurevych†, Markus Dreyer‡, Mohit Bansal‡,§</P><H3>†UKP Lab, Technical University of Darmstadt ‡Amazon Alexa AI §UNC Chapel Hill</H3><P>ribeiro@aiphes.tu-darmstadt.de, {mengwliu, mddreyer, mobansal}@amazon.com gurevych@ukp.informatik.tu-darmstadt.de, mbansal@cs.unc.edu</P><H3>Abstract</H3><P>Despite recent improvements in abstractive summarization, most current approaches gener-ate summaries that are not factually consistent with the source document, severely restricting their trust and usage in real-world applications. Recent works have shown promising improve-ments in factuality error identification using text or dependency arc entailments; however, they do not consider the entire semantic graph simultaneously. To this end, we propose FACT-GRAPH, a method that decomposes the docu-ment and the summary into structured mean-ing representations (MR), which are more suit-able for factuality evaluation. MRs describe core semantic concepts and their relations, ag-gregating the main content in both document and summary in a canonical form, and reduc-ing data sparsity. FACTGRAPH encodes such graphs using a graph encoder augmented with structure-aware adapters to capture interactions among the concepts based on the graph con-nectivity, along with text representations using an adapter-based text encoder. Experiments on different benchmarks for evaluating factuality show that FACTGRAPH outperforms previous approaches by up to 15%. Furthermore, FACT-GRAPH improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies.1</P><H3>1 Introduction</H3><P>Recent summarization approaches based on pre-trained language models (LM) have established a new level of performance (Zhang et al., 2020; Lewis et al., 2020), generating summaries that are grammatically fluent and capable of combining salient parts of the source document. However, cur-rent models suffer from a severe limitation, generat-ing summaries that are not factually consistent, that is, the content of the summary does not meet the</P><P>∗ Work done as an intern at Amazon Alexa AI. 1Our code will be publicly available at https:// github.com/amazon-research/fact-graph</P><P>Mr Mueller was given the role of special (a) [...]</P><P>counsel by the justice department to lead its investigation into alleged Russian interference in last year's US election [...] The NYT has reported that Mr Trump has considered firing Mr Mueller [...].</P><P>(b) US President Donald Trump has said he will fire special counsel Robert Mueller, who is investigating alleged Russian interference in the US election.</P><P>(c) report (d) say :ARG0 :ARG1</P><P>:ARG0</P><P>newspaper :ARG0 consider :ARG1</P><P>Donald :ARG1 Donald Trump :ARG0 Trump</P><P>:ARG0 Robert fire Robert fire Mueller :ARG1 Mueller :ARG1 US US lead :ARG0 :mod :ARG0 :A", "url": "https://assets.amazon.science/6f/89/402bcc6a468eb5eeab80078b9165/factgraph-evaluating-factuality-in-summarization-with-semantic-graph-representations.pdf", "pdf_url": "https://assets.amazon.science/6f/89/402bcc6a468eb5eeab80078b9165/factgraph-evaluating-factuality-in-summarization-with-semantic-graph-representations.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2409.15090", "title": "[2409.15090] Using Similarity to Evaluate Factual Consistency in Summaries", "authors": [], "abstract": "<TITLE>[2409.15090] Using Similarity to Evaluate Factual Consistency in Summaries</TITLE><H1>Computer Science > Computation and Language</H1><DIV>[Submitted on 23 Sep 2024]</DIV><DIV>Using Similarity to Evaluate Factual Consistency in Summaries</DIV><A>Yuxuan Ye</A><DIV>,</DIV><A>Edwin Simpson</A><DIV>,</DIV><A>Raul Santos Rodriguez</A><DIV>View a PDF of the paper titled Using Similarity to Evaluate Factual Consistency in Summaries, by Yuxuan Ye and 2 other authors</DIV><P>Cutting-edge abstractive summarisers generate fluent summaries, but the factuality of the generated text is not guaranteed. Early summary factuality evaluation metrics are usually based on n-gram overlap and embedding similarity, but are reported fail to align with human annotations. Therefore, many techniques for detecting factual inconsistencies build pipelines around natural language inference (NLI) or question-answering (QA) models with additional supervised learning steps. In this paper, we revisit similarity-based metrics, showing that this failure stems from the comparison text selection and its granularity. We propose a new zero-shot factuality evaluation metric, Sentence-BERT Score (SBERTScore), which compares sentences between the summary and the source document. It outperforms widely-used word-word metrics including BERTScore and can compete with existing NLI and QA-based factuality metrics on the benchmark without needing any fine-tuning. Our experiments indicate that each technique has different strengths, with SBERTScore particularly effective in identifying correct summaries. We demonstrate how a combination of techniques is more effective in detecting various types of error.</P><TABLE><TR><TD>Subjects:</TD>\n<TD>Computation and Language (cs.CL)</TD></TR><TR><TD>Cite as:</TD>\n<TD><A>arXiv:2409.15090</A> [cs.CL]</TD></TR><TR><TD> </TD>\n<TD>(or <A>arXiv:2409.15090v1</A> [cs.CL] for this version)</TD></TR><TR><TD> </TD>\n<TD><A>https://doi.org/10.48550/arXiv.2409.15090</A>\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><DIV>From: Yuxuan Ye [</DIV><A>view email</A><DIV>]</DIV><B>[v1]</B><DIV>Mon, 23 Sep 2024 15:02:38 UTC (62 KB)</DIV><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer <I>(<A>What is the Explorer?</A>)</I></P><P>Connected Papers <I>(<A>What is Connected Papers?</A>)</I></P><P>Litmaps <I>(<A>What is Litmaps?</A>)</I></P><P>scite Smart Citations <I>(<A>What are Smart Citations?</A>)</I></P><META description=\"Abstract page for arXiv paper 2409.15090: Using Similarity to Evaluate Factual Consistency in Summaries\"></META>", "url": "https://arxiv.org/abs/2409.15090", "pdf_url": "https://arxiv.org/pdf/2409.15090.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2204.06508", "title": "arXiv:2204.06508v2 [cs.CL] 19 Jul 2022", "authors": [], "abstract": "<H1>arXiv:2204.06508v2 [cs.CL] 19 Jul 2022\nFACTGRAPH: Evaluating Factuality in Summarizat<TITLE>arXiv:2204.06508v2 [cs.CL] 19 Jul 2022</TITLE>\nFACTGRAPH: Evaluating Factuality in Summarizat</H1>ion with Semantic Graph Representations<P>Leonardo F. R. Ribeiroy , Mengwen Liuz, Iryna Gurevychy, Markus Dreyerz, Mohit Bansalz;x</P><P>yUKP Lab, Technical University of Darmstadt zAmazon Alexa AI, xUNC Chapel Hill</P><P>ribeiro@aiphes.tu-darmstadt.de, {mengwliu, mddreyer, mobansal}@amazon.com gurevych@ukp.informatik.tu-darmstadt.de, mbansal@cs.unc.edu</P><H3>Abstract</H3><P>Despite recent improvements in abstractive summarization, most current approaches gener-ate summaries that are not factually consistent with the source document, severely restricting their trust and usage in real-world applications. Recent works have shown promising improve-ments in factuality error identification using text or dependency arc entailments; however, they do not consider the entire semantic graph simultaneously. To this end, we propose FACT-GRAPH, a method that decomposes the docu-ment and the summary into structured mean-ing representations (MR), which are more suit-able for factuality evaluation. MRs describe core semantic concepts and their relations, ag-gregating the main content in both document and summary in a canonical form, and reduc-ing data sparsity. FACTGRAPH encodes such graphs using a graph encoder augmented with structure-aware adapters to capture interactions among the concepts based on the graph con-nectivity, along with text representations using an adapter-based text encoder. Experiments on different benchmarks for evaluating factuality show that FACTGRAPH outperforms previous approaches by up to 15%. Furthermore, FACT-GRAPH improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies.1</P><H3>1 Introduction</H3><P>Recent summarization approaches based on pre-trained language models (LM) have established a new level of performance (Zhang et al., 2020; Lewis et al., 2020), generating summaries that are grammatically fluent and capable of combining salient parts of the source document. However, cur-rent models suffer from a severe limitation, generat-ing summaries that are not factually consistent, that is, the content of the summary does not meet the</P><P>Work done as an intern at Amazon Alexa AI. 1Our code is publicly available at https://github. com/amazon-research/fact-graph</P><P>Mr Mueller was given the role of special (a) [...]</P><P>counsel by the justice department to lead its investigation into alleged Russian interference in last year's US election [...] The NYT has reported that Mr Trump has considered firing Mr Mueller [...].</P><P>(b) US President Donald Trump has said he will fire special counsel Robert Mueller, who is investigating alleged Russian interference in the US election.</P><P>(c) report (d)</P><P>:ARG0 :ARG1 say :ARG0</P><P>newspaper :ARG0 consider :ARG1</P><P>Do", "url": "https://arxiv.org/pdf/2204.06508", "pdf_url": "https://arxiv.org/pdf/2204.06508.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "1807.11618v1", "title": "An Enhanced Latent Semantic Analysis Approach for Arabic Document Summarization", "authors": ["Kamal Al-Sabahi", "Zuping Zhang", "Jun Long", "Khaled Alwesabi"], "abstract": "The fast-growing amount of information on the Internet makes the research in automatic document summarization very urgent. It is an effective solution for information overload. Many approaches have been proposed based on different strategies, such as latent semantic analysis (LSA). However, LSA, when applied to document summarization, has some limitations which diminish its performance. In this work, we try to overcome these limitations by applying statistic and linear algebraic approaches combined with syntactic and semantic processing of text. First, the part of speech tagger is utilized to reduce the dimension of LSA. Then, the weight of the term in four adjacent sentences is added to the weighting schemes while calculating the input matrix to take into account the word order and the syntactic relations. In addition, a new LSA-based sentence selection algorithm is proposed, in which the term description is combined with sentence description for each topic which in turn makes the generated summary more informative and diverse. To ensure the effectiveness of the proposed LSA-based sentence selection algorithm, extensive experiment on Arabic and English are done. Four datasets are used to evaluate the new model, Linguistic Data Consortium (LDC) Arabic Newswire-a corpus, Essex Arabic Summaries Corpus (EASC), DUC2002, and Multilingual MSS 2015 dataset. Experimental results on the four datasets show the effectiveness of the proposed model on Arabic and English datasets. It performs comprehensively better compared to the state-of-the-art methods.", "url": "http://arxiv.org/abs/1807.11618v1", "pdf_url": "https://arxiv.org/pdf/1807.11618v1", "source": "arxiv", "published_date": "2018-07-31T00:50:15Z", "score_bm25": 0.0}, {"paper_id": "2409.19445v1", "title": "HTML-LSTM: Information Extraction from HTML Tables in Web Pages using Tree-Structured LSTM", "authors": ["Kazuki Kawamura", "Akihiro Yamamoto"], "abstract": "In this paper, we propose a novel method for extracting information from HTML tables with similar contents but with a different structure. We aim to integrate multiple HTML tables into a single table for retrieval of information containing in various Web pages. The method is designed by extending tree-structured LSTM, the neural network for tree-structured data, in order to extract information that is both linguistic and structural information of HTML data. We evaluate the proposed method through experiments using real data published on the WWW.", "url": "http://arxiv.org/abs/2409.19445v1", "pdf_url": "https://arxiv.org/pdf/2409.19445v1", "source": "arxiv", "published_date": "2024-09-28T19:58:29Z", "score_bm25": 0.0}, {"paper_id": "2108.01454v2", "title": "Inscriptis -- A Python-based HTML to text conversion library optimized for knowledge extraction from the Web", "authors": ["Albert Weichselbraun"], "abstract": "Inscriptis provides a library, command line client and Web service for converting HTML to plain text. Its development has been triggered by the need to obtain accurate text representations for knowledge extraction tasks that preserve the spatial alignment of text without drawing upon heavyweight, browser-based solutions such as Selenium. In contrast to related software packages, Inscriptis (i) provides a layout-aware conversion of HTML that more closely resembles the rendering obtained from standard Web browsers; and (ii) supports annotation rules, i.e., user-provided mappings that allow for annotating the extracted text based on structural and semantic information encoded in HTML tags and attributes. These unique features ensure that downstream knowledge extraction components can operate on accurate text representations, and may even use information on the semantics and structure of the original HTML document.", "url": "http://arxiv.org/abs/2108.01454v2", "pdf_url": "https://arxiv.org/pdf/2108.01454v2", "source": "arxiv", "published_date": "2021-07-12T12:40:43Z", "score_bm25": 0.0}, {"paper_id": "2406.05348v3", "title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets", "authors": ["Satanu Ghosh", "Neal R. Brodnik", "Carolina Frey", "Collin Holgate", "Tresa M. Pollock", "Samantha Daly", "Samuel Carton"], "abstract": "We explore the ability of GPT-4 to perform ad-hoc schema based information extraction from scientific literature. We assess specifically whether it can, with a basic prompting approach, replicate two existing material science datasets, given the manuscripts from which they were originally manually extracted. We employ materials scientists to perform a detailed manual error analysis to assess where the model struggles to faithfully extract the desired information, and draw on their insights to suggest research directions to address this broadly important task.", "url": "http://arxiv.org/abs/2406.05348v3", "pdf_url": "https://arxiv.org/pdf/2406.05348v3", "source": "arxiv", "published_date": "2024-06-08T04:24:16Z", "score_bm25": 0.0}, {"paper_id": "2011.07964v2", "title": "Learning from similarity and information extraction from structured documents", "authors": ["Martin Holeček"], "abstract": "The automation of document processing is gaining recent attention due to the great potential to reduce manual work through improved methods and hardware. Neural networks have been successfully applied before - even though they have been trained only on relatively small datasets with hundreds of documents so far. To successfully explore deep learning techniques and improve the information extraction results, a dataset with more than twenty-five thousand documents has been compiled, anonymized and is published as a part of this work. We will expand our previous work where we proved that convolutions, graph convolutions and self-attention can work together and exploit all the information present in a structured document. Taking the fully trainable method one step further, we will now design and examine various approaches to using siamese networks, concepts of similarity, one-shot learning and context/memory awareness. The aim is to improve micro F1 of per-word classification on the huge real-world document dataset. The results verify the hypothesis that trainable access to a similar (yet still different) page together with its already known target information improves the information extraction. Furthermore, the experiments confirm that all proposed architecture parts are all required to beat the previous results. The best model improves the previous state-of-the-art results by an 8.25 gain in F1 score. Qualitative analysis is provided to verify that the new model performs better for all target classes. Additionally, multiple structural observations about the causes of the underperformance of some architectures are revealed. All the source codes, parameters and implementation details are published together with the dataset in the hope to push the research boundaries since all the techniques used in this work are not problem-specific and can be generalized for other tasks and contexts.", "url": "http://arxiv.org/abs/2011.07964v2", "pdf_url": "https://arxiv.org/pdf/2011.07964v2", "source": "arxiv", "published_date": "2020-10-17T21:34:52Z", "score_bm25": 0.0}, {"paper_id": "2310.20274v1", "title": "Extracting Entities of Interest from Comparative Product Reviews", "authors": ["Jatin Arora", "Sumit Agrawal", "Pawan Goyal", "Sayan Pathak"], "abstract": "This paper presents a deep learning based approach to extract product comparison information out of user reviews on various e-commerce websites. Any comparative product review has three major entities of information: the names of the products being compared, the user opinion (predicate) and the feature or aspect under comparison. All these informing entities are dependent on each other and bound by the rules of the language, in the review. We observe that their inter-dependencies can be captured well using LSTMs. We evaluate our system on existing manually labeled datasets and observe out-performance over the existing Semantic Role Labeling (SRL) framework popular for this task.", "url": "http://arxiv.org/abs/2310.20274v1", "pdf_url": "https://arxiv.org/pdf/2310.20274v1", "source": "arxiv", "published_date": "2023-10-31T08:43:11Z", "score_bm25": 0.0}, {"paper_id": "2511.22858v1", "title": "RAG System for Supporting Japanese Litigation Procedures: Faithful Response Generation Complying with Legal Norms", "authors": ["Yuya Ishihara", "Atsushi Keyaki", "Hiroaki Yamada", "Ryutaro Ohara", "Mihoko Sumida"], "abstract": "This study discusses the essential components that a Retrieval-Augmented Generation (RAG)-based LLM system should possess in order to support Japanese medical litigation procedures complying with legal norms. In litigation, expert commissioners, such as physicians, architects, accountants, and engineers, provide specialized knowledge to help judges clarify points of dispute. When considering the substitution of these expert roles with a RAG-based LLM system, the constraint of strict adherence to legal norms is imposed. Specifically, three requirements arise: (1) the retrieval module must retrieve appropriate external knowledge relevant to the disputed issues in accordance with the principle prohibiting the use of private knowledge, (2) the responses generated must originate from the context provided by the RAG and remain faithful to that context, and (3) the retrieval module must reference external knowledge with appropriate timestamps corresponding to the issues at hand. This paper discusses the design of a RAG-based LLM system that satisfies these requirements.", "url": "http://arxiv.org/abs/2511.22858v1", "pdf_url": "https://arxiv.org/pdf/2511.22858v1", "source": "arxiv", "published_date": "2025-11-28T03:28:27Z", "score_bm25": 0.0}, {"paper_id": "2510.22344v1", "title": "FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation", "authors": ["Mohammad Aghajani Asl", "Majid Asgari-Bidhendi", "Behrooz Minaei-Bidgoli"], "abstract": "While Retrieval-Augmented Generation (RAG) mitigates hallucination and knowledge staleness in Large Language Models (LLMs), existing frameworks often falter on complex, multi-hop queries that require synthesizing information from disparate sources. Current advanced RAG methods, employing iterative or adaptive strategies, lack a robust mechanism to systematically identify and fill evidence gaps, often propagating noise or failing to gather a comprehensive context. We introduce FAIR-RAG, a novel agentic framework that transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning process. At its core is an Iterative Refinement Cycle governed by a module we term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating mechanism: it deconstructs the initial query into a checklist of required findings and audits the aggregated evidence to identify confirmed facts and, critically, explicit informational gaps. These gaps provide a precise signal to an Adaptive Query Refinement agent, which generates new, targeted sub-queries to retrieve missing information. This cycle repeats until the evidence is verified as sufficient, ensuring a comprehensive context for a final, strictly faithful generation. We conducted experiments on challenging multi-hop QA benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified experimental setup, FAIR-RAG significantly outperforms strong baselines. On HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3 points over the strongest iterative baseline -- establishing a new state-of-the-art for this class of methods on these benchmarks. Our work demonstrates that a structured, evidence-driven refinement process with explicit gap analysis is crucial for unlocking reliable and accurate reasoning in advanced RAG systems for complex, knowledge-intensive tasks.", "url": "http://arxiv.org/abs/2510.22344v1", "pdf_url": "https://arxiv.org/pdf/2510.22344v1", "source": "arxiv", "published_date": "2025-10-25T15:59:33Z", "score_bm25": 0.0}, {"paper_id": "2510.25621v1", "title": "FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering", "authors": ["Mohammad Aghajani Asl", "Behrooz Minaei Bidgoli"], "abstract": "The advent of Large Language Models (LLMs) has revolutionized Natural Language Processing, yet their application in high-stakes, specialized domains like religious question answering is hindered by challenges like hallucination and unfaithfulness to authoritative sources. This issue is particularly critical for the Persian-speaking Muslim community, where accuracy and trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG) systems, relying on simplistic single-pass pipelines, fall short on complex, multi-hop queries requiring multi-step reasoning and evidence aggregation. To address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful Advanced Question Answering in the Persian Islamic domain. FARSIQA is built upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting process: it adaptively decomposes complex queries, assesses evidence sufficiency, and enters an iterative loop to generate sub-queries, progressively filling information gaps. Operating on a curated knowledge base of over one million authoritative Islamic documents, FARSIQA demonstrates superior performance. Rigorous evaluation on the challenging IslamicPCQA benchmark shows state-of-the-art performance: the system achieves a remarkable 97.0% in Negative Rejection - a 40-point improvement over baselines - and a high Answer Correctness score of 74.3%. Our work establishes a new standard for Persian Islamic QA and validates that our iterative, adaptive architecture is crucial for building faithful, reliable AI systems in sensitive domains.", "url": "http://arxiv.org/abs/2510.25621v1", "pdf_url": "https://arxiv.org/pdf/2510.25621v1", "source": "arxiv", "published_date": "2025-10-29T15:25:34Z", "score_bm25": 0.0}]}