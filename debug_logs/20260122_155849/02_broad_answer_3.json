{"summary": "Evidence extraction from retrieved webpages in the context of Retrieval-Augmented Generation (RAG) refers to the process of selecting the most relevant, concise, and high-quality supporting content from retrieved documents before passing it to a language model. This step aims to reduce input redundancy, computational cost, and the risk of hallucination, while improving the faithfulness and helpfulness of generated outputs.\n\nThe recently proposed **SEER (Self-Aligned Evidence Extraction for RAG)** framework addresses limitations of earlier heuristic-based methods, such as poor generalization, semantic loss from rule-based chunking, and skewed evidence length. SEER introduces a model-based, self-aligned learning approach that optimizes a vanilla model to function as an evidence extractor. Instead of relying on hand-crafted filtering rules, SEER trains the extractor to align with desired properties—faithfulness, helpfulness, and conciseness—through self-supervision.\n\nExperimental results show SEER significantly improves RAG performance and reduces evidence length by 9.25×, meaning the model processes far less text while maintaining or improving quality. This approach also demonstrates enhanced generalization across tasks compared to heuristic-based context filtering.\n\nIn practical terms, evidence extraction ensures that only high-value content from retrieved webpages is used in downstream generation, leading to faster inference, reduced computational overhead, and more reliable outputs in applications such as question answering, summarization, and conversational AI.", "problem": null, "key_concepts": ["- **Retrieval-Augmented Generation (RAG):** A paradigm combining information retrieval with generative models to improve accuracy, reliability, and grounding.", "- **Evidence Extraction:** Selecting the most relevant parts of retrieved documents to support the answer while discarding irrelevant or distracting content.", "- **Self-Aligned Learning:** A training method where a model learns to align its outputs with certain desired qualities without external labels, using the model’s own predictions as guidance.", "- **Faithfulness:** The degree to which generated content accurately reflects the source evidence.", "- **Helpfulness:** How well the extracted evidence supports answering the given query.", "- **Conciseness:** Minimizing unnecessary text in extracted evidence while retaining informativeness."], "recent_developments": ["- Shift from **heuristic-based evidence filtering** (manual rules, sentence-level selection) to **model-based extraction** using self-aligned learning.", "- Introduction of SEER, which reduces evidence length by over nine times without sacrificing quality, improving both computational efficiency and generation reliability.", "- Growing emphasis on **faithfulness and grounding** in LLM outputs to mitigate hallucinations.", "- Integration of evidence extraction into RAG pipelines to handle the imperfection of retrieval systems and avoid passing irrelevant content to the model.", "- Trend toward open-source implementations—SEER’s code will be publicly available, encouraging adoption and further innovation."], "authoritative_sources": ["- **Xinping Zhao, Dongfang Li, Yan Zhong, Boren Hu, Yibin Chen, Baotian Hu, Min Zhang** — Authors of SEER, EMNLP 2024.", "- **EMNLP 2024 Proceedings** — Conference publication for SEER (DOI: [10.18653/v1/2024.emnlp-main.178](https://doi.org/10.18653/v1/2024.emnlp-main.178)).", "- **arXiv Preprint (2410.11315)** — Pre-publication version of SEER with detailed methodology and experiments.", "- **GitHub Repository for SEER** (to be released by HITsz-TMG) — Source code for reproducibility and application.", "- Prior work: **FILCO** (Wang et al., 2023) — Early attempt at evidence chunking and filtering in RAG context."], "search_results": [{"title": "[2410.11315] SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation", "url": "https://arxiv.org/abs/2410.11315", "snippet": "<TITLE>[2410.11315] SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation</TITLE><H1>Computer Science > Computation and Language</H1><DIV>[Submitted on 15 Oct 2024]</DIV><H1>SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation</H1><A>Xinping Zhao</A><DIV>,</DIV><A>Dongfang Li</A><DIV>,</DIV><A>Yan Zhong</A><DIV>,</DIV><A>Boren Hu</A><DIV>,</DIV><A>Yibin Chen</A><DIV>,</DIV><A>Baotian Hu</A><DIV>,</DIV><A>Min Zhang</A><DIV>View a PDF of the paper titled SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation, by Xinping Zhao and 6 other authors</DIV><P>Recent studies in Retrieval-Augmented Generation (RAG) have investigated extracting evidence from retrieved passages to reduce computational costs and enhance the final RAG performance, yet it remains challenging. Existing methods heavily rely on heuristic-based augmentation, encountering several issues: (1) Poor generalization due to hand-crafted context filtering; (2) Semantics deficiency due to rule-based context chunking; (3) Skewed length due to sentence-wise filter learning. To address these issues, we propose a model-based evidence extraction learning framework, SEER, optimizing a vanilla model as an evidence extractor with desired properties through self-aligned learning. Extensive experiments show that our method largely improves the final RAG performance, enhances the faithfulness, helpfulness, and conciseness of the extracted evidence, and reduces the evidence length by 9.25 times. The code will be available at <A>this https URL</A>.</P><TABLE><TR><TD>Comments:</TD>\n<TD>15 pages, 6 figures, 5 tables. Accepted by EMNLP 2024 (main)</TD></TR><TR><TD>Subjects:</TD>\n<TD>Computation and Language (cs.CL)</TD></TR><TR><TD>Cite as:</TD>\n<TD><A>arXiv:2410.11315</A> [cs.CL]</TD></TR><TR><TD> </TD>\n<TD>(or <A>arXiv:2410.11315v1</A> [cs.CL] for this version)</TD></TR><TR><TD> </TD>\n<TD><A>https://doi.org/10.48550/arXiv.2410.11315</A>\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><DIV>From: Xinping Zhao [</DIV><A>view email</A><DIV>]</DIV><B>[v1]</B><DIV>Tue, 15 Oct 2024 06:26:24 UTC (4,382 KB)</DIV><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer <I>(<A>What is the Explorer?</A>)</I></P><P>Connected Papers <I>(<A>What is Connected Papers?</A>)</I></P><P>Litmaps <I>(<A>What is Litmaps?</A>)</I></P><P>scite Smart Citations <I>(<A>What are Smart Citations?</A>)</I></P><META description=\"Abstract page for arXiv paper 2410.11315: SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation\"></META>", "source": "bing"}, {"title": "SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation - ACL Anthology", "url": "https://aclanthology.org/2024.emnlp-main.178/", "snippet": "<TITLE>SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation - ACL Anthology</TITLE><A>SEER</A><A>Xinping Zhao</A><DIV>,</DIV><A>Dongfang Li</A><DIV>,</DIV><A>Yan Zhong</A><DIV>,</DIV><A>Boren Hu</A><DIV>,</DIV><A>Yibin Chen</A><DIV>,</DIV><A>Baotian Hu</A><DIV>,</DIV><A>Min Zhang</A><P><B>Important</B>: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See <A>our corrections guidelines</A> if you need to change the PDF.</P><P>Title Adjust the title. Retain tags such as <fixed-case>.</P><P>Abstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>.</P><P>Verification against PDF Ensure that the new title/authors match the snapshot below. (If there is no snapshot or it is too small, consult <A>the PDF</A>.)Authors concatenated from the text boxes above:</P><P>ALL author names match the snapshot above—including middle initials, hyphens, and accents.</P><H5>Abstract</H5><P>Recent studies in Retrieval-Augmented Generation (RAG) have investigated extracting evidence from retrieved passages to reduce computational costs and enhance the final RAG performance, yet it remains challenging. Existing methods heavily rely on heuristic-based augmentation, encountering several issues: (1) Poor generalization due to hand-crafted context filtering; (2) Semantics deficiency due to rule-based context chunking; (3) Skewed length due to sentence-wise filter learning. To address these issues, we propose a model-based evidence extraction learning framework, SEER, optimizing a vanilla model as an evidence extractor with desired properties through self-aligned learning. Extensive experiments show that our method largely improves the final RAG performance, enhances the faithfulness, helpfulness, and conciseness of the extracted evidence, and reduces the evidence length by 9.25 times. The code will be available at https://github.com/HITsz-TMG/SEER.</P><DIV>Anthology ID:<BR>2024.emnlp-main.178<BR>Volume:</DIV><A>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</A><DIV>Month:<BR>November<BR>Year:<BR>2024<BR>Address:<BR>Miami, Florida, USA<BR>Editors:</DIV><A>Yaser Al-Onaizan</A><DIV>,</DIV><A>Mohit Bansal</A><DIV>,</DIV><A>Yun-Nung Chen</A><DIV>Venue:</DIV><A>EMNLP</A><DIV>SIG:<BR>Publisher:<BR>Association for Computational Linguistics<BR>Note:<BR>Pages:<BR>3027–3041<BR>Language:<BR>URL:</DIV><A>https://aclanthology.org/2024.emnlp-main.178/</A><DIV>DOI:</DIV><A>10.18653/v1/2024.emnlp-main.178</A><DIV>Bibkey:<BR>zhao-etal-2024-seer<BR>Cite (ACL):</DIV><P>Xinping Zhao, Dongfang Li, Yan Zhong, Boren Hu, Yibin Chen, Baotian Hu, and Min Zhang. 2024. <A>SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation</A>. In <I>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</I>, pages 3027–3041, Miami, Florida, USA. Association for Computational Linguistics.</P><DIV>Cite (Informal):</", "source": "bing"}, {"title": "https://openreview.net/attachment?id=vsRU4v83B0&name=pdf", "url": "https://openreview.net/attachment?id=vsRU4v83B0&name=pdf", "snippet": "<H1>SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation</H1><H3>Anonymous EMNLP submission</H3><P>001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020</P><P>021</P><P>022</P><P>023</P><P>024</P><P>025</P><P>026</P><P>027</P><P>028</P><P>029</P><P>030</P><P>031</P><P>032</P><P>033</P><P>034</P><P>035</P><P>036</P><P>037</P><P>038</P><P>039</P><P>040</P><P>041</P><H3>Abstract</H3><P>Recent studies in Retrieval-Augmented Gener-ation (RAG) have investigated extracting evi-dence from retrieved passages to reduce com-putational costs and enhance the final RAG per-formance, yet it remains challenging. Existing methods heavily rely on data-level augmenta-tion, encountering several issues: (1) Poor gen-eralization due to hand-crafted context filter-ing; (2) Semantics deficiency due to rule-based context chunking; (3) Skewed length due to sentence-wise filter learning. To address these issues, we propose a model-level evidence ex-traction learning framework, SEER, optimiz-ing a vanilla model as an evidence extractor with desired properties through self-aligned learning. Extensive experiments show that our method largely improves the final RAG per-formance, enhances the faithfulness, helpful-ness, and conciseness of the extracted evidence, and reduces the evidence length by 9.25 times.</P><H3>1 Introduction</H3><P>Recent years have witnessed the prevailing winds of Retrieval-augmented Generation (RAG), which is a prevailing paradigm for improving the perfor-mances of Large Language Models (LLMs) in var-ious downstream tasks, such as question answer-ing, making the output more reliable (Lewis et al., 2020; Chen et al., 2023; Jiang et al., 2023b; Ram et al., 2023), interpretable (Guu et al., 2020; Louis et al., 2024), and adaptable (Xu et al., 2023; Za-kka et al., 2024). Traditional practices (Karpukhin et al., 2020; Min et al., 2019) often involve provid-ing top-retrieved passages as the input context to LLMs without discrimination. However, imperfect retrieval systems frequently yield irrelevant content. Furthermore, indiscriminately feeding all retrieved content to LLMs will cause input redundancy, im-posing a significant computational cost and render-ing them prone to hallucination (Shi et al., 2023). Ideally, LLMs should be grounded on support-ing content that is both highly helpful to address user input and suficiently concise to facilitate infer- ence speed. However, it is practically impossible for imperfect retrieval systems to achieve such an ideal grounding solely (Wang et al., 2023). In fact, top-retrieved passages usually compose supporting and distracting content, inflicting a heavy blow on LLMs trained with high-quality corpora to generate the correct output. This motivates us to develop an evidence extractor, that aims at extracting support- ing content while filtering out distracting content. Recently, a pioneering study, FILCO (Wang et al., 2023), attempts to retrieve chunking doc- ument conte", "source": "bing"}]}