{"summary": "**检索增强生成（Retrieval-Augmented Generation，RAG）流程的内容摘要**  \n\nRAG 是一种将信息检索与大型语言模型（LLM）相结合的技术，用于显著提升生成内容的准确性、时效性和专业性。它的核心思想是“先检索，后生成”，即在 LLM 生成回复前，先从外部知识库检索与用户问题相关的信息，将这些信息作为上下文传递给模型，从而减少知识滞后和事实性错误。  \n\n一个完整的 RAG 流程可分为三个主要阶段：  \n\n---\n\n**1. 知识库预处理（数据准备阶段）**  \n- **数据采集**：从 PDF、Word、网页文本等多源收集领域相关文档。  \n- **文档分割**：将长文档拆分为较短的文本片段（Chunk），常用工具如 *LangChain RecursiveCharacterTextSplitter*，支持自定义片段长度和重叠度。  \n- **向量化处理**：利用嵌入模型（如 OpenAI Embeddings 或 Hugging Face sentence-transformers）将文本片段转为高维向量表示。  \n- **向量存储**：将向量及原文片段存入向量数据库（如 Chroma、Pinecone、FAISS），以便后续高效相似性检索。  \n\n---\n\n**2. 实时查询处理（查询推理阶段）**  \n- **查询转换**：接收用户自然语言问题，使用与文档编码一致的嵌入模型将其转为向量。  \n- **相似性检索**：在向量数据库中检索与问题向量最相似的 Top N 文档片段，可采用混合检索（关键词+向量）策略。  \n- **检索优化**：通过查询扩展、重写、调整返回片段数量等方式提升检索结果的相关性与覆盖度。  \n\n---\n\n**3. 增强生成（生成阶段）**  \n- **提示词工程**：将检索到的片段与用户原始问题组合成 Prompt，设置系统角色、任务说明及生成参数（如 Temperature）。  \n- **上下文驱动生成**：将完整 Prompt 输入 LLM（如 GPT-3.5-turbo、Llama 2），生成基于检索内容的高准确性回答。  \n- **生成优化**：加强内容过滤、事实验证、引用溯源，确保生成内容可追溯且可靠。  \n\n---\n\n**应用场景**：  \n- 企业知识管理（内部问答系统、技术支持、培训平台）  \n- 智能客服（产品信息查询、个性化推荐）  \n- 行业动态追踪（政策、市场变化的实时问答）  \n\n**挑战与解决方案**：  \n- *挑战*：检索结果噪声、知识库更新延迟、提示词工程复杂度高  \n- *解决方案*：混合检索、自动化知识库更新、提示模板优化、引入验证机制  \n\n---\n\n总体而言，RAG 的优势在于：\n1. **实时更新知识**：无需重新训练 LLM，即可通过更新外部知识库获取最新信息。  \n2. **提升事实准确性**：减少 LLM 生成虚假内容的概率。  \n3. **降低落地成本**：相比微调模型，RAG 对数据量和算力要求更低，可快速部署。", "problem": null, "key_concepts": ["- **RAG（Retrieval-Augmented Generation）**：结合信息检索与生成模型的技术流程。", "- **Retriever（检索器）**：从知识库检索相关文档片段。", "- **Encoder（编码器）**：将文本或查询转化为向量表示。", "- **Generator（生成器）**：基于检索结果生成最终内容的 LLM。", "- **Embedding Model（嵌入模型）**：将文本转化为向量的模型，如 OpenAI Embeddings、sentence-transformers。", "- **Vector Database（向量数据库）**：存储和检索向量化数据的数据库，如 Chroma、FAISS、Pinecone。", "- **Prompt Engineering（提示词工程）**：设计输入格式以优化 LLM 输出质量。"], "recent_developments": ["- **技术成熟度提升**：RAG 已从实验阶段进入成熟应用，出现了完整的框架（如 LangChain）封装端到端流程。", "- **组件生态完善**：开源工具（Chroma、sentence-transformers）降低了部署难度。", "- **混合检索流行**：结合关键词匹配与向量检索，以提高召回率和精准度。", "- **多语言支持增强**：嵌入模型和 LLM 支持更多语言，扩大了应用范围。", "- **事实验证机制**：在生成阶段加入引用、溯源功能，提升可信度。"], "authoritative_sources": ["- **腾讯云开发者社区**：《RAG技术工作流程详细指南》《RAG技术深度解析与实践：让LLM拥有实时知识库》", "- **华为云社区**：《RAG技术工作流程详细指南》", "- **LangChain 文档**（用于实现文档分割、检索、生成的 Python 框架）"], "search_results": [{"title": "RAG技术工作流程详细指南-腾讯云开发者社区-腾讯云", "url": "https://cloud.tencent.com/developer/article/2561682", "snippet": "<TITLE>RAG技术工作流程详细指南-腾讯云开发者社区-腾讯云</TITLE><A>社区首页</A><DIV>></DIV><A>专栏</A><DIV>> RAG技术工作流程详细指南</DIV><H1>RAG技术工作流程详细指南</H1><DIV>原创</DIV><DIV>发布于 2025-08-28 05:52:56</DIV><DIV>484 0</DIV><DIV>举报</DIV><DIV>文章被收录于专栏：</DIV><A>ceshiren0001</A><P>检索增强生成（Retrieval-Augmented Generation，简称RAG）是人工智能领域的一项关键技术。它将信息检索与大型语言模型相结合，大大提升了生成内容的准确性和及时性。</P><P>本文将深入解析RAG的工作流程及其核心技术。</P><H2><B>RAG系统架构</B></H2><B>核心组件</B><P>RAG系统包含三个关键模块：</P><UL><LI><B>检索器（Retriever）</B>：从知识库中检索相关文档</LI><LI><B>编码器（Encoder）</B>：将查询和文档转换为向量表示</LI><LI><B>生成器（Generator）</B>：基于检索结果生成最终答案</LI></UL><H3><B>工作流程详解</B></H3><B><H3>第一阶段：知识库预处理</H3></B><P><B>1. 文档加载与分割</B></P><UL><LI>从多种来源（PDF、HTML、文本等）加载文档</LI><LI>使用递归字符分割器将文档划分为适当大小的块</LI></UL><P><B>2. 向量化处理</B></P><UL><LI>使用嵌入模型（如OpenAI Embeddings）将文本转换为向量</LI><LI>建立向量索引以便高效相似性搜索</LI></UL><DIV>代码语言： javascript</DIV><DIV>AI代码解释</DIV><CODE># 示例代码：文档分割与向量化\nfrom langchain.text_splitter  import  RecursiveCharacterTextSplitter\nfrom langchain.embeddings  import  OpenAIEmbeddings\n\ntext_splitter  = RecursiveCharacterTextSplitter( \n    chunk_size =1000, \n    chunk_overlap =200 ) \ndocuments  =  text_splitter.split_documents(docs) \n\nembeddings  = OpenAIEmbeddings() \nvectorstore  =  Chroma.from_documents(documents,  embeddings)</CODE><P>    chunk_size =1000, </P><P>    chunk_overlap =200 ) </P><DIV>documents  =  text_splitter.split_documents(docs) <BR><BR>embeddings  = OpenAIEmbeddings() <BR>vectorstore  =  Chroma.from_documents(documents,  embeddings)</DIV><B><H3>第二阶段：实时查询处理</H3></B><P><B>1. 查询转换</B></P><UL><LI>接收用户原始查询</LI><LI>使用嵌入模型将查询转换为向量表示</LI></UL><P><B>2. 相似性检索</B></P><UL><LI>在向量数据库中执行相似性搜索</LI><LI>返回最相关的文档片段</LI></UL><DIV>代码语言： javascript</DIV><DIV>AI代码解释</DIV><CODE># 示例代码：相似性检索\nretriever  =  vectorstore.as_retriever( \n    search_type =\"similarity\", \n    search_kwargs ={\"k\": 4} ) \nrelevant_docs  =  retriever.get_relevant_documents(user_query)</CODE><P>    search_type =\"similarity\", </P><P>    search_kwargs ={\"k\": 4} ) </P><DIV>relevant_docs  =  retriever.get_relevant_documents(user_query)</DIV><B><H3>第三阶段：增强生成</H3></B><P><B>1. 提示词工程</B></P><UL><LI>构建包含检索内容和用户查询的提示模板</LI><LI>设置系统角色和生成参数</LI></UL><P><B>2. 上下文增强生成</B></P><UL><LI>将检索到的文档作为上下文提供给LLM</LI><LI>生成基于检索内容的准确回答</LI></UL><DIV>代码语言： javascript</DIV><DIV>AI代码解释</DIV><CODE># 示例代码：增强生成\nfrom langchain.chat_models  import  ChatOpenAI\nfrom langchain.schema  import  HumanMessage,  SystemMessage\n\nllm  = ChatOpenAI(temperature =0.7) \n\nprompt  =  f \"\"\"\n基于以下上下文信息：\n {context} \n\n请回答这个问题：\n {question} \"\"\"\n\nresponse  = llm([ SystemMessage(content =\"你是一个有帮助的助手\"), HumanMessage(content =prompt) ])</CODE><P> {context} </P><DIV>请回答这个问题：</DIV><P> {question} \"\"\"</P><DIV>response  = llm([ SystemMessage(content =\"你是一个有帮助的助手\"), HumanMessage(content =prompt) ])</DIV><H2><B>优化策略</B></H2><B>检索优化</B><UL><LI>使用混合搜索策略（关键词+向量）</LI><LI>实现查询扩展和重写</LI><LI>调整检索数量和质量平衡</LI></UL><B>生成优化</B><UL><LI>设计有效的提示模板</LI><LI>实施内容过滤和验证</LI><LI>添加引用和溯源机制</LI></UL><H2><B>应用场景</B></H2><B><H3>企业知识管理</H3></B><UL><LI>内部文档问答系统</LI><LI>技术支持和故障排除</LI><LI>员", "source": "bing"}, {"title": "RAG技术深度解析与实践：让LLM拥有实时知识库-腾讯云开发者社区-腾讯云", "url": "https://cloud.tencent.cn/developer/article/2617737", "snippet": "<TITLE>RAG技术深度解析与实践：让LLM拥有实时知识库-腾讯云开发者社区-腾讯云</TITLE><A>社区首页</A>><A>专栏</A>>RAG技术深度解析与实践：让LLM拥有实时知识库<H1>RAG技术深度解析与实践：让LLM拥有实时知识库</H1>九日大大<BR>关注<BR>发布于 2026-01-14 15:18:42<BR>发布于 2026-01-14 15:18:42<BR>350<BR>举报<P>在大语言模型（LLM）飞速发展的今天，其在自然语言理解、生成等领域展现出强大能力，但同时也存在“知识滞后”“事实性错误”等固有缺陷。检索增强生成（Retrieval-Augmented Generation，简称RAG）技术应运而生，通过将“检索外部知识库”与“LLM生成”相结合，有效弥补了LLM的不足，让生成内容更具准确性、时效性和专业性。本文将从RAG技术的核心原理、工作流程、核心组件出发，结合完整代码示例，带大家从零搭建一个简单的RAG系统。</P><H4>一、RAG技术核心原理</H4><P>RAG的核心思想是“先检索，后生成”：在LLM生成回复之前，先从外部知识库中检索与用户问题相关的信息，将这些信息作为“上下文”与用户问题一起输入给LLM，让LLM基于检索到的准确信息进行回复。其核心价值在于：</P><UL><LI>解决知识滞后：外部知识库可实时更新，无需重新训练庞大的LLM，就能让模型掌握最新信息（如行业动态、政策更新等）；\n提升事实准确性：减少LLM“一本正经地胡说八道”的概率，生成内容有外部知识支撑；\n降低应用成本：相较于微调LLM，RAG无需大量标注数据和高额计算资源，落地门槛更低。</LI></UL><H4>二、RAG技术工作流程</H4><P>一个完整的RAG系统主要包含两大阶段： <B>数据准备阶段</B> 和 <B>查询推理阶段</B>，具体流程如下：</P><OL><LI><B>数据准备阶段</B>：\n数据采集：收集领域相关的文档（如PDF、Word、网页文本等）；  文档分割：将长文档拆分为短文本片段（Chunk），避免因文本过长导致检索不准确；  向量编码：使用嵌入模型（Embedding Model）将文本片段转换为高维向量（Embedding）；  向量存储：将向量片段存入向量数据库（Vector Database），构建检索知识库。\n<OL><LI>数据采集：收集领域相关的文档（如PDF、Word、网页文本等）；\n文档分割：将长文档拆分为短文本片段（Chunk），避免因文本过长导致检索不准确；\n向量编码：使用嵌入模型（Embedding Model）将文本片段转换为高维向量（Embedding）；\n向量存储：将向量片段存入向量数据库（Vector Database），构建检索知识库。</LI></OL>\n<B>查询推理阶段</B>：\n用户提问：用户输入自然语言问题；  问题编码：使用与文档编码相同的嵌入模型，将用户问题转换为向量；  相似检索：在向量数据库中检索与问题向量最相似的Top N文本片段；  prompt构建：将检索到的文本片段作为上下文，与用户问题组合成新的prompt；  生成回复：将新prompt输入LLM，生成基于准确上下文的回复。\n<OL><LI>用户提问：用户输入自然语言问题；\n问题编码：使用与文档编码相同的嵌入模型，将用户问题转换为向量；\n相似检索：在向量数据库中检索与问题向量最相似的Top N文本片段；\nprompt构建：将检索到的文本片段作为上下文，与用户问题组合成新的prompt；\n生成回复：将新prompt输入LLM，生成基于准确上下文的回复。</LI></OL></LI></OL><H4>三、RAG核心组件选型</H4><P>搭建RAG系统需选择合适的核心组件，以下是主流且易上手的组件组合（适合初学者）：</P><TABLE><TR><TH>组件类型</TH>\n<TH>推荐选型</TH>\n<TH>优势</TH></TR><TR><TD>文档分割工具</TD>\n<TD>LangChain RecursiveCharacterTextSplitter</TD>\n<TD>自适应不同文档格式，可自定义片段长度和重叠度</TD></TR><TR><TD>嵌入模型</TD>\n<TD>Hugging Face sentence-transformers/all-MiniLM-L6-v2</TD>\n<TD>轻量高效，开源免费，支持多语言，适合小规模场景</TD></TR><TR><TD>向量数据库</TD>\n<TD>Chroma</TD>\n<TD>轻量级，无需复杂部署，支持内存模式，适合快速验证</TD></TR><TR><TD>大语言模型</TD>\n<TD>OpenAI GPT-3.5-turbo（或开源的Llama 2）</TD>\n<TD>GPT-3.5-turbo生成质量高、调用便捷；Llama 2可本地部署，隐私性好</TD></TR><TR><TD>开发框架</TD>\n<TD>LangChain</TD>\n<TD>封装了RAG全流程组件，降低开发难度，支持多组件灵活集成</TD></TR></TABLE><H4>四、实战：从零搭建RAG系统（附完整代码）</H4><P>本部分将使用上述组件，搭建一个“Python学习知识库”的RAG系统，支持用户查询Python相关知识点。</P><H5>4.1 环境准备</H5><P>首先安装所需依赖库：</P>代码语言： javascript<CODE># 安装LangChain（RAG开发框架） pip install langchain # 安装嵌入模型依赖 pip install sentence-transformers # 安装向量数据库Chroma pip install chromadb # 安装OpenAI SDK（调用GPT模型） pip install openai # 安装文档加载工具（用于加载文本文件） pip install python-dotenv</CODE><H5>4.2 完整代码实现</H5><P>代码分为5个模块：环境配置、数据准备（加载+分割+编码+存储）、检索模块、生成模块、RAG整体流程封装。</P>代码语言： javascript<P><CODE>import os from dotenv import load_dotenv from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.embeddings import HuggingFaceEmbeddings from langchain.vectorstores import Chroma from langchain.chat_models import ChatOpenAI from langchain.prompts import PromptTemplate from langchain.chains import RetrievalQA  # ------", "source": "bing"}, {"title": "RAG技术工作流程详细指南-云社区-华为云", "url": "https://bbs.huaweicloud.com/blogs/459394", "snippet": "<TITLE>RAG技术工作流程详细指南-云社区-华为云</TITLE><A>云社区</A><A>博客</A><DIV>RAG技术工作流程详细指南</DIV><H1>RAG技术工作流程详细指南</H1><DIV>举报</DIV><A>霍格沃兹测试学社</A><DIV>发表于 2025/08/28 20:58:55</DIV><DIV>2.7k+ 0 0</DIV><P>【摘要】 本文解析RAG（检索增强生成）技术的工作流程与核心组件，涵盖知识库预处理、实时查询处理和增强生成三阶段，介绍优化策略、应用场景及最佳实践，为企业构建高效AI问答系统提供全面指导。</P><P>检索增强生成（Retrieval-Augmented Generation，简称RAG）是人工智能领域的一项关键技术。它将信息检索与大型语言模型相结合，大大提升了生成内容的准确性和及时性。</P><P>本文将深入解析RAG的工作流程及其核心技术。</P><H2>RAG系统架构</H2><H3>核心组件</H3><P>RAG系统包含三个关键模块：</P><UL><LI><B>检索器（Retriever）</B>：从知识库中检索相关文档</LI><LI><B>编码器（Encoder）</B>：将查询和文档转换为向量表示</LI><LI><B>生成器（Generator）</B>：基于检索结果生成最终答案</LI></UL><H2>工作流程详解</H2><H3>第一阶段：知识库预处理</H3><OL><LI><B>文档加载与分割</B></LI><UL><LI>从多种来源（PDF、HTML、文本等）加载文档</LI><LI>使用递归字符分割器将文档划分为适当大小的块</LI></UL>\n<LI><B>向量化处理</B></LI><UL><LI>使用嵌入模型（如OpenAI Embeddings）将文本转换为向量</LI><LI>建立向量索引以便高效相似性搜索</LI></UL></OL><CODE># 示例代码：文档分割与向量化\nfrom langchain.text_splitter  import  RecursiveCharacterTextSplitter\nfrom langchain.embeddings  import  OpenAIEmbeddings\n\ntext_splitter  = RecursiveCharacterTextSplitter( \n    chunk_size=1000, \n    chunk_overlap=200 ) \ndocuments  =  text_splitter.split_documents(docs) \n\nembeddings  = OpenAIEmbeddings() \nvectorstore  =  Chroma.from_documents(documents,  embeddings)</CODE><P>    chunk_size=1000, </P><P>    chunk_overlap=200 ) </P><DIV>documents  =  text_splitter.split_documents(docs) <BR><BR>embeddings  = OpenAIEmbeddings() <BR>vectorstore  =  Chroma.from_documents(documents,  embeddings)</DIV><H3>第二阶段：实时查询处理</H3><OL><LI><B>查询转换</B></LI><UL><LI>接收用户原始查询</LI><LI>使用嵌入模型将查询转换为向量表示</LI></UL>\n<LI><B>相似性检索</B></LI><UL><LI>在向量数据库中执行相似性搜索</LI><LI>返回最相关的文档片段</LI></UL></OL><CODE># 示例代码：相似性检索\nretriever  =  vectorstore.as_retriever( \n    search_type=\"similarity\", \n    search_kwargs={\"k\": 4} ) \nrelevant_docs  =  retriever.get_relevant_documents(user_query)</CODE><P>    search_type=\"similarity\", </P><P>    search_kwargs={\"k\": 4} ) </P><DIV>relevant_docs  =  retriever.get_relevant_documents(user_query)</DIV><H3>第三阶段：增强生成</H3><OL><LI><B>提示词工程</B></LI><UL><LI>构建包含检索内容和用户查询的提示模板</LI><LI>设置系统角色和生成参数</LI></UL>\n<LI><B>上下文增强生成</B></LI><UL><LI>将检索到的文档作为上下文提供给LLM</LI><LI>生成基于检索内容的准确回答</LI></UL></OL><CODE># 示例代码：增强生成\nfrom langchain.chat_models  import  ChatOpenAI\nfrom langchain.schema  import  HumanMessage,  SystemMessage\n\nllm  = ChatOpenAI(temperature=0.7) \n\nprompt  =  f\"\"\"\n基于以下上下文信息：\n {context} \n\n请回答这个问题：\n {question} \"\"\"\n\nresponse  = llm([ SystemMessage(content=\"你是一个有帮助的助手\"), HumanMessage(content=prompt) ])</CODE><P> {context} </P><DIV>请回答这个问题：</DIV><P> {question} \"\"\"</P><DIV>response  = llm([ SystemMessage(content=\"你是一个有帮助的助手\"), HumanMessage(content=prompt) ])</DIV><H2>优化策略</H2><H3>检索优化</H3><UL><LI>使用混合搜索策略（关键词+向量）</LI><LI>实现查询扩展和重写</LI><LI>调整检索数量和质量平衡</LI></UL><H3>生成优化</H3><UL><LI>设计有效的提示模板</LI><LI>实施内容过滤和验证</LI><LI>添加引用和溯源机制</LI></UL><H2>应用场景</H2><H3>企业知识管理</H3><UL><LI>内部文档问答系统</LI><LI>技术支持和故障排除</LI><LI>员工培训和学习平台</LI></UL><H3>客户服务</H3><UL><LI>智能客服机器人</LI><LI>产品信息查询</LI><LI>个性化推荐系统</LI></UL><H2>挑战与解决方案</H2><H3>常见挑战</H3><OL><LI>", "source": "bing"}]}