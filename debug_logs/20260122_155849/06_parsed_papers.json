[{"title": "[2410.11315v1] SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation", "authors": ["Not explicitly listed in provided content"], "abstract": "Recent studies in Retrieval-Augmented Generation (RAG) have investigated extracting evidence from retrieved passages to improve generation quality. Existing methods heavily rely on heuristic-based augmentation, which suffers from poor generalization and limited effectiveness. This paper proposes a model-based augmentation method that largely improves RAG performance by enhancing the faithfulness, helpfulness, and conciseness of extracted evidence while reducing computational costs.", "objectives": "The objective of this research is to develop a model-based evidence extraction method for RAG that overcomes the limitations of heuristic-based augmentation, improves QA performance, and generates more faithful, helpful, and concise evidence with reduced computational overhead.", "methodology": "The proposed method, SEER, consists of three key stages: (1) Evidence Extraction Stage using a model-based approach instead of heuristics; (2) Weighting Oracle Scores with a listwise-aware Lambda Preference Optimization (LPO) algorithm that incorporates ranking position signals into preference learning; (3) Evidence Alignment through preference optimization methods (PPO, DPO, LPO) to refine extraction quality. The approach leverages response sampling, expert evaluation of evidence quality, and optimization of the base extractor.", "datasets": "Three benchmark QA datasets are used: Natural Questions (NQ), TriviaQA (TQA), and HotpotQA (abstractive QA). Dataset statistics include size and task metrics, with HotpotQA evaluated on the dev set due to the unavailability of the test set.", "models": "Experiments are conducted using Llama2 and Flan-T5 as generators, with various baseline evidence extraction methods including heuristic-based approaches (e.g., StrInc Heur-based Aug, FILCO, Full, LLM-Embedder, Bge-Ranker).", "evaluation": "Evaluation metrics include Exact Match (EM) and F1 for QA accuracy, as well as Faithfulness (FS), Helpfulness (HS), and Conciseness (CS) scores for evidence quality. Comparisons are made against baseline methods across multiple datasets, and computational efficiency is also assessed.", "results": "SEER consistently outperforms state-of-the-art baselines, achieving average QA accuracy improvements of 2.58% over the 'Full' method while reducing evidence length. It also improves faithfulness, helpfulness, and conciseness scores, with all key settings proving effective. The aligned model shows better generation stability than the base extractor in most cases.", "contributions": "Key contributions include: (1) Proposal of SEER, a model-based evidence extraction framework for RAG; (2) Introduction of Lambda Preference Optimization (LPO) to incorporate ranking signals into preference learning; (3) Demonstration of improved QA performance, evidence quality, and computational efficiency over heuristic-based methods; (4) Comprehensive evaluation across multiple datasets and baselines.", "limitations": "Limitations include: (1) Requirement for domain knowledge to design expert evaluators for evidence quality assessment; (2) Potential overestimation of performance due to reliance on EM and F1 metrics; (3) Current experiments are limited to smaller models, with future work planned for larger models like Llama2-70B.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2410.11315v1.pdf"}, {"title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries", "authors": ["Not explicitly provided in the given content"], "abstract": "Retrieval-augmented generation (RAG) augments large language models (LLMs) by retrieving relevant information to improve generation quality. Existing RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. This work introduces MultiHop-RAG, a novel dataset designed to evaluate and improve multi-hop retrieval and reasoning capabilities in RAG systems.", "objectives": "The objective of this research is to develop and present a benchmark dataset, MultiHop-RAG, specifically designed for evaluating the performance of RAG systems on multi-hop queries, and to assess the effectiveness of current RAG implementations in handling complex retrieval and reasoning tasks.", "methodology": "The methodology involves: (1) Designing multi-hop queries that require reasoning across multiple documents; (2) Using GPT-4 to generate claims, bridge-entities, bridge-topics, and queries; (3) Retrieving relevant documents using RAG methods; (4) Generating answers based on retrieved text; (5) Evaluating the accuracy of responses from different LLMs; (6) Manual examination to ensure data quality and accuracy.", "datasets": "The MultiHop-RAG dataset contains six different types of news articles and is designed to mimic real-world multi-hop RAG scenarios. Each query requires connecting information across multiple sources, with claims, bridge-entities, bridge-topics, and final answers provided. Dataset examples are included in Appendix B.", "models": "GPT-4; ChatGPT; Llama-2-70b-chat-hf; Mixtral-8x7B-Instruct; Claude-2.1", "evaluation": "Evaluation is conducted by measuring response accuracy rates of different LLMs when answering multi-hop queries using retrieved evidence. Metrics include accuracy rates for each model, with manual verification for data quality. Comparative analysis is performed across query types.", "results": "GPT-4 achieved the highest accuracy (0.56 without retrieval, 0.89 with retrieval), followed by Claude-2.1, ChatGPT, Mixtral-8x7B-Instruct, and Llama-2-70b-chat-hf. Results indicate that current RAG implementations are inadequate for effectively retrieving and answering multi-hop queries, though retrieval improves performance significantly.", "contributions": "Introduced MultiHop-RAG, the first benchmark dataset tailored for multi-hop RAG evaluation; Developed a methodology for generating high-quality multi-hop queries and claims using GPT-4; Provided comprehensive evaluation of multiple LLMs on multi-hop retrieval tasks; Highlighted limitations of current RAG systems in complex reasoning scenarios.", "limitations": "The paper notes that current RAG systems still struggle with multi-hop retrieval and reasoning despite improvements with retrieval augmentation. Limitations include reliance on GPT-4 for data generation, which may introduce biases, and the dataset’s focus on news articles, which may limit generalizability to other domains.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2401.15391v1"}, {"title": "Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation", "authors": ["Not specified in provided content"], "abstract": "Retrieval-Augmented Generation (RAG) systems often face limitations in specialized domains such as fintech, where domain-specific ontologies and dense knowledge structures challenge retrieval precision and relevance. This paper proposes an agentic RAG system that integrates hierarchical agents, automatic query enhancement, and encoder-based re-ranking to improve retrieval robustness. Experimental results demonstrate that the agentic RAG system outperforms a baseline in retrieval precision and relevance, albeit with increased latency, suggesting promising directions for complex, domain-specific applications.", "objectives": "To investigate whether a modular, agent-driven RAG pipeline can more effectively navigate specialized fintech knowledge bases compared to a baseline RAG system, improving retrieval accuracy and relevance.", "methodology": "The study designs a hierarchical agent framework incorporating: (1) targeted sub-query generation; (2) acronym resolution; (3) cross-encoder-based document re-ranking; (4) automatic query enhancement with continuous feedback; and (5) broader retrieval sweeps when initial results are insufficient. An enterprise knowledge base is prepared, and an evaluation dataset is constructed using LLM-driven synthetic data generation. The baseline RAG (B-RAG) and agentic RAG (A-RAG) architectures are implemented and compared under controlled experimental conditions.", "datasets": "Specialized internal fintech enterprise knowledge base; evaluation dataset constructed from this knowledge base using LLM-driven synthetic data generation; includes human-curated benchmark questions across categories such as definitional queries.", "models": "Llama-3.1-8B-Instruct as the LLM; embedding model (unspecified name) for retrieval; cross-encoder for document re-ranking.", "evaluation": "Evaluation includes quantitative metrics such as retrieval accuracy (Hit@5), coverage percentage, and semantic accuracy scored via an LLM-judge rubric (scale 1–10). Human-curated benchmarks are used for qualitative assessment. Latency is also measured.", "results": "The A-RAG system achieved a retrieval accuracy of 62.35% compared to the baseline's 54.12%. Coverage and semantic accuracy also improved across categories, with definitional queries showing notable gains. However, latency increased in A-RAG. Qualitative analysis showed better handling of vague queries and improved ontology navigation via sub-query phrasing.", "contributions": "1. Introduction of a hierarchical agent framework for RAG in specialized fintech domains. 2. Integration of automatic query enhancement with continuous feedback. 3. Use of cross-encoder-based document re-ranking to improve precision. 4. Development of a secure, reproducible, enterprise-scale evaluation methodology. 5. Demonstration of measurable improvements in retrieval accuracy and relevance over a baseline system.", "limitations": "1. Increased latency in the agentic RAG system compared to baseline. 2. Reliance on a single LLM and embedding model limits generalization to other architectures. 3. Results are domain-specific to fintech and may not directly transfer to other specialized domains without adaptation.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2510.25518v1"}, {"title": "T-RAG: Lessons from the LLM Trenches", "authors": ["Masoomali Fatehkia", "Ji Kim Lucas", "Sanjay Chawla"], "abstract": "Large Language Models (LLMs) have shown remarkable language capabilities fueling attempts to integrate them into organizational operations and decision-making. This paper presents T-RAG, a retrieval-augmented generation system fine-tuned for domain-specific tasks, and shares lessons learned from its development and deployment.", "objectives": "The objective of the research is to design, implement, and evaluate a domain-specific Retrieval-Augmented Generation (RAG) system, T-RAG, that leverages fine-tuning and contextual enhancements to improve the accuracy and relevance of LLM outputs in organizational settings.", "methodology": "The methodology involves: (1) fine-tuning a base LLM on a domain-specific dataset using parameter-efficient fine-tuning methods such as LoRA; (2) integrating a search and retrieval pipeline with a vector database; (3) incorporating a tree graph context for entity-related queries; (4) iterative prompt engineering; (5) evaluating performance through human-annotated correctness metrics and controlled experiments such as \"Needle in a Haystack\" tests.", "datasets": "The dataset was created from domain-specific documents, with questions generated by human experts and answers aggregated from multiple LLM iterations. Additional structured data such as organizational charts were used for tree graph context experiments.", "models": "Base LLM (Llama-2), fine-tuned LLM using LoRA parameter-efficient fine-tuning, RAG, and T-RAG variants with and without tree context.", "evaluation": "Evaluation was performed using human scoring of correctness (Correct, Correct-Verbose), aggregated results across multiple question sets, and specialized tests like \"Needle in a Haystack\" to assess context retrieval effectiveness. MMLU (5-shot) benchmark was also used for performance comparison between base and fine-tuned models.", "results": "T-RAG with tree context achieved the highest accuracy (100% correct on entity-related questions) compared to RAG and fine-tuned models without retrieval. Tree context significantly improved performance. In \"Needle in a Haystack\" tests, placing relevant context at the top yielded better results. Fine-tuning improved MMLU performance over the base model.", "contributions": "(1) Introduction of T-RAG, a domain-specific RAG system with fine-tuning and tree graph context integration; (2) Empirical evidence of the benefits of tree context for entity-related queries; (3) Insights into prompt engineering and context placement; (4) Comparative evaluation of RAG, fine-tuned LLM, and T-RAG variants.", "limitations": "The paper does not provide detailed quantitative metrics for all experiments, and the dataset is domain-specific, potentially limiting generalizability. The evaluation relies heavily on human scoring, which may introduce subjectivity.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2402.07483v2"}, {"title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Not explicitly provided in the given content"], "abstract": "Existing large language models (LLMs) show exceptional problem-solving capabilities but may struggle with complex multi-hop reasoning tasks when relying solely on internal knowledge. Tree-based search methods depend heavily on internal reasoning steps, limiting their effectiveness. RAG-Star proposes a retrieval-augmented verification framework that integrates both query-aware and answer-aware reward mechanisms to improve reasoning accuracy, outperforming previous retrieval-augmented generation (RAG) and reasoning methods.", "objectives": "The objective is to enhance the reasoning capabilities of LLMs in multi-hop question answering by combining external retrieval with verification mechanisms, thereby overcoming limitations of existing RAG and reasoning approaches.", "methodology": "RAG-Star integrates retrieval-augmented verification with a tree-based search framework. The method retrieves relevant documents from an external corpus, evaluates reasoning paths using both query-aware and answer-aware rewards, and updates search nodes via backpropagation. It reduces expensive rollouts by leveraging reward models and optimizes reasoning through Monte Carlo Tree Search (MCTS) with improved evaluation strategies.", "datasets": "Four multi-hop question answering datasets: HotpotQA, 2WikiMultihopQA, MusiQue, and StrategyQA. For Wikipedia-based datasets, only abstracts from the 2017 dump are used as the retrieval corpus; other datasets use their respective sources.", "models": "LLMs used include Llama-3.1-8B-Instruct and GPT-4o. Baselines include vanilla prompting (direct prompting, Chain-of-Thought, standard RAG) and improved RAG methods (Iterative RAG, Judge-then-retrieve, Generate-then-retrieve).", "evaluation": "Evaluation is conducted across four multi-hop QA datasets using metrics such as Exact Match (EM), Coverage EM (CEM), and F1 score. Comparative experiments are performed against multiple baseline methods, with ablation studies to assess the contribution of each component.", "results": "RAG-Star outperforms baselines by up to 18.98% (Llama-3.1-8B) and 16.19% (GPT-4o) on average across datasets. Ablation studies show that removing query-aware or answer-aware rewards reduces performance, confirming their importance. The method consistently achieves the highest scores in EM, CEM, and F1 across most datasets.", "contributions": "1. Proposes RAG-Star, a novel retrieval-augmented verification framework for multi-hop reasoning. 2. Introduces query-aware and answer-aware reward mechanisms to guide reasoning. 3. Optimizes MCTS search with reduced rollout costs using reward models. 4. Demonstrates significant performance improvements over state-of-the-art RAG and reasoning methods.", "limitations": "Experimental analysis is limited in scope, with potential for broader evaluation across more diverse tasks and datasets.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2412.12881v1"}, {"title": "RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation", "authors": ["Not specified in provided content"], "abstract": "Retrieval-augmented generation (RAG) has shown great promise for knowledge-intensive tasks and information retrieval. However, existing agentic RAG methods often depend on ad-hoc prompt engineering and lack a unified optimization framework. This work introduces RAG-Gym, a comprehensive framework for fine-grained process-level supervision and systematic evaluation of optimization methods for agentic RAG. We present the optimized agent Re2Search++, which surpasses recent methods such as Search-R1 by a relative increase of 3.2% to 11.6% in average F1, and demonstrate its robustness across unseen datasets.", "objectives": "To develop a unified and extensible framework (RAG-Gym) for systematically optimizing agentic RAG methods, reducing reliance on ad-hoc prompt engineering, and improving performance on knowledge-intensive tasks through fine-grained process-level supervision.", "methodology": "The authors introduce RAG-Gym, which formulates knowledge-intensive question answering as a process-level optimization problem. They collect process reward datasets, apply supervised fine-tuning (SFT), direct preference optimization (DPO), and proximal policy optimization (PPO) to train agents. They design and optimize the Re2Search++ agent using trained critics for action selection, and compare it against outcome-supervision-based RL methods. Experiments span zero-shot and actor-tuned settings, with analysis of reward sources, scaling with data, and inference-time search query generation.", "datasets": "Four datasets spanning general and medical domains: HotpotQA, 2WikiMultihopQA, Bamboogle, and MedQA. For some comparisons (e.g., with Search-R1), MedQA is excluded to focus on general-domain tasks.", "models": "Re2Search agent; Re2Search++ optimized agent; baseline models include RAG, ReAct, Search-o1, Search-R1, and R1-Searcher; reward models trained with GPT-4o annotations; Llama-3.1-8B used in some baselines.", "evaluation": "Performance evaluated on Exact Match (EM), F1 score, Accuracy, and Cover Exact Match (CEM). Comparisons made in zero-shot and tuned settings across datasets. Additional evaluation includes reward source quality (agreement with human preferences), scaling with training data, and inference-time search query generation.", "results": "Re2Search++ achieves +3.2% to +11.6% average F1 improvement over recent methods like Search-R1, and +8.5% to +24.7% on unseen datasets. GPT-4o-annotated reward models yield the highest performance and best human preference alignment (85.85% agreement). RL-based outcome supervision methods show overfitting, while process-level optimization in RAG-Gym demonstrates robustness and generalizability.", "contributions": "1) Introduction of RAG-Gym, a unified framework for process-level optimization of agentic RAG. 2) Development of Re2Search++ agent with superior performance over state-of-the-art baselines. 3) Detailed analysis of reward sources, showing GPT-4o annotations as most effective. 4) Systematic evaluation across multiple datasets and domains. 5) Insights into scaling effects and inference-time behavior of agents.", "limitations": "Authors note that rollout-based methods designed for math reasoning did not yield significant gains for complex reasoning and search tasks. RL-based outcome supervision methods suffer from overfitting to in-domain data. The paper does not provide explicit author list in the provided content.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2502.13957v2"}, {"title": "PReGAN: Answer Oriented Passage Ranking with Weakly Supervised GAN", "authors": ["Not specified in provided content"], "abstract": "Beyond topical relevance, passage ranking for open-domain factoid question answering also requires a focus on answerability. This paper proposes a novel ranking approach that explicitly models answerability in addition to relevance, aiming to improve the performance of open-domain QA systems.", "objectives": "The objective of this research is to develop a lightweight, answer-oriented passage ranking method that improves the ranking of passages containing correct answers in open-domain question answering tasks, by explicitly modeling answerability alongside relevance and mitigating noisy training data.", "methodology": "The proposed method, PReGAN, is a weakly supervised generative adversarial network-based ranker. It re-ranks passages retrieved by an initial retrieval component (e.g., BM25) using discriminators to model both relevance and answerability. The methodology involves: (1) retrieving candidate passages using a baseline retriever, (2) applying the PReGAN ranker to re-score and reorder passages, (3) using a BERT-based reader to extract answers from top-ranked passages, and (4) training with weak supervision to reduce noise impact.", "datasets": "The model is evaluated on five commonly-used open-domain QA datasets: Quasar-T, SearchQA, Curated TREC, TriviaQA, and NQ-Sub (Natural Questions subset).", "models": "The study uses BM25 and DPR as baseline retrievers, DSQA as a baseline QA model, and the proposed PReGAN ranker. For answer extraction, a BERT-based reader similar to DPR’s reader is used.", "evaluation": "Evaluation is conducted using ranking metrics such as Hits@N (N=1, 3, 5, 20, 50) to measure the proportion of top-N passages containing answer text, and Exact Match (EM) rate for final answer quality. Comparisons are made against BM25, DSQA, and DPR baselines under consistent retrieval conditions.", "results": "PReGAN consistently outperforms BM25 and DSQA in passage ranking across all datasets, achieving higher Hits@N scores. When combined with a BERT-based reader, PReGAN also improves EM rates compared to baselines. It matches or exceeds DPR performance despite using fewer candidate passages. Experiments show that increasing the initial retrieval size improves large-N Hits but may hurt small-N Hits due to noise.", "contributions": "(1) Introduction of a lightweight, answer-oriented passage ranking method that explicitly models answerability; (2) Use of weakly supervised GAN training to mitigate noisy data; (3) Demonstration that PReGAN can outperform or match state-of-the-art QA systems with fewer candidates; (4) Empirical analysis of retrieval size impact on ranking and answer extraction performance.", "limitations": "Experiments were limited by computational resources, preventing evaluation on larger datasets such as the full Natural Questions. Additionally, increasing the retrieval set size can introduce noise that negatively impacts small-N ranking performance.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2207.01762v1"}, {"title": "[2405.20654] Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models", "authors": ["Not explicitly mentioned in provided text", "Both authors contributed equally"], "abstract": "Effective passage retrieval and reranking methods are widely used in open-domain question answering to identify suitable candidates. However, their performance is highly sensitive to human-written prompts, and fine-tuning large language models (LLMs) is computationally intensive. This paper proposes Passage-Specific Prompt Tuning (PSPT), a parameter-efficient method that fine-tunes learnable passage-specific soft prompts to incorporate passage-specific knowledge from a limited set of question-passage relevance pairs. The method ranks retrieved passages based on the log-likelihood of the model generating the question conditioned on each passage and the learned soft prompt. Experiments demonstrate the effectiveness of PSPT in improving reranking performance.", "objectives": "To develop a parameter-efficient method for improving passage reranking in open-domain QA by reducing reliance on human-written prompts and avoiding full fine-tuning of LLMs.", "methodology": "The proposed PSPT method fine-tunes only a small number of parameters while keeping the LLM's original parameters fixed. It integrates a passage-specific embedding layer with a learnable soft prompt to form a prompt module, which is concatenated with the original input. The model ranks retrieved passages based on the log-likelihood of generating the question given each passage and its learned prompt. The approach is tested on both unsupervised and supervised retrievers, comparing against hard prompts and uniform soft prompts.", "datasets": "The study uses widely adopted open-domain QA datasets: Natural Questions (NQ), SQuAD, and TriviaQA. Retrieved passages are obtained using BM25, MSS, Contriever, DPR, and MSS-DPR retrievers.", "models": "Llama-2-Chat (7B parameters) as the base LLM; baseline retrievers include BM25, MSS, Contriever, DPR, MSS-DPR; comparison models include UPR and UPR-Inst.", "evaluation": "Evaluation is based on reranking performance using Recall@10 (R@10) and Hit Rate@10 (H@10) metrics. Statistical significance tests are applied to compare PSPT against baselines. Experiments include hyperparameter tuning (soft prompt length, initialization, training sample size, and hard negative sample size).", "results": "PSPT consistently outperforms both hard prompt and uniform soft prompt baselines across unsupervised and supervised retrievers. For example, with BM25, PSPT achieves R@10=36.89 and H@10=62.24 versus BM25 baseline R@10=22.01 and H@10=49.94. PSPT shows statistically significant improvements over basic retrievers and UPR-based methods. It is particularly effective for unsupervised retrievers but also improves strong supervised retrievers.", "contributions": "(1) Proposes a novel passage-specific prompt tuning method that is parameter-efficient; (2) Integrates a passage-specific embedding layer with a learnable soft prompt; (3) Demonstrates significant improvements in reranking performance on multiple QA datasets; (4) Shows that converting hard prompts into trainable soft prompts enhances performance; (5) Provides extensive analysis on hyperparameters and training settings.", "limitations": "(1) Performance gains for already strong supervised retrievers are less consistent compared to unsupervised retrievers; (2) Effectiveness is sensitive to parameter changes and prompt length; (3) Requires careful initialization and tuning to achieve optimal results; (4) The method is evaluated only on Llama-2-Chat and may need adaptation for other LLM architectures.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2405.20654.pdf"}, {"title": "Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models", "authors": ["Xuyang Wu", "Zhiyuan Peng", "Sravanthi Rajanala", "Hsin-Tai Wu", "Yi Fang"], "abstract": "Effective passage retrieval and reranking methods are widely used in open-domain question answering tasks. However, their performance is sensitive to human-written prompts and fine-tuning large language models can be computationally expensive. This paper proposes PSPT (Passage-Specific Prompt Tuning), a parameter-efficient method that fine-tunes learnable passage-specific soft prompts, incorporating passage-specific knowledge from a limited set of question-passage relevance pairs. The method ranks retrieved passages based on the log-likelihood of generating the question conditioned on each passage and the learned soft prompt. Experimental results demonstrate the effectiveness of PSPT across multiple open-domain QA datasets.", "objectives": "To develop a parameter-efficient approach for enhancing passage reranking in open-domain question answering by replacing hard prompts with learnable passage-specific soft prompts, thereby improving performance while reducing computational cost.", "methodology": "The proposed PSPT method fine-tunes a small number of parameters for passage-specific soft prompts while keeping the original LLM parameters fixed. Steps include: (1) retrieving top-k passages using BM25, MSS, Contriever, or DPR; (2) integrating a soft prompt with a passage-specific embedding layer to form a learnable prompt module; (3) concatenating the prompt module with the passage; (4) ranking passages based on the log-likelihood of generating the question conditioned on each passage; (5) training on limited question-passage relevance pairs; (6) evaluating improvements over baseline retrievers and reranking methods.", "datasets": "Widely used open-domain QA datasets, following DPR's dataset usage. Specific datasets are not named in the provided text but include multiple benchmark QA datasets for evaluating passage retrieval and reranking performance.", "models": "Llama-2 as the base large language model; BM25, MSS, Contriever, DPR, MSS-DPR as retrievers; PSPT as the proposed reranking model; UPR and UPR-Inst as baseline reranking methods.", "evaluation": "Evaluation metrics include Top-10 Recall (R@10) and Hit Rate (H@10). Comparative analysis is performed across unsupervised and supervised retrievers, with statistical significance indicated for improvements over baselines. Additional experiments analyze the impact of soft prompt length, hard prompt initialization, in-batch hard negative sample size, and training sample size.", "results": "PSPT consistently improves R@10 and H@10 over baseline retrievers and UPR methods, with statistically significant gains across both unsupervised and supervised retrievers. Converting hard prompts into trainable soft prompts enhances performance, and integrating passage-specific embedding layers further boosts ranking quality. Optimal results are achieved with specific soft prompt lengths and training configurations. PSPT is effective even with limited training data.", "contributions": "(1) Introduction of PSPT, a parameter-efficient passage-specific soft prompt tuning method; (2) Demonstration that replacing hard prompts with trainable soft prompts improves reranking performance; (3) Integration of passage-specific embedding layers with soft prompts; (4) Comprehensive evaluation across multiple retrievers and datasets; (5) Analysis of hyperparameters affecting PSPT performance.", "limitations": "(1) Sensitivity of passage-specific module effectiveness to parameter changes; (2) Performance dependent on prompt token length and initialization; (3) Using only certain loss functions (L_point or L_pair) does not yield optimal results; (4) Limited details on dataset composition in the paper.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2405.20654v1.pdf"}, {"title": "An In-depth Analysis of Passage-Level Label Transfer for Contextual Document Ranking", "authors": ["Koustav Rudra", "others (unspecified in provided text)"], "abstract": "Pre-trained contextual language models such as BERT, GPT, and XLNet work effectively for document retrieval tasks. These models are fine-tuned based on query-document pairs to improve retrieval performance. The paper investigates passage-level transfer strategies to enhance document ranking efficiency while maintaining high retrieval accuracy.", "objectives": "To address the limitations of existing BERT-based document ranking approaches by proposing an efficient passage-level transfer method (Qa-DocRank) that reduces inference cost while preserving or improving retrieval performance.", "methodology": "The study compares two transfer learning approaches—Doc-Labelled and Qa-DocRank—using contextual ranking models based on BERT. Qa-DocRank selectively transfers labels from documents to relevant passages, reducing training size and inference time. Multiple aggregation strategies for passage-level scores are evaluated, including MaxP, DecaySumP, and others. Experiments are conducted on multiple datasets with varying passage granularities and training sizes. Cross-domain retrieval performance is also tested.", "datasets": "TREC-DL, Core17, Core19, Robust04, ClueWeb09. Dataset statistics include average query length and number of passages per document. Training sizes range from 2K to 367K queries.", "models": "BERT-CLS; BERT-3S; Doc-Labelled; Qa-DocRank; QL Model; PacrrDrmm. PARADE method is referenced for passage aggregation.", "evaluation": "Evaluation metrics include MAP (Mean Average Precision), nDCG@20 (Normalized Discounted Cumulative Gain at rank 20), and P@20 (Precision at rank 20). Statistical significance is tested using paired t-tests. Experiments cover in-domain and cross-domain retrieval setups, passage granularity variation, and training size impact.", "results": "Qa-DocRank achieves competitive or superior performance compared to baselines, with higher efficiency due to fewer training instances. For example, on TREC-DL, Qa-DocRank achieves nDCG@20 of 0.603 compared to 0.595 for BERT-3S. Performance improves with larger training sizes. Cross-domain results show that aggregation strategy choice impacts retrieval effectiveness. Passage granularity affects model performance differently across datasets.", "contributions": "Introduced Qa-DocRank, a selective passage-level label transfer method that reduces training and inference costs for BERT-based document ranking. Provided a comparative analysis of multiple aggregation strategies. Demonstrated robustness across datasets, passage granularities, and training sizes. Offered insights into cross-domain retrieval performance.", "limitations": "High inference cost remains an open challenge for BERT-based models. No parallel optimization techniques were applied, which could improve runtime. Cross-domain retrieval performance is dependent on aggregation strategy selection, which may not generalize well without prior knowledge of the test dataset.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2103.16669v3"}, {"title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval", "authors": ["Not explicitly provided in the given content"], "abstract": "Pre-trained language models (PTMs) have shown strong capabilities in generating text representations for dense passage retrieval. This paper proposes a new token masking method that does not alter the architecture or learning objective of the original PTM. Experiments verify that the proposed method significantly improves passage retrieval performance.", "objectives": "To investigate and improve the token masking strategy used in PTM pre-training for dense passage retrieval, addressing the limitations of the conventional random masking approach which often focuses on stop words and punctuation.", "methodology": "The paper introduces a Retrieval Oriented Masking (ROM) method that adjusts token masking probability based on term weights, aiming to mask more informative tokens rather than stop words or punctuation. The method retains the basic uniform random probability to preserve the original MLM training characteristics. Term weights are computed using both unsupervised (BPROP) and supervised (DeepImpact) approaches. The ROM method is integrated into the PTM pre-training process without changing the architecture or loss function.", "datasets": "MS MARCO Passage Ranking dataset and Natural Questions dataset are used for evaluation. MS MARCO is a large-scale passage ranking benchmark, while Natural Questions is a QA dataset designed for retrieval tasks.", "models": "BM25 (baseline), DeepCT, DocT5Query, GAR, Condenser, coCondenser, ROM, coROM. ROM and coROM are the proposed models incorporating retrieval oriented masking.", "evaluation": "Evaluation follows prior work methodology (Gao and Callan, 2022) using metrics such as MRR@10, R@1000, R@5, R@20, and R@100. Statistical significance is tested using t-tests (p &lt; 0.05). Experiments are conducted on 8 NVIDIA Tesla V100 GPUs (32GB).", "results": "ROM achieves the best performance among all baselines, with MRR@10 of 37.3 on MS MARCO and improvements across all recall metrics. coROM further improves results to MRR@10 of 39.1. High-quality term weight computation (DeepImpact) slightly improves over BPROP. The method demonstrates statistically significant gains over baselines.", "contributions": "1) Identification of the suboptimal nature of random token masking in PTM pre-training for dense retrieval. 2) Proposal of Retrieval Oriented Masking (ROM) to prioritize masking informative tokens. 3) Integration of unsupervised and supervised term weight computation methods into ROM. 4) Empirical validation on two major benchmarks showing significant improvements over state-of-the-art baselines.", "limitations": "Authors did not compare ROM with other unsupervised term weight computation methods beyond BPROP. The supervised DeepImpact method introduces extra training cost. Further detailed studies on term weight distribution in ROM could yield deeper insights.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2210.15133v1"}, {"title": "Improving Passage Retrieval with Zero-Shot Question Generation", "authors": ["Not explicitly provided in the given text"], "abstract": "We propose a simple and effective re-ranking method for improving passage retrieval in open question answering. The re-ranker re-scores retrieved passages with a zero-shot question generation approach on top of any retrieval method (e.g., neural or keyword-based), without requiring any domain- or task-specific training. This method achieves new state-of-the-art results on full open-domain question answering by simply adding the new re-ranker to existing models with no further changes.", "objectives": "To develop an unsupervised passage re-ranking (UPR) method that improves the accuracy of passage retrieval in open-domain question answering systems without requiring domain-specific training.", "methodology": "The proposed UPR method uses pre-trained language models (PLMs) to perform zero-shot question generation conditioned on the query and passage, estimating relevance scores to re-rank retrieved passages. It can be applied on top of any retrieval method (sparse or dense) and leverages cross-attention between query and passage. The approach involves: (1) retrieving passages using existing retrievers (BM25, dense retrievers, etc.), (2) generating questions from passages using PLMs, (3) scoring passages based on generated questions, and (4) re-ranking them to improve retrieval accuracy.", "datasets": "Natural Questions (NQ), WebQuestions (WebQ), SQuAD-Open, Entity Questions, BEIR benchmark (including FIQA-2018, Trec-covid, Touche-2020, MS-Marco, HotpotQA, Fever, Climate-fever), MS MARCO dataset for supervised transfer experiments.", "models": "BM25, MSS, DPR, MSS-DPR, Contriever, SPAR; Pre-trained language models including T0-3B, T0-11B, T5 (3B), GPT-neo (2.7B), GPT-j (6B), T5-lm-adapt (250M), monoT5 (250M, 800M, 3B).", "evaluation": "Evaluation is based on top-K retrieval accuracy (Top-1, Top-5, Top-20, Top-100), nDCG@10, Recall@100 across multiple datasets. Comparisons are made between baseline retrievers and retrievers with UPR, as well as against supervised re-rankers (monoT5). Ablation studies assess the impact of question generation and instruction prompting.", "results": "UPR consistently improves retrieval accuracy across all tested retrievers, achieving gains of 8–20% in top-20 accuracy and 4–10% in top-100 accuracy. On SQuAD-Open and Entity Questions, UPR outperforms BM25 by 14% and 8%, respectively. In BEIR benchmark, UPR improves NDCG@10 by 3–8% and Recall@100 by 5–6%. In open-domain QA with FiD models, UPR improves EM scores by 1–3 points, achieving new state-of-the-art results without supervised training.", "contributions": "Introduces UPR, a zero-shot, unsupervised passage re-ranking method applicable to any retriever; Demonstrates significant performance improvements across diverse datasets and retrieval methods; Shows that UPR can match or surpass supervised re-rankers without requiring labeled data; Provides comprehensive evaluation and ablation studies highlighting the role of question generation and PLM scaling.", "limitations": "UPR performance may be sensitive to the training data of the PLM, potentially affecting results in domain-specific retrieval tasks; Re-ranked results may introduce bias or discrimination against certain communities; Performance drops observed in datasets with statement-like queries (e.g., Touche-2020); Further exploration needed for optimizing the number of top-K documents to re-rank in specific datasets.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2204.07496v4"}, {"title": "Multi-view-guided Passage Reranking with Large Language Models", "authors": ["Not specified in provided content"], "abstract": "Recent advances in large language models (LLMs) have shown impressive performance in passage reranking. However, LLM-based methods still face challenges in inefficiency and sensitivity to external biases. This paper proposes a reranking method that encodes query–passage information into diverse view embeddings without being influenced by external biases, enabling efficient and robust global ranking.", "objectives": "The objective of this research is to design an efficient and effective passage reranking model that mitigates external biases, supports global ranking in a single step, and achieves high performance while reducing computational cost compared to existing LLM-based listwise rerankers.", "methodology": "The proposed method, MVP (Multi-View-guided Passage reranking), uses a non-generative architecture that employs anchor vectors and multi-view encoding for listwise relevance scoring. Each query–passage pair is encoded independently into multiple orthogonal views, and scores are aggregated across views to produce a final ranking. Orthogonality regularization is applied to anchor vectors to ensure diversity among views. The approach avoids reliance on autoregressive generation, enabling faster inference and robustness to position and selection biases.", "datasets": "The experiments use TREC Deep Learning tracks (DL19, DL20) and BEIR benchmark datasets, including Covid, NFCorpus, Signal, and News. Initial candidate passages are retrieved using BM25.", "models": "Proposed model: MVP (220M and 3B parameter variants, based on T5-base). Baseline models include MonoT5 (3B), RankT5 (3B), FIRST (7B), PE-Rank (7B), RankVicuna (7B), RankZephyr, and ListT5.", "evaluation": "Evaluation is conducted using nDCG@10 on top-100 passages retrieved via BM25. Efficiency is measured in FLOPs and latency. Robustness is tested under candidate order permutations (original, shuffled, reversed) and identifier order changes. Ablation studies assess the impact of orthogonality regularization, multi-view encoding, and aggregation strategies.", "results": "MVP outperforms baseline models across TREC and BEIR benchmarks, achieving state-of-the-art results with only 220M parameters, matching or surpassing 7B-scale models. It demonstrates robustness to position and selection biases, with no performance degradation under candidate order permutations. The 3B variant achieves the highest performance overall. MVP reduces inference latency by up to 100× compared to generative listwise models.", "contributions": "1. Introduction of MVP, a non-generative multi-view-guided passage reranking model. 2. Orthogonal anchor vector design for diverse view embeddings. 3. Single-step global ranking capability. 4. Robustness to external biases, including position and selection biases. 5. Significant efficiency improvements over generative listwise rerankers while maintaining or improving accuracy.", "limitations": "MVP employs a fixed number of views across scenarios, which may limit flexibility in adapting to tasks requiring variable view configurations.", "figures": null, "tables": null, "url": "https://aclanthology.org/2025.emnlp-main.1459.pdf"}, {"title": "Enhancing Visual Question Answering through Ranking-Based Hybrid Training and Multimodal Fusion", "authors": ["Not specified"], "abstract": "Visual Question Answering (VQA) is a challenging task that requires systems to provide accurate answers to questions about images. This work introduces a ranking-based hybrid training strategy that significantly enhances the model's ability to handle complex questions by effectively integrating high-quality visual and semantic text features.", "objectives": "The objective of this research is to improve VQA model performance, particularly for complex questions, by introducing a ranking-based hybrid training strategy that integrates visual and textual features more effectively than traditional methods.", "methodology": "The proposed RankVQA model employs a ranking learning module to optimize the relative ranking of candidate answers, combined with a hybrid training strategy that merges classification loss and ranking loss. The methodology includes: (1) visual feature extraction using CNNs and Region Proposal Networks; (2) textual feature encoding using Transformer-based models (e.g., BERT); (3) multimodal fusion via multi-head self-attention; (4) ranking learning to prioritize correct answers; and (5) hybrid training to jointly optimize classification and ranking objectives.", "datasets": "Multiple standard VQA datasets were used, including VQA v2.0 and COCO-QA, for training and evaluation. These datasets contain diverse question types such as Yes/No, Number, and Open-ended questions, paired with corresponding images.", "models": "RankVQA (proposed), compared against state-of-the-art models including BUTD, BAN, MFH, BAN+Counter, v-AGCN, Lite-mdetr, VLT, and VinVL. Related work also references CNN-based visual feature extractors, RNN-based text understanding, Transformer architectures (BERT, GPT), Graph Neural Networks, and dynamic fusion methods.", "evaluation": "Evaluation was performed using accuracy and Mean Reciprocal Rank (MRR) metrics across VQA v2.0 and COCO-QA datasets. Comparative experiments with state-of-the-art models and ablation studies were conducted to assess the impact of each component. Qualitative analysis included attention map visualizations to demonstrate the model's focus on relevant image regions.", "results": "RankVQA achieved superior performance across datasets, with 72.30% accuracy and 0.76 MRR on COCO-QA, and 71.82% accuracy on VQA v2.0 test-std, outperforming all baseline models. Ablation studies confirmed the importance of ranking-based training, multimodal fusion, and multi-head self-attention. Visualizations showed effective attention to critical image regions, enhancing complex question answering capabilities.", "contributions": "1) Introduction of a ranking learning module to optimize answer ranking and improve accuracy. 2) Development of a hybrid training strategy combining classification and ranking losses. 3) Effective multimodal fusion leveraging CNNs, Transformers, and self-attention mechanisms. 4) Demonstration of state-of-the-art performance on multiple VQA datasets. 5) Comprehensive evaluation including quantitative and qualitative analyses.", "limitations": "High computational complexity due to attention mechanisms and multimodal fusion, which may limit scalability. Challenges remain in interpreting complex scenes and abstract concepts. Future work should explore architectural optimizations, integration of external knowledge sources, and balancing performance with resource consumption.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2408.07303v2"}, {"title": "Improving the Factual Accuracy of Abstractive Clinical Text Summarization using Multi-Objective Optimization", "authors": ["Amanuel Alambo", "Tanvi Banerjee", "Kri"], "abstract": "While there has been recent progress in abstractive summarization as applied to different domains including news articles, scientific articles, and blogs, this paper presents a framework for improving the factual accuracy of abstractive summarization of clinical text using knowledge-guided multi-objective optimization.", "objectives": "To develop and evaluate a framework that improves the factual accuracy of abstractive summarization in clinical text by incorporating knowledge-guided multi-objective optimization techniques.", "methodology": "The proposed approach uses a transformer encoder-decoder network fine-tuned with multi-objective optimization. The methodology includes: (1) extracting named entities and domain-specific knowledge from clinical text using medical ontologies (UMLS, RadLex) and the Stanza package; (2) generating entity chains from findings; (3) training models with different combinations of objectives—generative loss, entity chain loss, and knowledge loss; (4) fine-tuning state-of-the-art pre-trained models on clinical datasets; (5) evaluating performance against ground truth impressions.", "datasets": "15183 de-identified procedure notes spanning 4 years (May 2016 - August 2020); 6182 patient records for heart failure (HF) dataset; MIMIC-CXR dataset with 40,000 training, 5,000 validation, and 5,000 test samples.", "models": "Transformer-based encoder-decoder models; state-of-the-art pre-trained abstractive summarization models; ontology-aware content selector integrated with medical ontologies (UMLS, RadLex).", "evaluation": "Evaluation against ground truth impressions using factual accuracy metrics such as Precision-target and Recall-target; comparison of dual multi-objective optimization (generative loss + entity chain loss) and triple multi-objective optimization (generative loss + entity chain loss + knowledge loss).", "results": "Joint optimization of generative loss with entity chain and knowledge-based objectives significantly improved factual accuracy over baseline training settings. Triple multi-objective optimization yielded better performance than dual optimization. Named entities contributed substantially to semantic accuracy in clinical summaries.", "contributions": "1) A novel framework for improving factual accuracy in clinical abstractive summarization using knowledge-guided multi-objective optimization; 2) Integration of medical ontologies for content selection; 3) Empirical evaluation on large-scale clinical datasets; 4) Demonstration of performance gains through multi-objective training strategies.", "limitations": "The proposed approach is computationally expensive due to the complexity of multi-objective optimization and integration of ontology-based content selection.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2204.00797v1"}, {"title": "FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy in Large Language Models", "authors": ["Not specified in provided content"], "abstract": "We present a novel extension to Retrieval Augmented Generation (RAG) aimed at mitigating factual errors in large language models. Our method leverages the cognitive linguistic theory of frame semantics for indexing and retrieving factual information. We evaluate the method in terms of retrieval effectiveness and the relevance of automatically generated frames and frame relations. Results show that Frame Semantic-based Retrieval (FS-RAG) improves RAG performance and offers interpretability.", "objectives": "To design and evaluate a novel retrieval mechanism based on frame semantics that improves factual accuracy and interpretability in Retrieval Augmented Generation systems.", "methodology": "The proposed FS-RAG method consists of three stages: (1) Frame Identification — determining the frames associated with facts or questions; (2) Frame Relation Identification — finding logically connected frames; and (3) Retrieval based on frame semantics. The process uses prompting (with GPT-4) to generate frames and frame relations, followed by manual refinement and iterative model training to improve accuracy and scalability.", "datasets": "No specific datasets are named; experiments are conducted in a closed-domain question answering setting with between 15 and 25 distractors.", "models": "GPT-4 is used for prompting to generate frames and frame relations; baselines include traditional keyword-based retrieval (RAKE) and dense vector-based retrieval.", "evaluation": "Empirical evaluation compares FS-RAG to two baselines using Recall@k for k ∈ {35, 40, 45}. Both qualitative and quantitative analyses are conducted to assess retrieval effectiveness and interpretability.", "results": "FS-RAG outperforms both baselines across all tested k values: @35 (0.330, 0.385, 0.439), @40 (0.333, 0.390, 0.464), @45 (0.338, 0.396, 0.473). The method demonstrates feasibility, improved retrieval accuracy, and interpretability, though results are not perfect.", "contributions": "Introduced a novel frame semantic-based retrieval mechanism for RAG; demonstrated improved retrieval accuracy over traditional methods; showed that the method is interpretable; provided a proof of concept for scaling frame semantics in retrieval tasks; proposed a process that approximates reasoning steps to retrieve logically connected facts.", "limitations": "Effectiveness depends on accurate frame identification, which GPT-4 does not always achieve; results are far from perfect; experiments are limited to a closed-domain QA setting; broader evaluation across multiple tasks is needed; current approach relies heavily on prompting and manual refinement.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2406.16167v1"}, {"title": "arXiv:2212.09726v2 [cs.CL] 18 Jan 2024", "authors": ["Asish Ghoshal", "Arash Einolghozati", "Ankit Arun", "Haoran Li"], "abstract": "Lack of factual correctness is an issue that still plagues state-of-the-art summarization systems. Derived from theoretical results, the authors design a simple multi-task model to control confounding effects by leveraging human-annotated relevant sentences. Their best method achieves the highest faithfulness score while also obtaining state-of-the-art results on standard metrics like ROUGE and METEOR, corroborated through human evaluation.", "objectives": "To improve the faithfulness of abstractive summarization systems by controlling the confounding effect of irrelevant sentences, thereby reducing hallucinations and enhancing factual correctness.", "methodology": "The authors propose a supervised extractive-abstractive summarization approach called SURE-Summ. The methodology involves: (1) Relevant sentence extraction using human-annotated data; (2) Abstractive generation trained only on relevant sentences to reduce confounding from irrelevant content; (3) A unified multi-task seq2seq model that jointly learns extraction and generation; (4) Theoretical justification using causal generative modeling to quantify the effect of irrelevant sentences on summarization performance; (5) Post-processing of extractive summaries before abstractive generation.", "datasets": "The ANSWER SUMM dataset (Fabbri et al., 2021) containing 3,131 training examples, 500 validation examples, and human-annotated relevant sentences for community question answering summarization tasks.", "models": "Direct models: BART, T5, PEGASUS; Supervised extractive-abstractive models: SURE-Summ (RoBERTa-BART multi-task), Supervised RoBERTa-BART; Oracle BART trained and inferred with gold relevant sentences.", "evaluation": "Automatic evaluation using ROUGE-1/2/L, METEOR, BARTScore, log-likelihood, and faithfulness metrics. Human evaluation along five axes: faithfulness, coherence, relevance, summary quality, and length. Statistical significance testing (p-value = 0.05) for improvements. Comparison between top-50 and bottom-50 examples based on confounding levels.", "results": "SURE-Summ improved faithfulness scores by 20% over vanilla BART while matching ROUGE performance. Achieved best test log-likelihood scores among strong baselines. Human evaluation showed SURE-Summ matched or improved faithfulness over BART in 72% of examples. Multi-task objective improved relevant sentence selection F1 score by 6.9% compared to RoBERTa baseline.", "contributions": "1. Identification and quantification of confounding effects from irrelevant sentences in abstractive summarization. 2. Theoretical causal modeling to justify extractive-abstractive approaches. 3. Development of SURE-Summ, a supervised multi-task extractive-abstractive model that improves faithfulness without sacrificing standard summarization metrics. 4. Empirical validation through both automatic and human evaluation. 5. Demonstration that controlling irrelevant sentence effects leads to significant faithfulness gains.", "limitations": "The approach relies on human-annotated relevant sentences for training, which may not be available for all datasets. While faithfulness improves, the method does not fully eliminate hallucinations. Further research is needed to address faithfulness in fully unsupervised or low-resource settings.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2212.09726.pdf"}, {"title": "arXiv:2204.06508v2 [cs.CL] 19 Jul 2022", "authors": ["Not explicitly provided in the given content"], "abstract": "Despite recent improvements in abstractive summarization, most current approaches generate summaries that are not factually consistent with the source text. FACTGRAPH is introduced as a method that decomposes both the document and the summary into structured meaning representations (MR), specifically Abstract Meaning Representation (AMR), which are more suitable for factuality evaluation. By explicitly modeling semantic relations and abstracting away from surface text forms, FACTGRAPH reduces the negative influence of diverse text variations and improves factuality detection in generated summaries.", "objectives": "The objective of this research is to improve the detection of factual inconsistencies in abstractive summarization by leveraging structured semantic graph representations, specifically AMR, to better capture meaning and relations beyond surface-level text.", "methodology": "FACTGRAPH employs a dual encoding approach where both the source document and the generated summary are converted into semantic graphs using AMR parsing. These graphs abstract away from syntax and lexical variations, enabling direct semantic comparison. The methodology involves: (1) parsing documents and summaries into AMRs; (2) aligning and comparing semantic relations; (3) training factuality evaluation models using adapter-based fine-tuning rather than full model fine-tuning; (4) conducting experiments on multiple summarization datasets to evaluate performance against existing methods.", "datasets": "The study uses multiple datasets including CNN/DailyMail, XSum, and the FACTCOLLECT dataset (9,567 datapoints split into train: 8,667, dev: 500, test: 400). FACTCOLLECT contains human-annotated factuality labels for summaries. Synthetic data derived from these datasets is also used for evaluation.", "models": "Models compared include QAGS (QA-based), QUALS (QA-based), FACTCC, FACTCC+, DAE (Dependency Arc Evaluation), and the proposed FACTGRAPH (with and without pretrained structural adapters). FACTGRAPH variants include edge-level and sentence-level models, as well as AMR-based and dependency-based graph representations.", "evaluation": "Evaluation is conducted using balanced accuracy (BACC), F1 score, Pearson and Spearman correlation with human judgments, and various text similarity metrics (BLEU, METEOR, ROUGE-L, BERTScore, SMATCH-AMR). Comparisons are made against baseline models on CNN/DailyMail, XSum, and FACTCOLLECT datasets. Ablation studies and efficiency comparisons are also performed.", "results": "FACTGRAPH consistently outperforms baseline methods in factuality evaluation, achieving the highest BACC and F1 scores across datasets (e.g., CNN/DM: BACC 86.3, F1 86.7; XSum: BACC 73.0, F1 86.8). It shows stronger correlation with human judgments compared to QA-based and dependency-based methods, and is more effective for highly abstractive summaries. Adapter-based training achieves near full fine-tuning performance while using only 1.4% of parameters. AMR-based representations yield the best results in capturing semantic relations and detecting hallucinations.", "contributions": "Key contributions include: (1) Introduction of FACTGRAPH, a novel graph-based factuality evaluation method using AMR; (2) Demonstration that structured semantic representations improve factuality detection in abstractive summarization; (3) Development of an adapter-based training approach for efficiency; (4) Comprehensive evaluation across multiple datasets and baselines; (5) Empirical evidence that AMR abstractions outperform surface-level text comparisons for factuality tasks.", "limitations": "Limitations include reliance on the quality of AMR parsing, which may introduce errors; potential computational overhead in graph parsing for large-scale applications; and the need for further exploration of cross-lingual applicability since AMR is primarily developed for English.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2204.06508.pdf"}, {"title": "FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations", "authors": ["Not explicitly provided in the given text"], "abstract": "Despite recent improvements in abstractive summarization, most current approaches generate summaries that are not factually consistent with the source text. FACTGRAPH is a method that decomposes both the document and the summary into structured meaning representations (MR), specifically Abstract Meaning Representation (AMR), which are more suitable for factuality evaluation. By explicitly modeling semantic relations and abstracting away from surface text, FACTGRAPH improves factuality detection in summaries. Extensive experiments demonstrate substantial improvements over previous approaches across multiple benchmarks.", "objectives": "The objective of this research is to improve the detection of factual inconsistencies in abstractive summarization by leveraging structured semantic graph representations, specifically AMR, to better capture meaning and reduce semantic errors.", "methodology": "FACTGRAPH employs a dual approach that encodes both source documents and generated summaries into semantic graphs using AMR. These graphs abstract away from syntactic and lexical variations, enabling direct comparison of underlying semantic concepts and relations. The method limits the number of document graphs for efficiency, uses pretrained structural adapters for representation learning, and trains only adapter weights instead of full fine-tuning. Ablation studies and comparisons with QA-based and other factuality evaluation methods are conducted to validate the approach.", "datasets": "FACTCOLLECT dataset (9,567 datapoints: 8,667 train, remainder for dev/test), CNN/DailyMail (CNN/DM), XSum. Synthetic data derived from these datasets is also used for evaluation.", "models": "FACTGRAPH (with and without pretrained structural adapters), FACTGRAPH-E (edge-level variant), QA-based models QAGS and QUALS, FACTCC and FACTCC+, DAE (Dependency Arc Evaluation). AMR parsing is done using state-of-the-art parsers such as JAMR and Bevilacqua et al. (2021).", "evaluation": "Evaluation is performed using Balanced Accuracy (BACC), F1 score, Pearson and Spearman correlation coefficients with human judgments, and various text similarity metrics including BLEU, METEOR, ROUGE-L, BERTScore, and SMATCH-AMR. Comparisons are made across CNN/DM and XSum datasets. Ablation studies and efficiency comparisons are also included.", "results": "FACTGRAPH achieves the highest performance among compared methods, with BACC up to 86.4 and F1 up to 87.4 on CNN/DM, and significant improvements on XSum (BACC 70.4). It shows better correlation with human judgments and outperforms QA-based and dependency-based methods. FACTGRAPH requires only 1.4% of parameters compared to full fine-tuning while achieving higher accuracy. AMR-based semantic graphs are shown to be effective for detecting factual errors, especially in highly abstractive summaries.", "contributions": "Introduces FACTGRAPH, a novel graph-based factuality evaluation method using AMR; demonstrates that structured semantic representations improve factuality detection; achieves state-of-the-art results on multiple benchmarks; reduces computational cost by training only adapter weights; provides detailed ablation and efficiency analyses; shows applicability across datasets with varying levels of abstractiveness.", "limitations": "Authors note potential dependency on the quality of AMR parsing, which may introduce errors; efficiency constraints require limiting the number of document graphs; performance may vary with different parser implementations; factuality evaluation still faces challenges in extremely abstractive or noisy summaries.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2204.06508v2"}, {"title": "Instructive Dialogue Summarization with Query Aggregations", "authors": ["Not specified in provided content"], "abstract": "Conventional dialogue summarization methods directly generate summaries without considering user-specific interests, which limits their adaptability. This work proposes InstructDS, an instructive dialogue summarization model that incorporates user preferences via query-based summarization. The method synthesizes query-dialogue-summary (QDS) triples using summary-anchored techniques and leverages large language models for question generation and answering. Experiments on multiple datasets show that InstructDS outperforms state-of-the-art models, including larger ones, in both automatic and human evaluations.", "objectives": "To develop a dialogue summarization method that incorporates user-specific preferences through query-based summarization, improving flexibility, controllability, and performance over existing methods.", "methodology": "The proposed approach synthesizes QDS triples using summary-anchored techniques to generate queries aligned with user interests. The process includes: (1) generating candidate queries from summaries using large language models (LLMs); (2) applying text-based and semantic-based filtering to ensure quality and diversity; (3) training a unified summarization model with a mixed paradigm of dialogue summarization and reading comprehension tasks; (4) leveraging LoRA for parameter-efficient fine-tuning. The model is evaluated on multiple datasets without dataset-specific tuning.", "datasets": "SAMSum (Gliwa et al., 2019), DialogSum (Chen et al., 2021), TODSum, DREAM (reading comprehension dataset). Synthetic QDS triples are generated for training, with filtering to ensure quality.", "models": "InstructDS (proposed), BART, MV-BART, Coref-BART, ConDigSum, Alpaca, Flan-T5-Large, Flan-T5-XXL, ChatGPT (for comparison in some tasks).", "evaluation": "Evaluation includes automatic metrics (ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, Multi-Choice Accuracy for comprehension tasks) and human evaluation on faithfulness, relevance, coherence, and fluency. Comparisons are made against state-of-the-art baselines on multiple datasets.", "results": "InstructDS achieves the best performance on SAMSum, DialogSum, and TODSum datasets, outperforming both dedicated dialogue summarization models and general-purpose LLMs. It also shows competitive performance on the DREAM dataset without in-domain training. Ablation studies show that length augmentation improves results, while simply increasing QDS triples does not always help. Human evaluation confirms improvements in summary quality.", "contributions": "1) Proposes the first instructive dialogue summarization model (InstructDS) incorporating user preferences via query-based summarization. 2) Introduces a QDS triple synthesis method using summary-anchored techniques and LLM-based question generation. 3) Designs a mixed training paradigm combining summarization and comprehension tasks. 4) Demonstrates state-of-the-art performance across multiple datasets without dataset-specific tuning.", "limitations": "The method still lacks transparency and robustness in some evaluation aspects. It has not been fully tested on lengthy dialogue summarization tasks, and scalability to large meeting summarization remains a challenge. Additionally, while generalizable, dataset-specific tuning could further improve results at the cost of generalizability.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2310.10981v3"}, {"title": "https://assets.amazon.science/6f/89/402bcc6a468eb5eeab80078b9165/factgraph-evaluating-factuality-in-summarization-with-semantic-graph-representations.pdf", "authors": ["Not explicitly provided in the given text"], "abstract": "Despite recent improvements in abstractive summarization, most current approaches generate summaries that are not factually consistent with the source text. FACTGRAPH is introduced as a method that decomposes documents and summaries into structured meaning representations (MR), specifically Abstract Meaning Representation (AMR), to better capture semantic relations and reduce surface-level variance. Extensive experiments show that FACTGRAPH achieves substantial improvements over previous factuality evaluation approaches.", "objectives": "The objective of this research is to improve the factuality evaluation of abstractive summaries by leveraging structured semantic graph representations, aiming to detect factual inconsistencies more effectively than existing methods.", "methodology": "FACTGRAPH encodes both source documents and generated summaries into semantic graphs using Abstract Meaning Representation (AMR). The approach involves: (1) parsing text into AMR graphs, (2) aligning nodes to words, (3) selecting relevant document graphs using contextual representations, (4) training adapter weights in a pretrained language model to evaluate factuality, and (5) comparing semantic relations between document and summary graphs to detect inconsistencies. The method is tested against QA-based and sentence-level/edge-level factuality models.", "datasets": "The paper uses multiple datasets for evaluation, including CNN/DailyMail, XSum, and the human-annotated dataset from Pagnoni et al. (2021). Additionally, FACTCOLLECT is introduced, containing 9,567 datapoints split into train (8,667), dev, and test sets after removing duplicates.", "models": "FACTGRAPH (including variants with pretrained structural adapters and edge-level models), FACTCC, FACTCC+, QAGS, QUALS, DAE, Sent-Factuality. The approach relies on AMR parsing using JAMR aligner and adapter-based training within pretrained language models.", "evaluation": "Evaluation is conducted using Balanced Accuracy (BACC), F1-score, BLEU, METEOR, ROUGE-L, BERTScore, SMATCH-AMR scores, and correlation coefficients with human judgments. Comparisons are made against baseline models on multiple datasets. Ablation studies and speed comparisons are also performed.", "results": "FACTGRAPH outperforms FACTCC+ by 2.4 BACC points and achieves higher correlation with human judgments than QA-based methods. On CNN/DailyMail and XSum datasets, FACTGRAPH consistently achieves better BACC and F1 scores. Ablation studies show AMR-based representations yield the best results. Edge-level FACTGRAPH-E achieves 81.1 BACC compared to 78.7 for DAE. The approach is effective for highly abstractive summaries and detects hallucinations missed by other models.", "contributions": "Introduces FACTGRAPH, a novel factuality evaluation method using semantic graph representations; demonstrates the effectiveness of AMR in abstracting away from surface text to capture semantic relations; provides FACTCOLLECT dataset for factuality evaluation; shows substantial improvements over state-of-the-art factuality models; conducts comprehensive evaluation including ablation and speed studies.", "limitations": "The paper notes potential risks in relying on AMR parsing quality, as errors in parsing can affect factuality detection. The approach requires computational resources for AMR parsing, which can be slower compared to some baselines. Additionally, the method’s performance depends on the quality of human-annotated data for training.", "figures": null, "tables": null, "url": "https://assets.amazon.science/6f/89/402bcc6a468eb5eeab80078b9165/factgraph-evaluating-factuality-in-summarization-with-semantic-graph-representations.pdf"}, {"title": "[2409.15090] Using Similarity to Evaluate Factual Consistency in Summaries", "authors": ["Not explicitly stated in provided content"], "abstract": "Cutting-edge abstractive summarizers generate fluent summaries, but their factual accuracy is not guaranteed. Early exploration into factuality evaluation methods shows misalignment with human annotations. This paper investigates similarity-based metrics, particularly SBERTScore, for evaluating factual consistency in summaries. Experiments demonstrate that SBERTScore can outperform NLI-based metrics in zero-shot settings and is competitive with QA-based metrics, while being significantly faster. The study also explores combining multiple metrics to improve performance.", "objectives": "To investigate the suitability of similarity-based metrics, especially SBERTScore, for evaluating factuality in abstractive summarization, and to explore whether combining different metrics can outperform individual ones.", "methodology": "The study compares similarity-based metrics (SBERTScore, BERTScore) with NLI-based and QA-based factuality metrics across multiple datasets. It examines performance using both reference-summary and source-summary pairs, tests different text granularities, and evaluates metric combinations. Experiments are conducted in zero-shot and trained settings, with statistical significance testing (p &lt; 0.05).", "datasets": "Benchmark datasets include XSF, Polytope, FactCC, SummEval, FRANK, QAGS, CLIFF, Goyal’21, and XENT. These datasets vary in source length, summary length, and factuality annotation coverage, and are derived from English news articles and outputs from summarizers like BART, PEGASUS, and BERTSumAbs.", "models": "BART, PEGASUS, BERTSumAbs for summary generation; SBERTScore, BERTScore, SummaC (Conv and ZS), QAFactEval, DAE, QuestEval for factuality evaluation.", "evaluation": "Evaluation involves ROC-AUC computation, balanced accuracy, and statistical significance testing. Metrics are tested on both reference-summary and source-summary pairs, with performance compared across datasets. Speed comparisons are also made, showing SBERTScore is faster than NLI-based and QA-based metrics.", "results": "SBERTScore outperforms NLI-based metrics in zero-shot settings and is competitive with QA-based metrics while being up to 30 times faster. Source-summary evaluation yields better results than reference-summary evaluation. Combining metrics improves performance over individual metrics. Metric performance varies by dataset, indicating the importance of metric selection.", "contributions": "Introduced SBERTScore as a competitive and efficient factuality evaluation metric; demonstrated that source-summary evaluation is more effective than reference-summary evaluation; showed that combining diverse metrics can outperform individual ones; provided comprehensive benchmarking across multiple datasets.", "limitations": "Experiments are limited to English datasets; results may not generalize to other languages or domains; metric agreement is weak (Kohen’s κ &lt; 0.45), indicating inconsistency across evaluation methods; reliance on existing datasets may limit applicability to real-world scenarios.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2409.15090.pdf"}, {"title": "An Enhanced Latent Semantic Analysis Approach for Arabic Document Summarization", "authors": ["Binwahlan, M.S.", "Salim, N.", "Suanmali, L."], "abstract": "The fast-growing amount of information on the Internet makes research in automatic document summarization increasingly important. This paper proposes a Latent Semantic Analysis (LSA)-based model for extractive summarization in both Arabic and English, evaluated on multiple datasets. Experimental results show that the proposed model performs comprehensively better compared to existing methods.", "objectives": "To develop and evaluate an LSA-based automatic text summarization model that effectively handles both Arabic and English documents, improving semantic representation and outperforming existing summarization methods.", "methodology": "The proposed approach uses Latent Semantic Analysis (LSA) with Singular Value Decomposition (SVD) to capture semantic relationships between words and sentences. The methodology includes: (1) preprocessing text (tokenization, stemming/root extraction, POS tagging), (2) constructing term-sentence matrices, (3) applying weighting schemes including adjacent weights, (4) performing SVD to reduce dimensionality, (5) selecting sentences based on semantic importance, and (6) generating extractive summaries.", "datasets": "Four datasets were used: EASC (Arabic), LDC Arabic Newswire-a, DUC2002 (English), and Multilingual MSS 2015 Single-document dataset (Arabic and English).", "models": "Proposed LSA-based models with variations in preprocessing and weighting schemes, including RAWEF4ADJ, RAWEFPOS4ADJ, SAWEFPOS4ADJ, WAWEFPOS4ADJ, and baseline models such as LEAD-3, DUC-best, Steinberger2004, and Wang2013.", "evaluation": "Evaluation was conducted using both human judgment (questionnaires rating summaries from Very Poor to Very Good) and automatic metrics ROUGE-1 and ROUGE-2. Comparisons were made against baseline methods on multiple datasets.", "results": "The proposed model achieved the highest ROUGE scores among tested variations, with RAWEFPOS4ADJ scoring ROUGE-1: 0.6175 and ROUGE-2: 0.3336 on Arabic LDC, outperforming baselines on DUC2002 and Multilingual MSS 2015 datasets. Root extraction combined with POS tagging improved semantic similarity and reduced noise. The model consistently outperformed state-of-the-art methods in most cases.", "contributions": "Introduced an enhanced LSA-based summarization model incorporating root extraction, POS tagging, and adjacent weighting schemes; demonstrated effectiveness across Arabic and English datasets; provided comparative analysis against multiple baselines; showed that combining term and sentence descriptions improves summarization quality.", "limitations": "While the model significantly outperforms baselines, abstractive summarization capabilities remain weak; performance gains are less pronounced for ROUGE-2 in English; further work is needed to generalize the approach to more languages and diverse document types.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/1807.11618v1"}, {"title": "Inscriptis -- A Python-based HTML to text conversion library optimized for knowledge extraction from the Web", "authors": ["Stefan Behnel", "Martijn Faassen", "Ian Bicking", "Holger Joukl", "Simon Sapin", "Marc-Antoine Parent", "Oli"], "abstract": "Most knowledge extraction methods operate on text and require accurate conversion of HTML content while preserving spatial alignment between text elements. Inscriptis addresses this need by providing high-quality text representations of HTML documents, enabling better performance for algorithms that rely on term proximity, such as word embeddings and language models.", "objectives": "To develop and provide a tool that accurately converts HTML content into high-quality text representations while preserving spatial alignment, thereby improving the effectiveness of knowledge extraction methods.", "methodology": "Inscriptis processes HTML documents to extract text while maintaining spatial relationships between elements. The methodology focuses on preserving layout and proximity information that is critical for downstream algorithms such as word embeddings and language models.", "datasets": "Not explicitly mentioned in the provided content.", "models": "Not explicitly mentioned in the provided content, but references suggest potential use with word embeddings and language models.", "evaluation": "Not explicitly mentioned in the provided content.", "results": "Inscriptis has been publicly available since March 2016 and provides high-quality text extraction from HTML, supporting improved knowledge extraction performance for methods relying on spatial and proximity information.", "contributions": "Introduced Inscriptis as a reliable HTML-to-text conversion tool that preserves spatial alignment; supports algorithms dependent on term proximity; available as an open-source solution since 2016.", "limitations": "Specific limitations are not mentioned in the provided content.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2108.01454v2"}, {"title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets", "authors": ["Not explicitly provided in the given content"], "abstract": "We explore the ability of GPT-4 to perform ad-hoc schema-based information extraction from scientific literature.", "objectives": "To evaluate whether GPT-4 can effectively perform ad-hoc schema-based information extraction from scientific papers, particularly in the materials science domain.", "methodology": "The extraction pipeline consists of: (1) retrieving and parsing source PDFs; (2) chunking papers into smaller blocks of 2000 tokens with 1% overlap; (3) applying prompts (provided in appendix) to extract structured data; (4) aligning extracted rows with original datasets for evaluation.", "datasets": "The study uses an expanded MPEA dataset (originally published by Borg et al., 2020) and a Diffusion dataset containing multi-decade experimental results with varied formats and units.", "models": "GPT-4 was used as the primary large language model, integrated via the LangChain framework.", "evaluation": "Evaluation was based on matched entries between extracted and original datasets, calculating recall, precision, and hallucination rates. Manual error analysis was performed to identify common extraction issues.", "results": "For the MPEA dataset, performance varied depending on prompting strategy: 0-shot-simple (Recall: 0.062, Precision: 0.245), 0-shot-complex (Recall: 0.066, Precision: 0.369), 1-shot-simple (Recall: 0.280). Extraction of fewer properties improved recall and precision. Significant errors arose from table miscomprehension, narrative text interpretation, and unit mismatches.", "contributions": "The paper provides a systematic evaluation of GPT-4 for scientific information extraction, introduces an ad-hoc schema-based pipeline, and highlights the importance of chunking and prompt design for improved extraction accuracy.", "limitations": "Limitations include low recall for complex schemas, susceptibility to hallucinations, difficulty parsing complex or non-standard tables, challenges with narrative-form data, and inability to consistently normalize units across datasets.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2406.05348v3"}, {"title": "Learning from similarity and information extraction from structured documents", "authors": ["Martin Holeček"], "abstract": "The automation of document processing is gaining attention due to its potential to reduce manual work through improved methods and hardware. This work aims to improve information extraction results by compiling and publishing a dataset of over 25,000 anonymized documents and designing trainable methods using siamese networks, similarity concepts, and one-shot learning. Results verify that trainable access to similar yet different pages improves performance, achieving an 8.25% F1 score gain over previous state-of-the-art methods, with qualitative analysis confirming improvements across all target classes.", "objectives": "To improve information extraction from structured business documents by leveraging similarity-based neural architectures, specifically inspired by one-shot learning, and to surpass previous state-of-the-art performance while ensuring applicability to unseen documents.", "methodology": "The methodology involves: (1) compiling and anonymizing a large dataset of structured documents; (2) designing architectures incorporating similarity search, siamese networks, triplet loss, and pairwise classification; (3) integrating query-answer inspired architectures; (4) shuffling document order before each epoch to ensure robustness; (5) training models with known and similar documents to boost predictions; (6) applying standardized OCR for potential scanned image processing; (7) manual hyperparameter tuning to optimize performance.", "datasets": "The dataset consists of over 25,000 anonymized structured business documents (e.g., invoices), compiled and published for research purposes. Two subsets were used for training and validation: a \"small\" dataset and a larger dataset.", "models": "Models explored include: (1) Simple data extraction baseline; (2) Siamese network architectures; (3) Triplet loss architectures; (4) Pairwise classification architectures; (5) Query-answer inspired architectures; (6) Oracle baseline for upper-bound performance comparison.", "evaluation": "Evaluation was conducted using micro F1 score as the primary metric, with comparisons against previous state-of-the-art results (F1 score 0.8465). Metrics were scaled relative to an Oracle baseline. Qualitative analysis was performed to assess improvements across all classes. Baseline and advanced architectures were tested under identical conditions.", "results": "The best model achieved an 8.25% improvement in F1 score over the previous state-of-the-art, with uniform score increases of at least 0.02 across all target classes. The query-answer inspired architecture demonstrated meaningful improvements, translating to significant time and cost savings (e.g., reduced manual effort on 500+ invoices per month, ~$4000 savings). The method generalizes to unseen documents.", "contributions": "Key contributions include: (1) Creation and publication of a large anonymized dataset of structured business documents; (2) Introduction of similarity-based architectures for information extraction; (3) Demonstration that trainable access to similar documents boosts performance; (4) Achieving significant F1 score improvement over prior methods; (5) Providing qualitative and quantitative evidence of uniform class improvements; (6) Designing robust training procedures applicable to scanned documents.", "limitations": "Limitations include: (1) The method requires retraining or fine-tuning when new classes of words are introduced; (2) Similarity search alone is insufficient without more complex architectures; (3) Performance may depend on embedding quality and hyperparameter choices; (4) OCR errors could affect extraction accuracy when applied to scanned images.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2011.07964v2"}, {"title": "Extracting Entities of Interest from Comparative Product Reviews", "authors": ["Jatin Arora", "Sumit Agrawa"], "abstract": "This paper presents a deep learning based approach to extract product comparison information from user reviews, aiming to automate the process of identifying and annotating comparative opinions.", "objectives": "To develop a deep learning framework that can automatically extract and annotate product comparison information from textual user reviews, reducing the need for manual feature engineering and outperforming existing SRL-based systems.", "methodology": "The proposed methodology involves generating labeled data using pattern-based approaches, training LSTM-based models with word embeddings, and evaluating performance against baselines. The process includes sentence and word tokenization, POS tagging, and leveraging GloVe embeddings from the Text8 corpus. Various LSTM configurations were tested to determine the optimal architecture.", "datasets": "Two datasets were used: JDPA dataset (Manual annotations: 210 sentences, Pattern-based annotations: 24,164 sentences, Total: 26,895 sentences) and J&L dataset (313 sentences). These datasets contain annotated comparison sentences for training and evaluation.", "models": "Multiple LSTM-based models were tested: Model1 (Unidirectional LSTM, 1 layer, 300-dim embeddings from Text8), Model2 (Bidirectional LSTM, 1 layer, 300-dim GloVe embeddings from Text8) — Model2 achieved the best performance.", "evaluation": "Evaluation was conducted using predicate identification, argument identification, and classification tasks. Metrics included recall and F1-score. Baselines included an SRL-based system with gold predicates and another without gold predicates.", "results": "The best results were achieved using a single layer Bidirectional LSTM with 300-dimensional GloVe embeddings from the Text8 corpus, outperforming both baselines in recall and F1-score. The proposed model showed better performance without the need for extensive feature engineering.", "contributions": "Introduced a simple deep learning framework for comparison mining that avoids complex feature engineering, developed pattern-based methods for generating large-scale labeled data, and demonstrated superior performance over SRL-based baselines.", "limitations": "Pattern-based data generation led to relatively low recall in some cases; the approach may rely heavily on the quality of generated training data and may not generalize well to domains with significantly different comparison sentence structures.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2310.20274v1"}, {"title": "RAG System for Supporting Japanese Litigation Procedures: Faithful Response Generation Complying with Legal Norms", "authors": ["Not specified"], "abstract": "This study discusses the essential components that a Retrieval-Augmented Generation (RAG)-based large language model (LLM) system should have to ensure factual accuracy and faithfulness in generated responses, particularly in the context of supporting medical litigation proceedings in Japan.", "objectives": "To design and develop a RAG-based LLM system that can generate contextually faithful and factually accurate responses for use in medical litigation support in Japan.", "methodology": "The paper reviews related work on RAG, Data Attribution (DA), and faithfulness evaluation methods. It proposes approaches such as explicit constraints through prompting, chain-of-verification steps to ensure generated content aligns with retrieved documents, and filtering outputs that deviate from predefined scope. The methodology emphasizes factual verification and temporal validity of sources.", "datasets": "Not specified", "models": "Retrieval-Augmented Generation (RAG)-based Large Language Model", "evaluation": "Evaluation approaches include Data Attribution analysis, response faithfulness assessment, and labeling outputs that deviate from the predefined scope as inappropriate. Metrics are not explicitly specified but focus on factual accuracy and alignment with retrieved context.", "results": "The study concludes that ensuring faithfulness to retrieved documents and verifying factual accuracy are critical for RAG-based LLMs in sensitive domains like medical litigation. Future work will refine requirements, propose methods, improve evaluation metrics, implement them, and conduct experiments.", "contributions": "Identifies essential components for RAG-based LLM systems in legal-medical contexts; proposes methods to ensure faithfulness and factual accuracy; highlights the importance of temporal validity of sources; suggests evaluation strategies for attribution and response quality.", "limitations": "The paper does not provide experimental validation, specific datasets, or quantitative evaluation metrics; the proposed methods are conceptual and require future implementation and testing.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2511.22858v1"}, {"title": "FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation", "authors": ["Not specified in provided content"], "abstract": "While Retrieval-Augmented Generation (RAG) mitigates hallucination and knowledge staleness in large language models, existing iterative or adaptive RAG methods still lack a robust mechanism to systematically identify and fill evidence gaps. This paper introduces FAIR-RAG, a structured, evidence-driven refinement process with explicit gap analysis, achieving state-of-the-art results on multiple QA benchmarks.", "objectives": "The objective is to design and evaluate a novel agentic RAG framework that systematically identifies and addresses evidence gaps through a multi-stage, structured refinement process, thereby improving factual accuracy and reasoning in complex question answering tasks.", "methodology": "FAIR-RAG employs a multi-stage, iterative process with adaptive LLM selection and explicit evidence gap analysis. Steps include: (1) Initial query analysis and adaptive routing to specialized generator LLMs; (2) Hybrid retrieval using multiple methods with Reciprocal Rank Fusion for robust document ranking; (3) Checklist-based query decomposition to identify required informational components; (4) Iterative refinement cycles to retrieve missing evidence; (5) Structured Evidence Assessment (SEA) to determine sufficiency of retrieved information; (6) Final synthesis of answer. The framework uses modular agents without fine-tuning requirements, enabling flexibility and integration.", "datasets": "Benchmarks used: HotpotQA (multi-hop QA), 2WikiMultiHopQA (multi-hop QA), Musique (multi-hop QA), TriviaQA (single-hop factual QA). Each dataset contains complex queries requiring retrieval and reasoning over multiple documents.", "models": "Baseline and comparison models include Llama-3-8B-Instruct, Self-RAG (selfrag-llama), Iter-Retgen, ReAct, SuRe, Adaptive-RAG. FAIR-RAG variants use multiple generator LLMs (small, large, reasoning) and a dense retriever (e5-base-v2).", "evaluation": "Evaluation uses standard QA metrics: Exact Match (EM), F1 score, LLM-as-Judge Accuracy (ACC LLM) for binary correctness, and component-level quality scoring for fine-grained analysis. Ablation studies and failure mode analysis are conducted using LLM-as-Judge methodology to assess contributions of individual components.", "results": "FAIR-RAG achieves state-of-the-art results: On HotpotQA, F1=0.45 (outperforming Self-RAG by 6.9 points); On Musique, F1=0.264 (7.4 points higher than Iter-Retgen); Strong gains also observed on 2WikiMultiHopQA and TriviaQA. Adaptive LLM selection further boosts performance. Ablation studies confirm each module’s contribution. Optimal performance achieved at 2–3 iterations for multi-hop datasets.", "contributions": "Key contributions: (1) Introduction of FAIR-RAG, a multi-agent, structured RAG framework with explicit evidence gap analysis; (2) Checklist-based query decomposition to guide retrieval; (3) Hybrid retrieval with Reciprocal Rank Fusion; (4) Modular design requiring no fine-tuning; (5) Comprehensive evaluation with standardized conditions ensuring fair comparison; (6) Demonstrated state-of-the-art performance across multiple QA benchmarks.", "limitations": "(1) Dependency on LLM reasoning fidelity and prompt engineering quality; (2) Potential error propagation in multi-stage pipeline; (3) Fixed iteration count may not be optimal for all queries; (4) Computational cost considerations for complex multi-hop queries.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2510.22344v1"}, {"title": "FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering", "authors": ["Not explicitly provided in the given content"], "abstract": "The advent of Large Language Models (LLMs) has revolutionized Natural Language Processing, enabling more sophisticated question answering systems. This paper introduces FARSIQA, a specialized Islamic Question Answering system that combines dynamic LLM allocation, hybrid retrieval, and iterative refinement to ensure faithful, safe, and responsible answers.", "objectives": "To develop a reliable, faithful, and responsible Islamic Question Answering system that surpasses traditional retrieve-and-read pipelines by leveraging dynamic LLM allocation, hybrid retrieval methods, and structured evidence assessment.", "methodology": "The FARSIQA pipeline consists of multiple phases: (1) Query validation and dynamic model selection to allocate computational resources based on query complexity; (2) Hybrid retrieval using both sparse (BM25) and dense retrieval methods, followed by re-ranking; (3) Structured Evidence Assessment (SEA) using prompted LLM agents to decompose queries into checklists and assess evidence sufficiency; (4) Iterative refinement to adapt search strategies dynamically; (5) Strict grounding to ensure faithfulness and safety in generated answers.", "datasets": "Evaluation dataset consists of 800 samples from encyclopedic and Q&A platform sources, with metadata including source URLs for citation. Knowledge base statistics are detailed in Appendix A.", "models": "GPT-4, Llama 3, Llama-4-Maverick (used as LLM-as-Judge), Dense Passage Retriever (DPR), BM25 within Elasticsearch.", "evaluation": "Evaluation uses LLM-as-Judge methodology for scalable and consistent assessment, verified by human oversight. Metrics include a 1-to-5 scale scoring, F1-score, and qualitative error mode analysis. Iteration effects and dynamic LLM allocation strategies are also evaluated.", "results": "FARSIQA achieved superior performance compared to a naive RAG baseline, with scores: 3.98 average rating, 74.3% correctness, 81.6% faithfulness, 3.49 reasoning score, and 97.0% safety. Optimal configuration determined as a maximum of 3 iterations. Dynamic LLM allocation proved effective in balancing quality and efficiency.", "contributions": "Introduced FARSIQA, a specialized Islamic QA system with dynamic LLM allocation; Developed a hybrid retrieval and re-ranking mechanism; Implemented Structured Evidence Assessment for iterative refinement; Demonstrated superior performance over naive RAG baselines; Established optimal iteration limits for efficiency-quality trade-off.", "limitations": "Authors note the need for continuous knowledge base expansion and automated verification of new sources; Potential dependency on LLM judgment reliability; System performance may vary with domain-specific queries outside Islamic knowledge scope.", "figures": null, "tables": null, "url": "https://arxiv.org/pdf/2510.25621v1"}]