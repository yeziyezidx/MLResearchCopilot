"{\"summary\": \"Passage ranking in the context of open-domain question answering (OpenQA) and related knowledge-intensive tasks has evolved beyond simple topical relevance to incorporate deeper notions such as **answerability**, **multi-answer coverage**, and **generation-alignment**.  \\n\\nTraditionally, retrieval systems ranked passages largely on topical similarity between a question and candidate texts. However, this approach often fails when retrieved passages are relevant but do not contain the actual answer, or when they contain the answer entity but lack the necessary context to correctly address the question. Modern passage ranking research tackles these shortcomings via three complementary approaches:\\n\\n1. **Answer-oriented ranking** — Models like **PReGAN** integrate discriminators for both topical relevance and answerability in a generative adversarial framework. This ensures higher ranking for passages that are both relevant *and* contain an answer in context. PReGAN addresses noisy training data issues by refining what constitutes a “positive” sample, boosting QA performance without depending on external data.\\n\\n2. **Joint, multi-answer retrieval** — For ambiguous or open-ended questions with multiple valid answers, **Joint Passage Retrieval (JPR)** uses an autoregressive reranker to select a diverse set of passages that jointly maximize answer coverage. This avoids redundancy (retrieving different passages with the same answer) and improves downstream QA by providing broader, non-overlapping information.\\n\\n3. **Generation-aware ranking** — **GripRank** bridges retrieval and answer generation by distilling knowledge from a generative passage estimator (GPE) into a ranker. The GPE evaluates how likely a passage will help generate the correct answer, and its ranking is progressively taught to the retrieval model using curriculum knowledge distillation. This aligns passage selection with the needs of generative QA systems, improving both retrieval and downstream answer quality.\\n\\nOverall, the field is moving toward **joint optimization of retrieval and answer generation**, **greater diversity in retrieved sets**, and **robust handling of noisy labels**. These approaches collectively enhance the performance of open-domain QA and other generation-based tasks by supplying high-quality, varied, and answer-rich context.\", \"problem\": null, \"key_concepts\": [\"- **Passage Ranking**: Ordering retrieved text segments from a corpus by their relevance to a query.\", \"- **Topical Relevance**: Degree of subject matter similarity between query and passage.\", \"- **Answerability**: Likelihood that a passage contains the correct answer in appropriate context for the given question.\", \"- **Generative Adversarial Networks (GANs)**: Machine learning framework with generator-discriminator models, used in PReGAN for ranking optimization.\", \"- **Joint Passage Retrieval (JPR)**: Multi-answer retrieval model optimizing coverage across distinct answers using autoregressive reranking.\", \"- **Generative Passage Estimator (GPE)**: A generative model that assesses passage utility for answer generation, central to GripRank.\", \"- **Knowledge Distillation**: Transferring knowledge from a larger/more complex teacher model to a simpler student model.\", \"- **Curriculum Learning**: Gradually increasing task difficulty during training to improve learning efficiency and model performance.\"], \"recent_developments\": [\"- Integration of **answerability** into ranking models (PReGAN), addressing noisy label problems in OpenQA datasets.\", \"- Focus on **multi-answer retrieval** (JPR), optimizing for diversity and coverage rather than single-answer precision.\", \"- Emergence of **generation-aware ranking** (GripRank), aligning retrieval with answer generation potential through knowledge distillation.\", \"- Adoption of **curriculum knowledge distillation** to progressively improve ranking quality.\", \"- Shift from independent passage scoring to **joint modeling** of passage sets, enabling retrieval systems to consider inter-passage relationships.\"], \"authoritative_sources\": [\"- **PReGAN**: Pan Du, Jian-Yun Nie, Yutao Zhu, Hao Jiang, Lixin Zou, Xiaohui Yan — University of Montreal, Huawei Poisson Lab, Tsinghua University.\", \"- **Joint Passage Retrieval (JPR)**: Sewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, Hannaneh Hajishirzi — University of Washington, Google Research.\", \"- **GripRank**: Jiaqi Bai, Hongcheng Guo, Jiaheng Liu, Jian Yang, Xinnian Liang, Zhao Yan, Zhoujun Li — Beihang University, Tencent Cloud AI.\"], \"search_results\": [{\"title\": \"PReGAN: Answer Oriented Passage Ranking with Weakly Supervised GAN\", \"url\": \"https://arxiv.org/pdf/2207.01762.pdf\", \"snippet\": \"<H1>PReGAN: Answer Oriented Passage Ranking with Weakly Supervised GAN</H1><H3>Pan Du∗ Jian-Yun Nie</H3><P>du@youark.com</P><H3>Yutao Zhu</H3><P>University of Montreal Thomson Reuters Labs University of Montreal Montreal, Canada Toronto, Canada Montreal, Canada nie@iro.umontreal.ca</P><H3>Hao Jiang</H3><P>yutaozhu94@gmail.com</P><H3>Lixin Zou</H3><P>Huawei Poisson Lab.</P><H3>Xiaohui Yan</H3><P>Tshinghua Univeristy Shenzhen, China Huawei Poisson Lab. Beijing, China jianghao66@huawei.com Shenzhen, China zoulx15@mails.tsinghua.edu.cn</P><H3>ABSTRACT</H3><P>Beyond topical relevance, passage ranking for open-domain factoid question answering also requires a passage to contain an answer (answerability). While a few recent studies have incorporated some reading capability into a ranker to account for answerability, the ranker is still hindered by the noisy nature of the training data typi-cally available in this area, which considers any passage containing an answer entity as a positive sample. However, the answer entity in a passage is not necessarily mentioned in relation with the given question. To address the problem, we propose an approach called</P><H3>PReGAN</H3><P>for Passage Reranking based on Generative Adversarial Neural networks, which incorporates a discriminator on answer-ability, in addition to a discriminator on topical relevance. The goal is to force the generator to rank higher a passage that is topically rel-evant and contains an answer. Experiments on five public datasets</P><H3>PReGAN</H3><P>show that can better rank appropriate passages, which in turn, boosts the efectiveness of QA systems, and outperforms the existing approaches without using external data.</P><P>yanxiaohui2@huawei.com</P><P>right</P><P>when reading the passage. However, when reading the pas-</P><P>retrieved</P><P>sages with the question, the performance drops dramati-cally [19, 27], showing the critical importance of retrieving good candidate passages. Many studies have been devoted to improving the topical relevance of retrieved candidate passages [19, 22, 25, 41],</P><P>answerability</P><P>but few studies have investigated the problem of ,</P><H3>i.e.</H3><P>, whether a retrieved passage may contain an answer. Both crite-ria - topical relevance and answerability, are critical in the context of OpenQA. In fact, a passage highly relevant to the question may not necessarily contain an answer, and a passage that contains an answer entity may not be relevant to the question. In both cases, the reader may be misled by these passages to select a wrong answer, as the reader highly relies on the few passages provided to it. It is thus important that the selected passages for the reader should both be relevant and contain an asnwer.</P><P>In this paper, we propose an approach to refine the list of candi-date passages according to both criteria through an answer-oriented passage (re-)ranking.</P><P>Training an efective reranking process is not trivial due to the</P><TABL\", \"source\": \"bing\"}, {\"title\": \"Abstract\", \"url\": \"https://arxiv.org/pdf/2104.08445v1\", \"snippet\": \"<P>Joint Passage Ranking for Diverse Multi-Answer Retrieval</P><P>Sewon Min1,Kenton Lee2, Ming-Wei Chang 2, Kristina Toutanova 2, Hannaneh Hajishirzi 1 1University of Washington2Google Research {sewon,hannaneh}@cs.washington.edu {kentonl,mingweichang,kristout}@google.com Abstract</P><P>We study multi-answer retrieval, an under- explored problem that requires retrieving pas- sages to cover multiple distinct answers for a given question. This task requires joint mod- eling of retrieved passages, as models should not repeatedly retrieve passages containing the same answer at the cost of missing a different valid answer. Prior work focusing on single- answer retrieval is limited as it cannot reason about the set of passages jointly. In this pa- per, we introduce JPR, a joint passage retrieval model focusing on reranking. To model the joint probability of the retrieved passages, JPR makes use of an autoregressive reranker that selects a sequence of passages, equipped with novel training and decoding algorithms. Com- pared to prior approaches, JPR achieves signif- icantly better answer coverage on three multi- answer datasets. When combined with down- stream question answering, the improved re- trieval enables larger answer generation mod- els since they need to consider fewer passages, establishing a new state-of-the-art.</P><P>1 Introduction</P><P>There has been growing interest in passage retrieval where a model is given a natural language question and retrieves a set of relevant passages from a large text corpus (Ramos et al.,2003;Liu,2011;Lee et al.,2019). This is an important problem both as an end application where the retrieval results are presented to users (Järvelin and Kekäläinen, 2002), and for downstream tasks such as question answering where the retrieval results are consumed by another model (Chen et al.,2017). Questions posed by humans are often open- ended and ambiguous, leading to multiple valid answers (Min et al.,2020). Given a question in Figure1, “What was Eli Whitney’s job?”, an ideal retrieval system should provide passages covering all professions of Eli Whitney. This introduces</P><P>Work done while interning at Google.</P><P>Eli Whitney was an American inventor , widely known for inventing the cotton  gin É Whitney worked as a  farm laborer and  school teacher to save money.</P><P>Q: What  was Eli WhitneyÕs job? ! Retrieval System</P><P>Figure 1: The problem of multi-answer retrieval. A re- trieval system must retrieve a set of kpassages (k= 5 in the ﬁgure) which has maximal coverage of the an- swers to the input question. This requires modeling the joint probability of the passages in the output set: P(p</P><P>1:::p</P><P>kjq). Our proposed model JPR achieves this by employing an autoregressive model.</P><P>the problem of multi-answer retrieval—retrieval of multiple passages with maximal coverage of all distinct answers—which is a challenging, yet un- derstudied problem. Most prior work focuses on single-answer retrieval whe\", \"source\": \"bing\"}, {\"title\": \"GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved  Passage Ranking\", \"url\": \"https://arxiv.org/pdf/2305.18144v1\", \"snippet\": \"<H1>GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking</H1><P>Jiaqi Bai1,2, Hongcheng Guo2, Jiaheng Liu2, Jian Yang2, Xinnian Liang2, Zhao Yan3and Zhoujun Li2</P><P>1School of Cyber Science and Technology, Beihang University 2State Key Lab of Software Development Environment, Beihang University 3Tencent Cloud AI</P><P>Beijing, China</P><P>{bjq,hongchengguo,liujiaheng,jiaya,xnliang,lizj}@buaa.edu.cn</P><P>zhaoyan@tencent.com</P><H3>ABSTRACT</H3><P>Retrieval-enhanced text generation, which aims to leverage pas-sages retrieved from a large passage corpus for delivering a proper answer given the input query, has shown remarkable progress on knowledge-intensive language tasks such as open-domain question answering and knowledge-enhanced dialogue generation. How-ever, the retrieved passages are not ideal for guiding answer gener-ation because of the discrepancy between retrieval and generation, i.e., the candidate passages are all treated equally during the re-trieval procedure without considering their potential to generate the proper answers. This discrepancy makes a passage retriever deliver a sub-optimal collection of candidate passages to gener-ate answers. In this paper, we propose the GeneRative Knowledge Improved Passage Ranking (GripRank) approach, addressing the above challenge by distilling knowledge from a generative passage estimator (GPE) to a passage ranker, where the GPE is a generative language model used to measure how likely the candidate passages can generate the proper answer. We realize the distillation proce-dure by teaching the passage ranker learning to rank the passages ordered by the GPE. Furthermore, we improve the distillation qual-ity by devising a curriculum knowledge distillation mechanism, which allows the knowledge provided by the GPE can be progres-sively distilled to the ranker through an easy-to-hard curriculum, enabling the passage ranker to correctly recognize the provenance of the answer from many plausible candidates. We conduct exten-sive experiments on four datasets across three knowledge-intensive language tasks. Experimental results show advantages over the state-of-the-art methods for both passage ranking and answer gen-eration on the KILT benchmark.</P><H3>CCS CONCEPTS</H3><P>• Information systems → Retrieval models and ranking.</P><H3>KEYWORDS</H3><P>Knowledge-intensive language tasks, Retrieval-enhanced text gen-eration, Passage ranking, Knowledge distillation</P><H3>1 INTRODUCTION</H3><P>Knowledge-intensive language tasks, including open-domain ques-tion answering, knowledge-grounded conversation, and fact verifi-cation, pose a challenge for retrieving passages most likely to be the provenance of the target answer from a large passage corpus (e.g., Wikipedia). One of the most successful paradigms to deal with these tasks is retrieval-enhanced text generation [14, 20, 29]. It first trains a passage retriever (e.g., DPR [24] and GTR [34]) to c\", \"source\": \"bing\"}]}"