"{\"summary\": \"Evidence extraction from retrieved webpages—particularly in the context of Retrieval-Augmented Generation (RAG)—is an emerging area focused on improving the efficiency, accuracy, and reliability of large language model (LLM) outputs by selecting only the most relevant and helpful information from retrieved documents.  \\n\\nTraditional RAG systems feed all top-ranked retrieved passages to the LLM, which can lead to redundancy, high computational costs, and hallucinations due to irrelevant or distracting content. This is especially problematic because imperfect retrieval systems often return mixed-quality results containing both supporting and irrelevant material.  \\n\\nThe newly proposed **SEER (Self-Aligned Evidence Extraction for Retrieval-Augmented Generation)** framework addresses these challenges by replacing heuristic, rule-based filtering and chunking with a model-based, self-aligned learning approach. Instead of relying on hand-crafted rules, SEER trains a vanilla model to act as an evidence extractor, optimizing it to produce concise, semantically rich, and highly relevant context for generation.  \\n\\nKey advantages of SEER over prior methods include:  \\n- Significant reduction in evidence length (by approximately 9.25×), lowering computation costs.  \\n- Improved faithfulness, helpfulness, and conciseness of the extracted supporting content.  \\n- Better generalization beyond the training data, avoiding limitations of heuristic-based augmentation.  \\n\\nExtensive experiments show that SEER leads to measurable gains in RAG performance across multiple tasks and datasets, making LLM outputs more grounded, interpretable, and efficient.  \\n\\nThis development reflects a broader trend in RAG research: moving from simple retrieval-and-feed pipelines toward intelligent, context-aware preprocessing that ensures the LLM sees only the most essential evidence.\", \"problem\": null, \"key_concepts\": [\"- **Retrieval-Augmented Generation (RAG):** A paradigm where retrieved documents are provided as context to LLMs to improve factual accuracy and grounding.\", \"- **Evidence Extraction:** The process of selecting only the most relevant portions of retrieved text for use in generation.\", \"- **Heuristic-based Augmentation:** Rule-based filtering and chunking of retrieved text; often suffers from poor generalization and semantic loss.\", \"- **Self-Aligned Learning:** A training approach where a model learns to align its evidence extraction behavior with desired qualities (faithfulness, helpfulness, conciseness) without relying solely on external rules.\", \"- **Evidence Length Reduction:** Minimizing the amount of text passed to the LLM while preserving necessary information, thereby improving efficiency and reducing noise.\"], \"recent_developments\": [\"- Introduction of **SEER** (2024, EMNLP) as a model-based alternative to heuristic filtering in RAG pipelines.\", \"- Demonstrated gains in RAG performance and efficiency, with up to a 9.25× reduction in evidence length.\", \"- Shift toward self-aligned learning frameworks that optimize evidence extraction for semantic richness and relevance rather than rule-based constraints.\", \"- Growing emphasis on making LLM outputs more interpretable and trustworthy by carefully curating context before generation.\"], \"authoritative_sources\": [\"- Xinping Zhao, Dongfang Li, Yan Zhong, Boren Hu, Yibin Chen, Baotian Hu, Min Zhang (2024). *SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation*, Proceedings of EMNLP 2024. Association for Computational Linguistics. [arXiv:2410.11315](https://arxiv.org/abs/2410.11315)\", \"- ACL Anthology entry: [https://aclanthology.org/2024.emnlp-main.178/](https://aclanthology.org/2024.emnlp-main.178/)\", \"- FILCO (Wang et al., 2023) — earlier work on document chunking and filtering for RAG.\"], \"search_results\": [{\"title\": \"[2410.11315] SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation\", \"url\": \"https://arxiv.org/abs/2410.11315\", \"snippet\": \"<H1>Computer Science > Computation and Language</H1><H1>SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation</H1><P>Recent studies in Retrieval-Augmented Generation (RAG) have investigated extracting evidence from retrieved passages to reduce computational costs and enhance the final RAG performance, yet it remains challenging. Existing methods heavily rely on heuristic-based augmentation, encountering several issues: (1) Poor generalization due to hand-crafted context filtering; (2) Semantics deficiency due to rule-based context chunking; (3) Skewed length due to sentence-wise filter learning. To address these issues, we propose a model-based evidence extraction learning framework, SEER, optimizing a vanilla model as an evidence extractor with desired properties through self-aligned learning. Extensive experiments show that our method largely improves the final RAG performance, enhances the faithfulness, helpfulness, and conciseness of the extracted evidence, and reduces the evidence length by 9.25 times. The code will be available at this https URL.</P><TABLE><TR><TD>Comments:</TD><TD>15 pages, 6 figures, 5 tables. Accepted by EMNLP 2024 (main)</TD></TR><TR><TD>Subjects:</TD><TD>Computation and Language (cs.CL)</TD></TR><TR><TD>Cite as:</TD><TD>arXiv:2410.11315 [cs.CL]</TD></TR><TR><TD> </TD><TD>(or arXiv:2410.11315v1 [cs.CL] for this version)</TD></TR><TR><TD> </TD><TD>https://doi.org/10.48550/arXiv.2410.11315\\nFocus to learn more\\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer (What is the Explorer?)</P><P>Connected Papers (What is Connected Papers?)</P><P>Litmaps (What is Litmaps?)</P><P>scite Smart Citations (What are Smart Citations?)</P>\", \"source\": \"bing\"}, {\"title\": \"SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation - ACL Anthology\", \"url\": \"https://aclanthology.org/2024.emnlp-main.178/\", \"snippet\": \"<P>Important: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF.</P><P>Title Adjust the title. Retain tags such as <fixed-case>.</P><P>Abstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>.</P><P>Verification against PDF Ensure that the new title/authors match the snapshot below. (If there is no snapshot or it is too small, consult the PDF.)Authors concatenated from the text boxes above:</P><P>ALL author names match the snapshot above—including middle initials, hyphens, and accents.</P><H5>Abstract</H5><P>Recent studies in Retrieval-Augmented Generation (RAG) have investigated extracting evidence from retrieved passages to reduce computational costs and enhance the final RAG performance, yet it remains challenging. Existing methods heavily rely on heuristic-based augmentation, encountering several issues: (1) Poor generalization due to hand-crafted context filtering; (2) Semantics deficiency due to rule-based context chunking; (3) Skewed length due to sentence-wise filter learning. To address these issues, we propose a model-based evidence extraction learning framework, SEER, optimizing a vanilla model as an evidence extractor with desired properties through self-aligned learning. Extensive experiments show that our method largely improves the final RAG performance, enhances the faithfulness, helpfulness, and conciseness of the extracted evidence, and reduces the evidence length by 9.25 times. The code will be available at https://github.com/HITsz-TMG/SEER.</P><P>Xinping Zhao, Dongfang Li, Yan Zhong, Boren Hu, Yibin Chen, Baotian Hu, and Min Zhang. 2024. SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 3027–3041, Miami, Florida, USA. Association for Computational Linguistics.</P><P>More options…</P>\", \"source\": \"bing\"}, {\"title\": \"https://openreview.net/attachment?id=vsRU4v83B0&name=pdf\", \"url\": \"https://openreview.net/attachment?id=vsRU4v83B0&name=pdf\", \"snippet\": \"<H1>SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation</H1><H3>Anonymous EMNLP submission</H3><P>001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020</P><P>021</P><P>022</P><P>023</P><P>024</P><P>025</P><P>026</P><P>027</P><P>028</P><P>029</P><P>030</P><P>031</P><P>032</P><P>033</P><P>034</P><P>035</P><P>036</P><P>037</P><P>038</P><P>039</P><P>040</P><P>041</P><H3>Abstract</H3><P>Recent studies in Retrieval-Augmented Gener-ation (RAG) have investigated extracting evi-dence from retrieved passages to reduce com-putational costs and enhance the final RAG per-formance, yet it remains challenging. Existing methods heavily rely on data-level augmenta-tion, encountering several issues: (1) Poor gen-eralization due to hand-crafted context filter-ing; (2) Semantics deficiency due to rule-based context chunking; (3) Skewed length due to sentence-wise filter learning. To address these issues, we propose a model-level evidence ex-traction learning framework, SEER, optimiz-ing a vanilla model as an evidence extractor with desired properties through self-aligned learning. Extensive experiments show that our method largely improves the final RAG per-formance, enhances the faithfulness, helpful-ness, and conciseness of the extracted evidence, and reduces the evidence length by 9.25 times.</P><H3>1 Introduction</H3><P>Recent years have witnessed the prevailing winds of Retrieval-augmented Generation (RAG), which is a prevailing paradigm for improving the perfor-mances of Large Language Models (LLMs) in var-ious downstream tasks, such as question answer-ing, making the output more reliable (Lewis et al., 2020; Chen et al., 2023; Jiang et al., 2023b; Ram et al., 2023), interpretable (Guu et al., 2020; Louis et al., 2024), and adaptable (Xu et al., 2023; Za-kka et al., 2024). Traditional practices (Karpukhin et al., 2020; Min et al., 2019) often involve provid-ing top-retrieved passages as the input context to LLMs without discrimination. However, imperfect retrieval systems frequently yield irrelevant content. Furthermore, indiscriminately feeding all retrieved content to LLMs will cause input redundancy, im-posing a significant computational cost and render-ing them prone to hallucination (Shi et al., 2023). Ideally, LLMs should be grounded on support-ing content that is both highly helpful to address user input and suficiently concise to facilitate infer- ence speed. However, it is practically impossible for imperfect retrieval systems to achieve such an ideal grounding solely (Wang et al., 2023). In fact, top-retrieved passages usually compose supporting and distracting content, inflicting a heavy blow on LLMs trained with high-quality corpora to generate the correct output. This motivates us to develop an evidence extractor, that aims at extracting support- ing content while filtering out distracting content. Recently, a pioneering study, FILCO (Wang et al., 2023), attempts to retrieve chunking doc- ument conte\", \"source\": \"bing\"}]}"