{"problem": "The user is asking how modern AI search systems (especially deep-research or AI-connected search) process retrieved webpages to extract the most relevant and useful information for answer generation.  \n    True problem: Understanding the **post-retrieval evidence extraction** stage in a RAG pipeline â€” specifically, the transformation from raw retrieved content to concise, high-quality evidence that supports accurate, efficient, and explainable answers.", "scenario": "Retrieval-Augmented Generation (RAG) system for AI-connected search and question answering", "workflow": "1. **Query Understanding**  \n       - Input: User query (text or voice)  \n       - Output: Normalized, intent-classified query representation  \n       - Signals: Language detection, named entity recognition (NER), query expansion terms  \n\n    2. **Document Retrieval**  \n       - Input: Query vector from embedding model  \n       - Output: Top-K relevant document chunks from vector database or search index  \n       - Signals: Embedding similarity scores, keyword match scores  \n\n    3. **Post-Retrieval Evidence Extraction** *(user's focus)*  \n       - Input: Retrieved raw passages/webpages  \n       - Output: Filtered, compressed, and ranked evidence snippets  \n       - Signals: Passage relevance, answerability scores, LLM confidence signals, redundancy detection  \n       - Submodules:  \n         - Evidence Identification (locating minimal supporting sentences)  \n         - Contextual Compression/Filtering (removing irrelevant text, reducing token count)  \n         - Evidence Fusion & Re-ranking (ordering by utility for answer generation)  \n\n    4. **Answer Synthesis**  \n       - Input: Enhanced prompt containing query + extracted evidence  \n       - Output: Generated answer text with citations  \n       - Signals: Factual consistency checks, coverage metrics  \n\n    5. **Output Formatting & Delivery**  \n       - Input: Generated answer object  \n       - Output: UI-rendered answer with source links, highlights  \n       - Signals: Presentation logic, trust indicators", "key_concepts": ["- **Post-retrieval evidence extraction**: Filtering and refining retrieved content before generation", "- **Evidence Identification**: Detecting minimal text spans that directly support the answer", "- **Contextual Compression / Filtering**: Removing irrelevant or redundant content to reduce token usage", "- **Evidence Fusion & Re-ranking**: Combining and ordering evidence snippets for optimal downstream use", "- **Answer-oriented passage ranking**: Prioritizing passages based on answerability, not just topical relevance", "- **Confidence-based dynamic retrieval**: Using LLM internal signals to decide retrieval necessity and ranking", "- **Query-driven summarization**: Condensing retrieved content in response to specific queries", "- **Generative Passage Estimator (GPE)**: Ranking passages by their utility for generation"]}