{"deep-research 或者当前的AI联网搜索中，在拿到了目标网页之后，都是如何如何抽取有效信息组成答案的": [{"paper_id": "2509.06472v1", "title": "[2509.06472v1] Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for Dynamic Retrieval and Reranking", "authors": [], "abstract": "<H1>Computer Science > Information Retrieval</H1><H1>Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for Dynamic Retrieval and Reranking</H1><P>Large Language Models (LLMs) often generate inaccurate responses (hallucinations) when faced with questions beyond their knowledge scope. Retrieval-Augmented Generation (RAG) addresses this by leveraging external knowledge, but a critical challenge remains: determining whether retrieved contexts effectively enhance the model`s ability to answer specific queries. This challenge underscores the importance of knowledge boundary awareness, which current methods-relying on discrete labels or limited signals-fail to address adequately, as they overlook the rich information in LLMs` continuous internal hidden states. To tackle this, we propose a novel post-retrieval knowledge filtering approach. First, we construct a confidence detection model based on LLMs` internal hidden states to quantify how retrieved contexts enhance the model`s confidence. Using this model, we build a preference dataset (NQ_Rerank) to fine-tune a reranker, enabling it to prioritize contexts preferred by the downstream LLM during reranking. Additionally, we introduce Confidence-Based Dynamic Retrieval (CBDR), which adaptively triggers retrieval based on the LLM`s initial confidence in the original question, reducing knowledge conflicts and improving efficiency. Experimental results demonstrate significant improvements in accuracy for context screening and end-to-end RAG performance, along with a notable reduction in retrieval costs while maintaining competitive accuracy.</P><TABLE><TR><TD>Subjects:</TD><TD>Information Retrieval (cs.IR)</TD></TR><TR><TD>Cite as:</TD><TD>arXiv:2509.06472 [cs.IR]</TD></TR><TR><TD> </TD><TD>(or arXiv:2509.06472v1 [cs.IR] for this version)</TD></TR><TR><TD> </TD><TD>https://doi.org/10.48550/arXiv.2509.06472\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer (What is the Explorer?)</P><P>Connected Papers (What is Connected Papers?)</P><P>Litmaps (What is Litmaps?)</P><P>scite Smart Citations (What are Smart Citations?)</P>", "url": "https://arxiv.org/abs/2509.06472v1", "pdf_url": "https://arxiv.org/pdf/2509.06472v1.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2507.17948v2", "title": "VeriRAG: A Post-Retrieval Auditing of Scientific Study Summaries", "authors": [], "abstract": "<H1>VeriRAG: A Post-Retrieval Auditing of Scientific Study Summaries</H1><P>Shubham Mohole1, Hongjun Choi2, Shusen Liu2, Christine Klymko2,\nShashank Kushwaha3, Derek Shi4, Wesam Sakla2, Sainyam Galhotra1, Ruben Glatt2\n1Cornell University, USA\n2Lawrence Livermore National Laboratory, USA\n3University of Illinois Urbana‑Champaign, USA\n4University of California, Los Angeles, USA</P><H6>Abstract</H6><P>Can democratized information gatekeepers and community note writers effectively decide what scientific information to amplify? Lacking domain expertise, such gatekeepers rely on automated reasoning agents that use RAG to ground evidence to cited sources. But such standard RAG systems validate summaries via semantic grounding and suffer from “methodological blindness,” treating all cited evidence as equally valid regardless of rigor. To address this, we introduce VeriRAG, a post-retrieval auditing framework that shifts the task from classification to methodological vulnerability detection. Using private Small Language Models (SLMs), VeriRAG audits source papers against the Veritable taxonomy of statistical rigor. We contribute: (1) a benchmark of 1,730 summaries with realistic, non-obvious perturbations modeled after retracted papers; (2) the auditable Veritable taxonomy; and (3) an operational system that improves Macro F1 by at least 19 points over baselines using GPT-based SLMs, a result that replicates across MISTRAL and Gemma architectures. Given the complexity of detecting non-obvious flaws, we view VeriRAG as a “vulnerability-detection copilot,” providing structured audit trails for human editors. In our experiments, individual human testers found over 80% of the generated audit trails useful for decision-making. We plan to release the dataset and code to support responsible science advocacy.</P><H2>1 Introduction</H2><P>Today, science journalists and social media moderators act as ‘amplification stations’ kasperson1988social, where poorly vetted studies erode public trust ophir2021effects. While high-profile retractions highlight this danger, expert fact-checking remains unscalable or biased. Effective science advocacy therefore requires tools that prioritize evidence quality and transparency national2017communicating. Retrieval-Augmented Generation (RAG) systems lewis2021retrievalaugmentedgenerationknowledgeintensivenlp, which ground claims in sources to reduce hallucination, are integral to such solutions, yet they suffer from a critical flaw: “methodological blindness.”</P><P>Furthermore, even existing specialized scientific claim validation models often focus on abstracts and use benchmarks like SCIFACT wadden-etal-2020-fact or restrict analysis to abstract-level argumentation freedman2024detecting, prioritizing dataset scale over full-text inference. Similarly, validation based on citation patterns or reputational signals chen2025pubguard fails to audit internal methodological rigor, making these systems unreliable for advocacy decisions.</P", "url": "https://arxiv.org/html/2507.17948v2", "pdf_url": "https://arxiv.org/pdf/2507.17948v2.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2507.17948v2", "title": "[2507.17948v2] VERIRAG: A Post-Retrieval Auditing of Scientific Study Summaries", "authors": [], "abstract": "<H1>Computer Science > Information Retrieval</H1><P>Can democratized information gatekeepers and community note writers effectively decide what scientific information to amplify? Lacking domain expertise, such gatekeepers rely on automated reasoning agents that use RAG to ground evidence to cited sources. But such standard RAG systems validate summaries via semantic grounding and suffer from \"methodological blindness,\" treating all cited evidence as equally valid regardless of rigor. To address this, we introduce VERIRAG, a post-retrieval auditing framework that shifts the task from classification to methodological vulnerability detection. Using private Small Language Models (SLMs), VERIRAG audits source papers against the Veritable taxonomy of statistical rigor. We contribute: (1) a benchmark of 1,730 summaries with realistic, non-obvious perturbations modeled after retracted papers; (2) the auditable Veritable taxonomy; and (3) an operational system that improves Macro F1 by at least 19 points over baselines using GPT-based SLMs, a result that replicates across MISTRAL and Gemma architectures. Given the complexity of detecting non-obvious flaws, we view VERIRAG as a \"vulnerability-detection copilot,\" providing structured audit trails for human editors. In our experiments, individual human testers found over 80% of the generated audit trails useful for decision-making. We plan to release the dataset and code to support responsible science advocacy.</P><TABLE><TR><TD>Subjects:</TD><TD>Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)</TD></TR><TR><TD>Cite as:</TD><TD>arXiv:2507.17948 [cs.IR]</TD></TR><TR><TD> </TD><TD>(or arXiv:2507.17948v2 [cs.IR] for this version)</TD></TR><TR><TD> </TD><TD>https://doi.org/10.48550/arXiv.2507.17948\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer (What is the Explorer?)</P><P>Connected Papers (What is Connected Papers?)</P><P>Litmaps (What is Litmaps?)</P><P>scite Smart Citations (What are Smart Citations?)</P>", "url": "https://arxiv.org/abs/2507.17948v2", "pdf_url": "https://arxiv.org/pdf/2507.17948v2.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2509.06472v2", "title": "Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for Dynamic Retrieval and Reranking", "authors": ["Haoxiang Jin", "Ronghan Li", "Zixiang Lu", "Qiguang Miao"], "abstract": "Large Language Models (LLMs) often generate inaccurate responses (hallucinations) when faced with questions beyond their knowledge scope. Retrieval-Augmented Generation (RAG) addresses this by leveraging external knowledge, but a critical challenge remains: determining whether retrieved contexts effectively enhance the model`s ability to answer specific queries. This challenge underscores the importance of knowledge boundary awareness, which current methods-relying on discrete labels or limited signals-fail to address adequately, as they overlook the rich information in LLMs` continuous internal hidden states. To tackle this, we propose a novel post-retrieval knowledge filtering approach. First, we construct a confidence detection model based on LLMs` internal hidden states to quantify how retrieved contexts enhance the model`s confidence. Using this model, we build a preference dataset (NQ_Rerank) to fine-tune a reranker, enabling it to prioritize contexts preferred by the downstream LLM during reranking. Additionally, we introduce Confidence-Based Dynamic Retrieval (CBDR), which adaptively triggers retrieval based on the LLM`s initial confidence in the original question, reducing knowledge conflicts and improving efficiency. Experimental results demonstrate significant improvements in accuracy for context screening and end-to-end RAG performance, along with a notable reduction in retrieval costs while maintaining competitive accuracy.", "url": "http://arxiv.org/abs/2509.06472v2", "pdf_url": "https://arxiv.org/pdf/2509.06472v2", "source": "arxiv", "published_date": "2025-09-08T09:37:20Z", "score_bm25": 0.0}, {"paper_id": "2410.11315", "title": "[2410.11315] SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation", "authors": [], "abstract": "<H1>Computer Science > Computation and Language</H1><H1>SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation</H1><P>Recent studies in Retrieval-Augmented Generation (RAG) have investigated extracting evidence from retrieved passages to reduce computational costs and enhance the final RAG performance, yet it remains challenging. Existing methods heavily rely on heuristic-based augmentation, encountering several issues: (1) Poor generalization due to hand-crafted context filtering; (2) Semantics deficiency due to rule-based context chunking; (3) Skewed length due to sentence-wise filter learning. To address these issues, we propose a model-based evidence extraction learning framework, SEER, optimizing a vanilla model as an evidence extractor with desired properties through self-aligned learning. Extensive experiments show that our method largely improves the final RAG performance, enhances the faithfulness, helpfulness, and conciseness of the extracted evidence, and reduces the evidence length by 9.25 times. The code will be available at this https URL.</P><TABLE><TR><TD>Comments:</TD><TD>15 pages, 6 figures, 5 tables. Accepted by EMNLP 2024 (main)</TD></TR><TR><TD>Subjects:</TD><TD>Computation and Language (cs.CL)</TD></TR><TR><TD>Cite as:</TD><TD>arXiv:2410.11315 [cs.CL]</TD></TR><TR><TD> </TD><TD>(or arXiv:2410.11315v1 [cs.CL] for this version)</TD></TR><TR><TD> </TD><TD>https://doi.org/10.48550/arXiv.2410.11315\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer (What is the Explorer?)</P><P>Connected Papers (What is Connected Papers?)</P><P>Litmaps (What is Litmaps?)</P><P>scite Smart Citations (What are Smart Citations?)</P>", "url": "https://arxiv.org/abs/2410.11315", "pdf_url": "https://arxiv.org/pdf/2410.11315.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2509.08381v1", "title": "Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model", "authors": ["Yu Cheng Chih", "Yong Hao Hou"], "abstract": "Deploying large language models (LLMs) for structured data extraction in domains such as financial compliance reporting, legal document analytics, and multilingual knowledge base construction is often impractical for smaller teams due to the high cost of running large architectures and the difficulty of preparing large, high-quality datasets. Most recent instruction-tuning studies focus on seven-billion-parameter or larger models, leaving limited evidence on whether much smaller models can work reliably under low-resource, multi-task conditions. This work presents ETLCH, a billion-parameter LLaMA-based model fine-tuned with low-rank adaptation on only a few hundred to one thousand samples per task for JSON extraction, knowledge graph extraction, and named entity recognition. Despite its small scale, ETLCH outperforms strong baselines across most evaluation metrics, with substantial gains observed even at the lowest data scale. These findings demonstrate that well-tuned small models can deliver stable and accurate structured outputs at a fraction of the computational cost, enabling cost-effective and reliable information extraction pipelines in resource-constrained environments.", "url": "http://arxiv.org/abs/2509.08381v1", "pdf_url": "https://arxiv.org/pdf/2509.08381v1", "source": "arxiv", "published_date": "2025-09-10T08:19:07Z", "score_bm25": 0.0}, {"paper_id": "2103.16929v1", "title": "Deep Neural Approaches to Relation Triplets Extraction: A Comprehensive Survey", "authors": ["Tapas Nayak", "Navonil Majumder", "Pawan Goyal", "Soujanya Poria"], "abstract": "Recently, with the advances made in continuous representation of words (word embeddings) and deep neural architectures, many research works are published in the area of relation extraction and it is very difficult to keep track of so many papers. To help future research, we present a comprehensive review of the recently published research works in relation extraction. We mostly focus on relation extraction using deep neural networks which have achieved state-of-the-art performance on publicly available datasets. In this survey, we cover sentence-level relation extraction to document-level relation extraction, pipeline-based approaches to joint extraction approaches, annotated datasets to distantly supervised datasets along with few very recent research directions such as zero-shot or few-shot relation extraction, noise mitigation in distantly supervised datasets. Regarding neural architectures, we cover convolutional models, recurrent network models, attention network models, and graph convolutional models in this survey.", "url": "http://arxiv.org/abs/2103.16929v1", "pdf_url": "https://arxiv.org/pdf/2103.16929v1", "source": "arxiv", "published_date": "2021-03-31T09:27:15Z", "score_bm25": 0.0}, {"paper_id": null, "title": "SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation - ACL Anthology", "authors": [], "abstract": "<P>Important: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF.</P><P>Title Adjust the title. Retain tags such as <fixed-case>.</P><P>Abstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>.</P><P>Verification against PDF Ensure that the new title/authors match the snapshot below. (If there is no snapshot or it is too small, consult the PDF.)Authors concatenated from the text boxes above:</P><P>ALL author names match the snapshot above—including middle initials, hyphens, and accents.</P><H5>Abstract</H5><P>Recent studies in Retrieval-Augmented Generation (RAG) have investigated extracting evidence from retrieved passages to reduce computational costs and enhance the final RAG performance, yet it remains challenging. Existing methods heavily rely on heuristic-based augmentation, encountering several issues: (1) Poor generalization due to hand-crafted context filtering; (2) Semantics deficiency due to rule-based context chunking; (3) Skewed length due to sentence-wise filter learning. To address these issues, we propose a model-based evidence extraction learning framework, SEER, optimizing a vanilla model as an evidence extractor with desired properties through self-aligned learning. Extensive experiments show that our method largely improves the final RAG performance, enhances the faithfulness, helpfulness, and conciseness of the extracted evidence, and reduces the evidence length by 9.25 times. The code will be available at https://github.com/HITsz-TMG/SEER.</P><P>Xinping Zhao, Dongfang Li, Yan Zhong, Boren Hu, Yibin Chen, Baotian Hu, and Min Zhang. 2024. SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 3027–3041, Miami, Florida, USA. Association for Computational Linguistics.</P><P>More options…</P>", "url": "https://aclanthology.org/2024.emnlp-main.178/", "pdf_url": "https://aclanthology.org/2024.emnlp-main.178.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2207.01888v2", "title": "Keyword Extraction in Scientific Documents", "authors": ["Susie Xi Rao", "Piriyakorn Piriyatamwong", "Parijat Ghoshal", "Sara Nasirian", "Emmanuel de Salis", "Sandra Mitrović", "Michael Wechner", "Vanya Brucker", "Peter Egger", "Ce Zhang"], "abstract": "The scientific publication output grows exponentially. Therefore, it is increasingly challenging to keep track of trends and changes. Understanding scientific documents is an important step in downstream tasks such as knowledge graph building, text mining, and discipline classification. In this workshop, we provide a better understanding of keyword and keyphrase extraction from the abstract of scientific publications.", "url": "http://arxiv.org/abs/2207.01888v2", "pdf_url": "https://arxiv.org/pdf/2207.01888v2", "source": "arxiv", "published_date": "2022-07-05T08:33:47Z", "score_bm25": 0.0}, {"paper_id": null, "title": "Label-dependent Feature Extraction in Social Networks for Node Classification", "authors": ["Tomasz Kajdanowicz", "Przemyslaw Kazienko", "Piotr Doskocz"], "abstract": "A new method of feature extraction in the social network for within-network classification is proposed in the paper. The method provides new features calculated by combination of both: network structure information and class labels assigned to nodes. The influence of various features on classification performance has also been studied. The experiments on real-world data have shown that features created owing to the proposed method can lead to significant improvement of classification accuracy.", "url": "http://arxiv.org/abs/1303.0095v1", "pdf_url": "https://arxiv.org/pdf/1303.0095v1", "source": "arxiv", "published_date": "2013-03-01T06:31:02Z", "score_bm25": 0.0}, {"paper_id": "1904.09765v1", "title": "hf0: A hybrid pitch extraction method for multimodal voice", "authors": ["Pradeep Rengaswamy", "Gurunath Reddy M", "Krothapalli Sreenivasa Rao"], "abstract": "Pitch or fundamental frequency (f0) extraction is a fundamental problem studied extensively for its potential applications in speech and clinical applications. In literature, explicit mode specific (modal speech or singing voice or emotional/ expressive speech or noisy speech) signal processing and deep learning f0 extraction methods that exploit the quasi periodic nature of the signal in time, harmonic property in spectral or combined form to extract the pitch is developed. Hence, there is no single unified method which can reliably extract the pitch from various modes of the acoustic signal. In this work, we propose a hybrid f0 extraction method which seamlessly extracts the pitch across modes of speech production with very high accuracy required for many applications. The proposed hybrid model exploits the advantages of deep learning and signal processing methods to minimize the pitch detection error and adopts to various modes of acoustic signal. Specifically, we propose an ordinal regression convolutional neural networks to map the periodicity rich input representation to obtain the nominal pitch classes which drastically reduces the number of classes required for pitch detection unlike other deep learning approaches. Further, the accurate f0 is estimated from the nominal pitch class labels by filtering and autocorrelation. We show that the proposed method generalizes to the unseen modes of voice production and various noises for large scale datasets. Also, the proposed hybrid model significantly reduces the learning parameters required to train the deep model compared to other methods. Furthermore,the evaluation measures showed that the proposed method is significantly better than the state-of-the-art signal processing and deep learning approaches.", "url": "http://arxiv.org/abs/1904.09765v1", "pdf_url": "https://arxiv.org/pdf/1904.09765v1", "source": "arxiv", "published_date": "2019-04-22T08:08:12Z", "score_bm25": 0.0}, {"paper_id": null, "title": "SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation", "authors": [], "abstract": "<P>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 3027–3041 November 12-16, 2024 ©2024 Association for Computational Linguistics</P><P>SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation</P><P>Xinping Zhao1, Dongfang Li1, Yan Zhong2, Boren Hu3, Yibin Chen4, Baotian Hu1B, Min Zhang1 1Harbin Institute of Technology (Shenzhen),2Peking University,</P><P>3Zhejiang University,4Huawei Cloud, Huawei Technologies Ltd. zhaoxinping@stu.hit.edu.cn , {zhongyan946, huboren99}@gmail.com , chenyibin4@huawei.com , {lidongfang, hubaotian, zhangmin2021}@hit.edu.cn Abstract</P><P>Recent studies in Retrieval-Augmented Gener- ation (RAG) have investigated extracting evi- dence from retrieved passages to reduce com- putational costs and enhance the nal RAG performance, yet it remains challenging. Ex- isting methods heavily rely on heuristic-based augmentation, encountering several issues: (1) Poorgeneralizationduetohand-craftedcontext ltering; (2) Semantics deciency due to rule- based context chunking; (3) Skewed length due to sentence-wise lter learning. To address these issues, we propose a model-based evi- dence extraction learning framework, SEER, optimizing a vanilla model as an evidence ex- tractor with desired properties through self- aligned learning. Extensive experiments show thatourmethodlargelyimprovesthenalRAG performance, enhances the faithfulness, help- fulness, and conciseness of the extracted ev- idence, and reduces the evidence length by 9.25 times. The code will be available at https://github.com/HITsz-TMG/SEER .</P><P>1 Introduction</P><P>Recent years have witnessed the prevailing winds of Retrieval-augmented Generation (RAG), which is a popular paradigm for improving the perfor- mances of Large Language Models (LLMs) in var- ious downstream tasks, such as question answer- ing, making the output more reliable (Lewis et al., 2020;Chen et al.,2023;Jiang et al.,2023b;Ram et al.,2023), interpretable (Guu et al.,2020;Louis et al.,2024), and adaptable (Xu et al.,2023;Za- kka et al.,2024). Traditional practices (Karpukhin et al.,2020;Min et al.,2019) often involve provid- ing top-retrieved passages as the input context to LLMs without discrimination. However, imperfect retrievalsystemsfrequentlyyieldirrelevantcontent. Furthermore, indiscriminately feeding all retrieved content to LLMs will cause input redundancy, im-</P><P>BCorresponding author.</P><P>posing a high computational cost and making them prone to hallucination (Shi et al.,2023). Ideally, LLMs should be grounded on support- ing content that is both highly helpful to address userinputandsufcientlyconcisetofacilitateinfer- ence speed. However, it is practically impossible for imperfect retrieval systems to achieve such an ideal grounding solely (Wang et al.,2023). In fact, top-retrieved passages usually compose supporting and distracting content, inicting a heavy blow on LLMstrainedwithhigh-qualitycorporatogenerate the correct ou", "url": "https://aclanthology.org/2024.emnlp-main.178.pdf", "pdf_url": "https://aclanthology.org/2024.emnlp-main.178.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2411.00469v1", "title": "MIRFLEX: Music Information Retrieval Feature Library for Extraction", "authors": ["Anuradha Chopra", "Abhinaba Roy", "Dorien Herremans"], "abstract": "This paper introduces an extendable modular system that compiles a range of music feature extraction models to aid music information retrieval research. The features include musical elements like key, downbeats, and genre, as well as audio characteristics like instrument recognition, vocals/instrumental classification, and vocals gender detection. The integrated models are state-of-the-art or latest open-source. The features can be extracted as latent or post-processed labels, enabling integration into music applications such as generative music, recommendation, and playlist generation. The modular design allows easy integration of newly developed systems, making it a good benchmarking and comparison tool. This versatile toolkit supports the research community in developing innovative solutions by providing concrete musical features.", "url": "http://arxiv.org/abs/2411.00469v1", "pdf_url": "https://arxiv.org/pdf/2411.00469v1", "source": "arxiv", "published_date": "2024-11-01T09:34:36Z", "score_bm25": 0.0}, {"paper_id": "2507.15586v3", "title": "[2507.15586v3] Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation", "authors": [], "abstract": "<H1>Computer Science > Computation and Language</H1><P>Retrieval-Augmented Generation (RAG) effectively improves the accuracy of Large Language Models (LLMs). However, retrieval noises significantly impact the quality of LLMs' generation, necessitating the development of denoising mechanisms. Previous methods extract evidence straightforwardly without explicit thinking, which risks filtering out key clues and struggles with generalization. To this end, we propose LEAR, which learns to extract rational evidence by (1) explicitly reasoning to identify potential cues within retrieval contents first, and then (2) consciously extracting to avoid omitting any key cues helpful for answering questions. Specifically, we frame evidence reasoning and evidence extraction into one unified response for end-to-end training; apply knowledge token masks for disentanglement to derive reasoning-based and extraction-based answers; and devise three types of verifiable reward functions, including answer, length, and format, to update the model via the policy optimization algorithm. Extensive experiments on three benchmark datasets show the effectiveness of LEAR, providing compact and high-quality evidence, improving the accuracy of downstream tasks, and promoting effective application in online RAG systems.</P><TABLE><TR><TD>Comments:</TD><TD>16 pages, 7 Figures, 10 Tables</TD></TR><TR><TD>Subjects:</TD><TD>Computation and Language (cs.CL)</TD></TR><TR><TD>Cite as:</TD><TD>arXiv:2507.15586 [cs.CL]</TD></TR><TR><TD>(or arXiv:2507.15586v3 [cs.CL] for this version)</TD></TR><TR><TD>https://doi.org/10.48550/arXiv.2507.15586\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><H2>Access Paper:</H2><H3>Bookmark</H3><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer(What is the Explorer?)</P><P>Connected Papers(What is Connected Papers?)</P><P>Litmaps(What is Litmaps?)</P><P>scite Smart Citations(What are Smart Citations?)</P>", "url": "https://arxiv.org/abs/2507.15586v3", "pdf_url": "https://arxiv.org/pdf/2507.15586v3.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2011.00577v3", "title": "FusiformNet: Extracting Discriminative Facial Features on Different Levels", "authors": ["Kyo Takano"], "abstract": "Over the last several years, research on facial recognition based on Deep Neural Network has evolved with approaches like task-specific loss functions, image normalization and augmentation, network architectures, etc. However, there have been few approaches with attention to how human faces differ from person to person. Premising that inter-personal differences are found both generally and locally on the human face, I propose FusiformNet, a novel framework for feature extraction that leverages the nature of discriminative facial features. Tested on Image-Unrestricted setting of Labeled Faces in the Wild benchmark, this method achieved a state-of-the-art accuracy of 96.67% without labeled outside data, image augmentation, normalization, or special loss functions. Likewise, the method also performed on a par with previous state-of-the-arts when pre-trained on CASIA-WebFace dataset. Considering its ability to extract both general and local facial features, the utility of FusiformNet may not be limited to facial recognition but also extend to other DNN-based tasks.", "url": "http://arxiv.org/abs/2011.00577v3", "pdf_url": "https://arxiv.org/pdf/2011.00577v3", "source": "arxiv", "published_date": "2020-11-01T18:00:59Z", "score_bm25": 0.0}, {"paper_id": "2507.15586", "title": "Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation", "authors": [], "abstract": "<H1>Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation</H1><P>Xinping Zhao1, Shouzheng Huang1, Yan Zhong2, Xinshuo Hu, Meishan Zhang1, Baotian Hu1B, Min Zhang1</P><P>1Harbin Institute of Technology (Shenzhen), 2Peking University</P><P>{zhaoxinping, 210110129}@stu.hit.edu.cn, zhongyan@stu.pku.edu.cn, yanshek.woo@gmail.com, mason.zms@gmail.com, {hubaotian, zhangmin2021}@hit.edu.cn</P><H3>Abstract</H3><P>Retrieval-Augmented Generation (RAG) ef-fectively improves the accuracy of Large Language Models (LLMs). However, re-trieval noises significantly impact the qual-ity of LLMs’ generation, necessitating the development of denoising mechanisms. Previous methods extract evidence straight-forwardly without explicit thinking, which risks filtering out key clues and struggles with generalization. To this end, we propose EviOmni, which learns to extract rational evidence by (1) explicitly reasoning to iden-tify potential cues within retrieval contents first, and then (2) consciously extracting to avoid omitting any key cues helpful for an-swering questions. Specifically, we frame evidence reasoning and evidence extraction into one unified response for end-to-end training; apply knowledge token masks for disentanglement to derive reasoning-based and extraction-based answers; and devise three types of verifiable reward functions, including answer, length, and format, to up-date the model via the policy optimization algorithm. Extensive experiments on three benchmark datasets show the effectiveness of EviOmni, providing compact and high-quality evidence, improving the accuracy of downstream tasks, and promoting effective application in online RAG systems1.</P><H3>1 Introduction</H3><P>Retrieval-Augmented Generation (RAG) prevails in Large Language Models (LLMs). It has been shown highly effective for many knowledge-intensive tasks (Lewis et al., 2020; Wu et al., 2022), such as open-domain question answer-ing (Shi et al., 2024; Trivedi et al., 2023), fact-checking (Du et al., 2023; Zhao et al., 2024b), and</P><P>BCorresponding author.</P><P>1The code, data, and models will be available at https: //github.com/HITsz-TMG/EviOmni.</P><P>Figure 1: Motivating example, where key clues are marked in green: 1 The key clues are filtered out, and LLMs answer incorrectly; 2 The key clues are ex-tracted successfully, guided by the evidence reasoning.</P><P>dialog generation (Izacard et al., 2023; Thoppilan et al., 2022), to produce more faithful and reli-able outputs. In ideal conditions, LLMs should be grounded on purely relevant retrieval contents to generate accurate output and also facilitate infer-ence. However, due to imperfect retrieval systems or noisy data in the retrieval corpus (Wang et al., 2023; Wei et al., 2024; Zhao et al., 2024a), the re-trieval contents usually contain lots of irrelevant or noisy snippets, which distract LLMs’ attention and also inflict a heavy blow on the generation quality of LLMs. Therefor", "url": "https://arxiv.org/pdf/2507.15586", "pdf_url": "https://arxiv.org/pdf/2507.15586.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": null, "title": "https://neurips.cc/media/neurips-2025/Slides/129850.pdf", "authors": [], "abstract": "<H1>Biomedical Evidence Retrieval with Agentic RAG and Dual Text Encoders</H1><P>Dhruv Goyal, Ema Seibert, Ryan Ding, Matteo Migliarini, Kevin Zhu</P><P>Agentic RAG framework for evidence retrieval, using iterative query refinement across notes.</P><P>Retrieval-Augmented Generation (RAG) has emerged as a leading approach for evidence-based retrieval, combining dense retrieval with generation. In medicine, this paradigm was adapted domain-specific models like BioBERT to handle specialized terminology, yet traditional RAG pipelines are often static, retrieving once without adapting their reasoning. A more advanced paradigm, Agentic RAG, extends this by embedding autonomous decision-making and iterative reflection into the retrieval loop.</P><UL><LI><P>dual domain-specific encoders</P></LI><LI><P>self-critique loops</P></LI><LI><P>benchmarks on established biomedical QA datasets</P></LI><LI><P>Patients-PMC benchmark to assess generalization for clinical discovery</P></LI><LI><P>corrective feedback or query routing to achieve more adaptive reasoning</P></LI><LI><P>adaptive retrieval for clinical decision support</P></LI></UL><H3>Workshop on KNOwledge GRaphs & Agentic Systems</H3><P>Figure 1: Hybrid biomedical RAG with iterative self-critique. Evidence from PubMed (literature) and MIMIC-IV (clinical notes) is retrieved via domain-specific encoders and re-ranked. An agent cycles between reflect and refine, yielding a final, evidence-grounded response.</P><H3>Conclusion</H3><H3>Methodology Developing more reliable tools for evidence based medicine</H3><P>Embedding clinical notes We demonstrated the effectiveness of an agentic RAG framework for complex biomedical retrieval. Our system Our system employs an agentic RAG framework that iteratively achieved competitive performance on the PMC-Patients and refines search queries and integrates evidence from biomedical PubMedQA benchmarks, highlighting the advantages of literature (PubMed) and clinical notes (MIMIC-IV). The core is a agentic strategies over static pipelines. dual encoder as shown in figure 1. We encode queries and documents using two specialized models: PubMedBERT for literature and ClinicalBERT for clinical notes, enabling parallel searches. Retrieved documents are then merged and refined using a cross-encoder reranker.</P><H3>Results</H3><H3>Testing and Comparing to Baselines</H3><UL><LI><P>We evaluate our agentic retrieval system on the PMC-Patients benchmark; covering Patient-to-Article Retrieval (PAR) and Patient-to-Patient Retrieval (PPR); and the reasoning-free setting of PubMedQA.</P></LI><LI><P>As shown in Table 1, our framework achieves competitive results across all tasks. While the model also performs competitively on the more challenging PPR task, the PAR scores highlight the system’s strength in precise evidence matching.</P></LI><LI><P>On PubMedQA, our framework attains an accuracy of 82.09%, outperforming key baselines like BioBERT (80.80%) shown in table 2.</P></LI></UL>", "url": "https://neurips.cc/media/neurips-2025/Slides/129850.pdf", "pdf_url": "https://neurips.cc/media/neurips-2025/Slides/129850.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "1707.06112v1", "title": "Microblog Retrieval for Post-Disaster Relief: Applying and Comparing Neural IR Models", "authors": ["Prannay Khosla", "Moumita Basu", "Kripabandhu Ghosh", "Saptarshi Ghosh"], "abstract": "Microblogging sites like Twitter and Weibo have emerged as important sourcesof real-time information on ongoing events, including socio-political events, emergency events, and so on. For instance, during emergency events (such as earthquakes, floods, terror attacks), microblogging sites are very useful for gathering situational information in real-time. During such an event, typically only a small fraction of the microblogs (tweets) posted are relevant to the information need. Hence, it is necessary to design effective methodologies for microblog retrieval, so that the relevant tweets can be automatically extracted from large sets of documents (tweets).\n  In this work, we apply and compare various neural network-based IR models for microblog retrieval for a specific application, as follows. In a disaster situation, one of the primary and practical challenges in coordinating the post-disaster relief operations is to know about what resources are needed and what resources are available in the disaster-affected area. Thus, in this study, we focus on extracting these two specific types of microblogs or tweets namely need tweets and avail tweets, which are tweets which define some needs of the people and the tweets which offer some solutions or aid for the people, respectively.", "url": "http://arxiv.org/abs/1707.06112v1", "pdf_url": "https://arxiv.org/pdf/1707.06112v1", "source": "arxiv", "published_date": "2017-07-19T14:28:05Z", "score_bm25": 0.0}, {"paper_id": "2005.12739v1", "title": "An Effective Pipeline for a Real-world Clothes Retrieval System", "authors": ["Yang-Ho Ji", "HeeJae Jun", "Insik Kim", "Jongtack Kim", "Youngjoon Kim", "Byungsoo Ko", "Hyong-Keun Kook", "Jingeun Lee", "Sangwon Lee", "Sanghyuk Park"], "abstract": "In this paper, we propose an effective pipeline for clothes retrieval system which has sturdiness on large-scale real-world fashion data. Our proposed method consists of three components: detection, retrieval, and post-processing. We firstly conduct a detection task for precise retrieval on target clothes, then retrieve the corresponding items with the metric learning-based model. To improve the retrieval robustness against noise and misleading bounding boxes, we apply post-processing methods such as weighted boxes fusion and feature concatenation. With the proposed methodology, we achieved 2nd place in the DeepFashion2 Clothes Retrieval 2020 challenge.", "url": "http://arxiv.org/abs/2005.12739v1", "pdf_url": "https://arxiv.org/pdf/2005.12739v1", "source": "arxiv", "published_date": "2020-05-26T14:08:49Z", "score_bm25": 0.0}, {"paper_id": "2412.12559", "title": "[2412.12559] EXIT: Context-Aware Extractive Compression for Enhancing Retrieval-Augmented Generation", "authors": [], "abstract": "<H1>Computer Science > Computation and Language</H1><H1>EXIT: Context-Aware Extractive Compression for Enhancing Retrieval-Augmented Generation</H1><P>We introduce EXIT, an extractive context compression framework that enhances both the effectiveness and efficiency of retrieval-augmented generation (RAG) in question answering (QA). Current RAG systems often struggle when retrieval models fail to rank the most relevant documents, leading to the inclusion of more context at the expense of latency and accuracy. While abstractive compression methods can drastically reduce token counts, their token-by-token generation process significantly increases end-to-end latency. Conversely, existing extractive methods reduce latency but rely on independent, non-adaptive sentence selection, failing to fully utilize contextual information. EXIT addresses these limitations by classifying sentences from retrieved documents - while preserving their contextual dependencies - enabling parallelizable, context-aware extraction that adapts to query complexity and retrieval quality. Our evaluations on both single-hop and multi-hop QA tasks show that EXIT consistently surpasses existing compression methods and even uncompressed baselines in QA accuracy, while also delivering substantial reductions in inference time and token count. By improving both effectiveness and efficiency, EXIT provides a promising direction for developing scalable, high-quality QA solutions in RAG pipelines. Our code is available at this https URL</P><TABLE><TR><TD>Comments:</TD><TD>Findings of ACL 2025</TD></TR><TR><TD>Subjects:</TD><TD>Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)</TD></TR><TR><TD>Cite as:</TD><TD>arXiv:2412.12559 [cs.CL]</TD></TR><TR><TD> </TD><TD>(or arXiv:2412.12559v3 [cs.CL] for this version)</TD></TR><TR><TD> </TD><TD>https://doi.org/10.48550/arXiv.2412.12559\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer (What is the Explorer?)</P><P>Connected Papers (What is Connected Papers?)</P><P>Litmaps (What is Litmaps?)</P><P>scite Smart Citations (What are Smart Citations?)</P>", "url": "https://arxiv.org/abs/2412.12559", "pdf_url": "https://arxiv.org/pdf/2412.12559.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2409.13385", "title": "[2409.13385] Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": [], "abstract": "<H6>Abstract</H6><P>Large Language Models (LLMs) showcase remarkable abilities, yet they struggle with limitations such as hallucinations, outdated knowledge, opacity, and inexplicable reasoning. To address these challenges, Retrieval-Augmented Generation (RAG) has proven to be a viable solution, leveraging external databases to improve the consistency and coherence of generated content, especially valuable for complex, knowledge-rich tasks, and facilitates continuous improvement by leveraging domain-specific insights. By combining the intrinsic knowledge of LLMs with the vast, dynamic repositories of external databases, RAG achieves a synergistic effect. However, RAG is not without its limitations, including a limited context window, irrelevant information, and the high processing overhead for extensive contextual data. In this comprehensive work, we explore the evolution of Contextual Compression paradigms, providing an in-depth examination of the field. Finally, we outline the current challenges and suggest potential research and development directions, paving the way for future advancements in this area. 111Resources are available at https://github.com/SrGrace/Contextual-Compression</P><H2>1 Introduction</H2><P>The pioneering accomplishments of large language models (LLMs) have galvanized research initiatives across both industrial and academic spheres. These LLMs showcase their capacity to converse with humans in a natural and articulate manner, excelling across various tasks such as document summarization, Q&A systems, conversational AI, and coding assistants. Despite their advancements, LLMs continue to struggle with tasks that require specialized knowledge or domain-specific expertise. Kandpal et al. (2023). Notably, they may produce “hallucinations” Zhang et al. (2023) when confronted with out-of-scope queries or requests that necessitate up-to-date knowledge. To address these challenges, Retrieval-Augmented Generation (RAG) leverages external knowledge bases to retrieve relevant document snippets, utilizing semantic similarity metrics to identify the most pertinent information. By tapping into external knowledge sources, RAG successfully alleviates the issue of generating inaccurate content, thereby increasing the reliability of LLMs and paving the way for their widespread adoption in real-world applications.</P><P>However, RAG also has its challenges. One issue is that when retrieving relevant documents, the important information may be buried in a large amount of irrelevant text, leading to inefficient and poor responses. Another challenge is that current language models have a limited input length, which causes their performance to decline when processing lengthy documents, such as academic articles, research papers, or literary works. This constraint has fueled research into developing methods to increase the input length while maintaining the model’s accuracy and efficiency.</P><P>This paper aims to shed light on the latest advance", "url": "https://ar5iv.labs.arxiv.org/html/2409.13385", "pdf_url": "https://arxiv.org/pdf/2409.13385.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2510.25518v1", "title": "Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation", "authors": ["Thomas Cook", "Richard Osuagwu", "Liman Tsatiashvili", "Vrynsia Vrynsia", "Koustav Ghosal", "Maraim Masoud", "Riccardo Mattivi"], "abstract": "Retrieval-Augmented Generation (RAG) systems often face limitations in specialized domains such as fintech, where domain-specific ontologies, dense terminology, and acronyms complicate effective retrieval and synthesis. This paper introduces an agentic RAG architecture designed to address these challenges through a modular pipeline of specialized agents. The proposed system supports intelligent query reformulation, iterative sub-query decomposition guided by keyphrase extraction, contextual acronym resolution, and cross-encoder-based context re-ranking. We evaluate our approach against a standard RAG baseline using a curated dataset of 85 question--answer--reference triples derived from an enterprise fintech knowledge base. Experimental results demonstrate that the agentic RAG system outperforms the baseline in retrieval precision and relevance, albeit with increased latency. These findings suggest that structured, multi-agent methodologies offer a promising direction for enhancing retrieval robustness in complex, domain-specific settings.", "url": "http://arxiv.org/abs/2510.25518v1", "pdf_url": "https://arxiv.org/pdf/2510.25518v1", "source": "arxiv", "published_date": "2025-10-29T13:41:36Z", "score_bm25": 0.0}, {"paper_id": "2409.03708v2", "title": "RAG based Question-Answering for Contextual Response Prediction System", "authors": ["Sriram Veturi", "Saurabh Vaichal", "Reshma Lal Jagadheesh", "Nafis Irtiza Tripto", "Nian Yan"], "abstract": "Large Language Models (LLMs) have shown versatility in various Natural Language Processing (NLP) tasks, including their potential as effective question-answering systems. However, to provide precise and relevant information in response to specific customer queries in industry settings, LLMs require access to a comprehensive knowledge base to avoid hallucinations. Retrieval Augmented Generation (RAG) emerges as a promising technique to address this challenge. Yet, developing an accurate question-answering framework for real-world applications using RAG entails several challenges: 1) data availability issues, 2) evaluating the quality of generated content, and 3) the costly nature of human evaluation. In this paper, we introduce an end-to-end framework that employs LLMs with RAG capabilities for industry use cases. Given a customer query, the proposed system retrieves relevant knowledge documents and leverages them, along with previous chat history, to generate response suggestions for customer service agents in the contact centers of a major retail company. Through comprehensive automated and human evaluations, we show that this solution outperforms the current BERT-based algorithms in accuracy and relevance. Our findings suggest that RAG-based LLMs can be an excellent support to human customer service representatives by lightening their workload.", "url": "http://arxiv.org/abs/2409.03708v2", "pdf_url": "https://arxiv.org/pdf/2409.03708v2", "source": "arxiv", "published_date": "2024-09-05T17:14:23Z", "score_bm25": 0.0}, {"paper_id": "2507.22931", "title": "[2507.22931] Enhancing RAG Efficiency with Adaptive Context Compression", "authors": [], "abstract": "<H1>Computer Science > Computation and Language</H1><H1>Enhancing RAG Efficiency with Adaptive Context Compression</H1><P>Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but incurs significant inference costs due to lengthy retrieved contexts. While context compression mitigates this issue, existing methods apply fixed compression rates, over-compressing simple queries or under-compressing complex ones. We propose Adaptive Context Compression for RAG (ACC-RAG), a framework that dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without sacrificing accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal sufficient information, akin to human skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms fixed-rate methods and matches/unlocks over 4 times faster inference versus standard RAG while maintaining or improving accuracy.</P><TABLE><TR><TD>Subjects:</TD><TD>Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</TD></TR><TR><TD>Cite as:</TD><TD>arXiv:2507.22931 [cs.CL]</TD></TR><TR><TD> </TD><TD>(or arXiv:2507.22931v3 [cs.CL] for this version)</TD></TR><TR><TD> </TD><TD>https://doi.org/10.48550/arXiv.2507.22931\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer (What is the Explorer?)</P><P>Connected Papers (What is Connected Papers?)</P><P>Litmaps (What is Litmaps?)</P><P>scite Smart Citations (What are Smart Citations?)</P>", "url": "https://arxiv.org/abs/2507.22931", "pdf_url": "https://arxiv.org/pdf/2507.22931.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2507.22931v3", "title": "Enhancing RAG Efficiency with Adaptive Context Compression", "authors": [], "abstract": "<P>Enhancing RAG Efficiency with Adaptive Context Compression</P><P>Shuyu Guo*</P><P>Shandong University Qingdao, China guoshuyu225@gmail.com Shuo Zhang Bloomberg London, United Kingdom szhang611@bloomberg.net Zhaochun Ren†</P><P>Leiden University Leiden, The Netherlands z.ren@liacs.leidenuniv.nl Abstract</P><P>Retrieval-augmented generation (RAG) en- hances large language models (LLMs) with external knowledge but incurs significant in- ference costs due to lengthy retrieved contexts. While context compression mitigates this is- sue, existing methods apply fixed compres- sion rates—over-compressing simple queries or under-compressing complex ones. We propose Adaptive Context Compression for RAG (ACC- RAG), a framework that dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without loss of accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal suf- ficient information, akin to human skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms fixed-rate methods and unlocks >4×faster inference versus standard RAG while maintaining or improving accuracy.</P><P>1 Introduction</P><P>Large Language Models (LLMs) are pre-trained on massive datasets, encoding vast knowledge within billions of parameters. While they excel at many tasks, their parametric knowledge often falls short for knowledge-intensive applications. Retrieval- Augmented Generation (RAG) addresses this limi- tation by extending the model’s knowledge bound- aries through external context retrieval (Gao et al., 2023). However, integrating lengthy retrieved con- texts into prompts increases inference costs and may exceed LLMs’ context window limits (Cheva- lier et al.,2023). Context compression mitigates this issue by transforming long contexts into shorter input se- quences. Existing methods fall into two cate- gories: (1) Lexical-based compression, which re- duces input length by preserving key tokens (Wang</P><P>*Work performed during a visit to Leiden University.</P><P>†Corresponding Author.</P><P>What album is sacrifice by  elton john on? Document:  Sacrifice (song) is a ballad performed by musician Elton John. The song appears on the 1989 album \"Sleeping with the Past ´« Output: Sleeping with the Past Vanilla RAG Document: with Context Compression Output: Too Low for Zero Document: Output: Sleeping with the Past [X] [X] [X] [X] [X] [X] [X] [X] [X] Document: Output: Sleeping with the Past with Adaptive Context Compression Redundant document embeddings Insufficient document embeddings Perfect document embeddings Too long document [X] [X] [X] [X] [X] [X] [X] [X]  </P><P>Figure 1: An example of a retrieval-augmented model with different context compression methods.</P><P>et al.,2023b) or generating summaries (Xu et al., 2024a); and (2) Embedding-based compression, which encodes text into dense embeddings (Ge et al.,2024) for inference. Embedding-based meth- o", "url": "https://arxiv.org/pdf/2507.22931v3", "pdf_url": "https://arxiv.org/pdf/2507.22931v3.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2507.22931v2", "title": "Dynamic Context Compression for Efficient RAG", "authors": [], "abstract": "<P>Dynamic Context Compression for Efficient RAG</P><P>Shuyu Guo Shandong University Qingdao, China guoshuyu225@gmail.com Zhaochun Ren Leiden University Leiden, The Netherlands z.ren@liacs.leidenuniv.nl Abstract</P><P>Retrieval-augmented generation (RAG) en- hances large language models (LLMs) with external knowledge but incurs significant in- ference costs due to lengthy retrieved contexts. While context compression mitigates this is- sue, existing methods apply fixed compres- sion rates—over-compressing simple queries or under-compressing complex ones. We pro- pose Adaptive Context Compression for RAG (ACC-RAG), a framework that dynamically ad- justs compression rates based on input com- plexity, optimizing inference efficiency with- out sacrificing accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal sufficient information, akin to human skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms fixed-rate methods and matches/unlocks >4× faster infer- ence versus standard RAG while maintaining or improving accuracy.</P><P>1 Introduction</P><P>Large Language Models (LLMs) are pre-trained on massive datasets, encoding vast knowledge within billions of parameters. While they excel at many tasks, their parametric knowledge often falls short for knowledge-intensive applications. Retrieval- Augmented Generation (RAG) addresses this limi- tation by extending the model’s knowledge bound- aries through external context retrieval (Gao et al., 2023). However, integrating lengthy retrieved con- texts into prompts increases inference costs and may exceed LLMs’ context window limits (Cheva- lier et al.,2023)2. Context compression mitigates this issue by transforming long contexts into shorter input se- quences. Existing methods fall into two cate- gories: (1) Lexical-based compression, which re- duces input length by preserving key tokens (Wang et al.,2023b) or generating summaries (Xu et al.,</P><P>What album is sacrifice by  elton john on? Document:  Sacrifice (song) is a ballad performed by musician Elton John. The song appears on the 1989 album \"Sleeping with the Past ´« Output: Sleeping with the Past Vanilla RAG Document: with Context Compression Output: Too Low for Zero Document: Output: Sleeping with the Past [X] [X] [X] [X] [X] [X] [X] [X] [X] Document: Output: Sleeping with the Past with Adaptive Context Compression Redundant document embeddings Insufficient document embeddings Perfect document embeddings Too long document [X] [X] [X] [X] [X] [X] [X] [X]  </P><P>Figure 1: A case of the retrieval augmented model with different context compression methods.</P><P>2024a); and (2) Embedding-based compression, which encodes text into dense embeddings (Ge et al.,2024) for inference. Embedding-based meth- ods have been proven more efficient and effec- tive (Cheng et al.,2024), typically employing a compressor trained in two phases: first through pre- training (e.g., v", "url": "https://arxiv.org/pdf/2507.22931v2", "pdf_url": "https://arxiv.org/pdf/2507.22931v2.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2507.05633", "title": "[2507.05633] SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression", "authors": [], "abstract": "<H1>Computer Science > Computation and Language</H1><H1>SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression</H1><P>Retrieval-augmented Generation (RAG) extends large language models (LLMs) with external knowledge but faces key challenges: restricted effective context length and redundancy in retrieved documents. Pure compression-based approaches reduce input size but often discard fine-grained details essential for factual accuracy. We propose SARA, a unified RAG framework that balances local precision and global knowledge coverage under tight context budgets. SARA combines natural-language text snippets with semantic compression vectors to jointly enhance context efficiency and answer correctness. It represents contexts at two complementary levels: 1) fine-grained natural-language spans that preserve critical entities and numerical values, and 2) compact, interpretable vectors that summarize high-level semantics. An iterative evidence-selection module employs the compression vectors for dynamic reranking of contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families (Mistral, Llama, and Gemma), SARA consistently improves answer relevance (+17.71), answer correctness (+13.72), and semantic similarity (+15.53), demonstrating the importance of integrating textual and compressed representations for robust, context-efficient RAG.</P><TABLE><TR><TD>Comments:</TD><TD>20 pages</TD></TR><TR><TD>Subjects:</TD><TD>Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)</TD></TR><TR><TD>Cite as:</TD><TD>arXiv:2507.05633 [cs.CL]</TD></TR><TR><TD> </TD><TD>(or arXiv:2507.05633v1 [cs.CL] for this version)</TD></TR><TR><TD> </TD><TD>https://doi.org/10.48550/arXiv.2507.05633\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer (What is the Explorer?)</P><P>Connected Papers (What is Connected Papers?)</P><P>Litmaps (What is Litmaps?)</P><P>scite Smart Citations (What are Smart Citations?)</P>", "url": "https://arxiv.org/abs/2507.05633", "pdf_url": "https://arxiv.org/pdf/2507.05633.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2507.05633", "title": "SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression", "authors": [], "abstract": "<H1>SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression</H1><H3>Yiqiao Jin, Kartik Sharma</H3><P>Georgia Institute of Technology</P><P>{ksartik,yjin328}@gatech.edu</P><H3>Menghai Pan, Mahashweta Das</H3><P>Visa Research</P><P>{menpan,mahdas}@visa.com</P><H3>Vineeth Rakesh, Yingtong Dou</H3><P>Visa Research</P><P>{vinmohan,yidou}@visa.com</P><H3>Srijan Kumar</H3><P>Georgia Institute of Technology</P><P>srijan@gatech.edu</P><H3>Abstract</H3><P>Retrieval-augmented Generation (RAG) extends large language models (LLMs) with external knowledge but faces key challenges: restricted effective context length and redundancy in retrieved documents. Pure compression-based approaches re-duce input size but often discard fine-grained details essential for factual accuracy. We propose SARA, a unified RAG framework that balances local precision and global knowledge coverage under tight context budgets. SARA combines natural-language text snippets with semantic compression vectors to jointly enhance context eficiency and answer correctness. It represents contexts at two complementary levels: 1) fine-grained natural-language spans that preserve critical entities and numerical values, and 2) compact, interpretable vectors that summarize high-level semantics. An iterative evidence-selection module employs the compression vec-tors for dynamic reranking of contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families (Mistral, Llama, and Gemma), SARA consistently improves answer relevance (+17.71), answer correctness (+13.72), and seman-tic similarity (+15.53), demonstrating the importance of integrating textual and compressed representations for robust, context-eficient RAG.</P><H3>1 Introduction</H3><P>Large language models (LLMs) have demonstrated remarkable capabilities across various natural language understanding and generation tasks (Xiao et al., 2024a; Zhao et al., 2024). Meanwhile, as LLMs are parametric in nature, their knowledge is inherently constrained by the scope, domain, and recency of their training data (Jin et al., 2024b; Liu et al., 2025). Retrieval-augmented generation (RAG) (Lewis et al., 2020) addresses this by retrieving from external non-parametric knowledge sources, essential for knowledge-intensive tasks.</P><P>Challenges. Despite its promise, RAG still faces key challenges in effectively retrieving, selecting, and integrating external evidence. 1) Limited Effective Context. While some LLMs support long inputs, their attention is biased toward earlier tokens (Li et al., 2024b), making them sensitive to input order and prone to overlooking important information near the end of the input (Yu et al., 2024). Extending usable context often requires costly, model-specific architectural changes (Ding et al., 2023). 2) Context Redundancy. Retrieved documents often include redundant or loosely structured content (e.g. transcripts or news articles) (Yu et al., 2024; Ge et al., 2024). Without careful post-", "url": "https://arxiv.org/pdf/2507.05633", "pdf_url": "https://arxiv.org/pdf/2507.05633.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2401.15391v1", "title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries", "authors": ["Yixuan Tang", "Yi Yang"], "abstract": "Retrieval-augmented generation (RAG) augments large language models (LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, thereby facilitating the great adoption of LLMs in practice. However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying RAG knowledge base. We demonstrate the benchmarking utility of MultiHop-RAG in two experiments. The first experiment compares different embedding models for retrieving evidence for multi-hop queries. In the second experiment, we examine the capabilities of various state-of-the-art LLMs, including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop queries given the evidence. Both experiments reveal that existing RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable resource for the community in developing effective RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop-RAG and implemented RAG system is publicly available at https://github.com/yixuantt/MultiHop-RAG/.", "url": "http://arxiv.org/abs/2401.15391v1", "pdf_url": "https://arxiv.org/pdf/2401.15391v1", "source": "arxiv", "published_date": "2024-01-27T11:41:48Z", "score_bm25": 0.0}, {"paper_id": "2404.15939v3", "title": "Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications", "authors": ["Andrei-Laurentiu Bornea", "Fadhel Ayed", "Antonio De Domenico", "Nicola Piovesan", "Ali Maatouk"], "abstract": "The application of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems in the telecommunication domain presents unique challenges, primarily due to the complex nature of telecom standard documents and the rapid evolution of the field. The paper introduces Telco-RAG, an open-source RAG framework designed to handle the specific needs of telecommunications standards, particularly 3rd Generation Partnership Project (3GPP) documents. Telco-RAG addresses the critical challenges of implementing a RAG pipeline on highly technical content, paving the way for applying LLMs in telecommunications and offering guidelines for RAG implementation in other technical domains.", "url": "http://arxiv.org/abs/2404.15939v3", "pdf_url": "https://arxiv.org/pdf/2404.15939v3", "source": "arxiv", "published_date": "2024-04-24T15:58:59Z", "score_bm25": 0.0}, {"paper_id": "2401.04670v1", "title": "Modified Levenberg-Marquardt Algorithm For Tensor CP Decomposition in Image Compression", "authors": ["Ramin Goudarzi Karim", "Dipak Dulal", "Carmeliza Navasca"], "abstract": "This paper explores a new version of the Levenberg-Marquardt algorithm used for Tensor Canonical Polyadic (CP) decomposition with an emphasis on image compression and reconstruction. Tensor computation, especially CP decomposition, holds significant applications in data compression and analysis. In this study, we formulate CP as a nonlinear least squares optimization problem. Then, we present an iterative Levenberg-Marquardt (LM) based algorithm for computing the CP decomposition. Ultimately, we test the algorithm on various datasets, including randomly generated tensors and RGB images. The proposed method proves to be both efficient and effective, offering a reduced computational burden when compared to the traditional Levenberg-Marquardt technique.", "url": "http://arxiv.org/abs/2401.04670v1", "pdf_url": "https://arxiv.org/pdf/2401.04670v1", "source": "arxiv", "published_date": "2024-01-09T16:53:47Z", "score_bm25": 0.0}, {"paper_id": "2405.13792v2", "title": "[2405.13792v2] xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token", "authors": [], "abstract": "<H1>Computer Science > Computation and Language</H1><H1>xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token</H1><P>This paper introduces xRAG, an innovative context compression method tailored for retrieval-augmented generation. xRAG reinterprets document embeddings in dense retrieval--traditionally used solely for retrieval--as features from the retrieval modality. By employing a modality fusion methodology, xRAG seamlessly integrates these embeddings into the language model representation space, effectively eliminating the need for their textual counterparts and achieving an extreme compression rate. In xRAG, the only trainable component is the modality bridge, while both the retriever and the language model remain frozen. This design choice allows for the reuse of offline-constructed document embeddings and preserves the plug-and-play nature of retrieval augmentation. Experimental results demonstrate that xRAG achieves an average improvement of over 10% across six knowledge-intensive tasks, adaptable to various language model backbones, ranging from a dense 7B model to an 8x7B Mixture of Experts configuration. xRAG not only significantly outperforms previous context compression methods but also matches the performance of uncompressed models on several datasets, while reducing overall FLOPs by a factor of 3.53. Our work pioneers new directions in retrieval-augmented generation from the perspective of multimodality fusion, and we hope it lays the foundation for future efficient and scalable retrieval-augmented systems</P><TABLE><TR><TD>Comments:</TD><TD>Neurips 2024</TD></TR><TR><TD>Subjects:</TD><TD>Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)</TD></TR><TR><TD>Cite as:</TD><TD>arXiv:2405.13792 [cs.CL]</TD></TR><TR><TD> </TD><TD>(or arXiv:2405.13792v2 [cs.CL] for this version)</TD></TR><TR><TD> </TD><TD>https://doi.org/10.48550/arXiv.2405.13792\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer (What is the Explorer?)</P><P>Connected Papers (What is Connected Papers?)</P><P>Litmaps (What is Litmaps?)</P><P>scite Smart Citations (What are Smart Citations?)</P>", "url": "https://arxiv.org/abs/2405.13792v2", "pdf_url": "https://arxiv.org/pdf/2405.13792v2.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": null, "title": "Structure induction by lossless graph compression", "authors": ["Leonid Peshkin"], "abstract": "This work is motivated by the necessity to automate the discovery of structure in vast and evergrowing collection of relational data commonly represented as graphs, for example genomic networks. A novel algorithm, dubbed Graphitour, for structure induction by lossless graph compression is presented and illustrated by a clear and broadly known case of nested structure in a DNA molecule. This work extends to graphs some well established approaches to grammatical inference previously applied only to strings. The bottom-up graph compression problem is related to the maximum cardinality (non-bipartite) maximum cardinality matching problem. The algorithm accepts a variety of graph types including directed graphs and graphs with labeled nodes and arcs. The resulting structure could be used for representation and classification of graphs.", "url": "http://arxiv.org/abs/cs/0703132v1", "pdf_url": "https://arxiv.org/pdf/cs/0703132v1", "source": "arxiv", "published_date": "2007-03-27T05:46:31Z", "score_bm25": 0.0}, {"paper_id": "2405.13792", "title": "xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token", "authors": [], "abstract": "<H6>Abstract</H6><P>This paper introduces xRAG, an innovative context compression method tailored for retrieval-augmented generation. xRAG reinterprets document embeddings in dense retrieval—traditionally used solely for retrieval—as features from the retrieval modality. By employing a modality fusion methodology, xRAG seamlessly integrates these embeddings into the language model representation space, effectively eliminating the need for their textual counterparts and achieving an extreme compression rate. In xRAG, the only trainable component is the modality bridge, while both the retriever and the language model remain frozen. This design choice allows for the reuse of offline-constructed document embeddings and preserves the plug-and-play nature of retrieval augmentation. Experimental results demonstrate that xRAG achieves an average improvement of over 10% across six knowledge-intensive tasks, adaptable to various language model backbones, ranging from a dense 7B model to an 8x7B Mixture of Experts configuration. xRAG not only significantly outperforms previous context compression methods but also matches the performance of uncompressed models on several datasets, while reducing overall FLOPs by a factor of 3.53. Our work pioneers new directions in retrieval-augmented generation from the perspective of multimodality fusion, and we hope it lays the foundation for future efficient and scalable retrieval-augmented systems 111Code available at: https://github.com/Hannibal046/xRAG. .</P><P>Figure 1: xRAG enables efficient retrieval augmentation by adding one document token [X].</P><H2>1 Introduction</H2><P>Retrieval-Augmented Language Models (RALMs) [41, 21, 8, 13, 66] have demonstrated remarkable performance in various knowledge-intensive tasks. By retrieving domain-specific, long-tailed, and up-to-date knowledge from a non-parametric datastore, RALMs significantly extend the boundaries of parametric Large Language Models (LLMs). However, integrating entire documents into prompts can substantially increase inference costs and potentially exceed the context limits of LLMs [28, 75]. As shown in Figure 1, with the help of relevant document, LLM could generate the correct answer, albeit at the cost of processing a document that extends the original query by more than tenfold.</P><P>How can we mitigate the costs associated with extended context while maintaining the benefits of retrieval augmentation? Recent research interest has converged on a promising direction: Context Compression. This explores two main approaches: soft-prompting methods, such as Gist [56], AutoCompressor [14], and ICAE [19], which compress the context into dense memory slots, and hard-prompting methods, such as LLMLingua [28] and RECOMP [75], where compression is applied on the surface form. These approaches, however, either require significant memory for storing LLM activations (e.g., 1.05 MB per token as reported by [56]) or suffer from relatively low compression rates. More", "url": "https://arxiv.org/html/2405.13792", "pdf_url": "https://arxiv.org/pdf/2405.13792.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2402.07483v2", "title": "T-RAG: Lessons from the LLM Trenches", "authors": ["Masoomali Fatehkia", "Ji Kim Lucas", "Sanjay Chawla"], "abstract": "Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure to represent entity hierarchies within the organization. This is used to generate a textual description to augment the context when responding to user queries pertaining to entities within the organization's hierarchy. Our evaluations, including a Needle in a Haystack test, show that this combination performs better than a simple RAG or finetuning implementation. Finally, we share some lessons learned based on our experiences building an LLM application for real-world use.", "url": "http://arxiv.org/abs/2402.07483v2", "pdf_url": "https://arxiv.org/pdf/2402.07483v2", "source": "arxiv", "published_date": "2024-02-12T08:45:08Z", "score_bm25": 0.0}, {"paper_id": "2504.01346v4", "title": "RAG over Tables: Hierarchical Memory Index, Multi-Stage Retrieval, and Benchmarking", "authors": ["Jiaru Zou", "Dongqi Fu", "Sirui Chen", "Xinrui He", "Zihao Li", "Yada Zhu", "Jiawei Han", "Jingrui He"], "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating them with an external knowledge base to improve the answer relevance and accuracy. In real-world scenarios, beyond pure text, a substantial amount of knowledge is stored in tables, and user questions often require retrieving answers that are distributed across multiple tables. Retrieving knowledge from a table corpora (i.e., various individual tables) for a question remains nascent, at least, for (i) how to understand intra- and inter-table knowledge effectively, (ii) how to filter unnecessary tables and how to retrieve the most relevant tables efficiently, (iii) how to prompt LLMs to infer over the retrieval, (iv) how to evaluate the corresponding performance in a realistic setting. Facing the above challenges, in this paper, we first propose a table-corpora-aware RAG framework, named T-RAG, which consists of the hierarchical memory index, multi-stage retrieval, and graph-aware prompting for effective and efficient table knowledge retrieval and inference. Further, we first develop a multi-table question answering benchmark named MultiTableQA, which spans 3 different task types, 57,193 tables, and 23,758 questions in total, and the sources are all from real-world scenarios. Based on MultiTableQA, we did the holistic comparison over table retrieval methods, RAG methods, and table-to-graph representation learning methods, where T-RAG shows the leading accuracy, recall, and running time performance. Also, under T-RAG, we evaluate the inference ability upgrade of different LLMs. Code and Data are available at https://github.com/jiaruzouu/T-RAG", "url": "http://arxiv.org/abs/2504.01346v4", "pdf_url": "https://arxiv.org/pdf/2504.01346v4", "source": "arxiv", "published_date": "2025-04-02T04:24:41Z", "score_bm25": 0.0}, {"paper_id": "2412.12881v1", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "abstract": "Existing large language models (LLMs) show exceptional problem-solving capabilities but might struggle with complex reasoning tasks. Despite the successes of chain-of-thought and tree-based search methods, they mainly depend on the internal knowledge of LLMs to search over intermediate reasoning steps, limited to dealing with simple tasks involving fewer reasoning steps. In this paper, we propose \\textbf{RAG-Star}, a novel RAG approach that integrates the retrieved information to guide the tree-based deliberative reasoning process that relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree Search, RAG-Star iteratively plans intermediate sub-queries and answers for reasoning based on the LLM itself. To consolidate internal and external knowledge, we propose an retrieval-augmented verification that utilizes query- and answer-aware reward modeling to provide feedback for the inherent reasoning of LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate that RAG-Star significantly outperforms previous RAG and reasoning methods.", "url": "http://arxiv.org/abs/2412.12881v1", "pdf_url": "https://arxiv.org/pdf/2412.12881v1", "source": "arxiv", "published_date": "2024-12-17T13:05:36Z", "score_bm25": 0.0}, {"paper_id": "2502.13957v2", "title": "RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation", "authors": ["Guangzhi Xiong", "Qiao Jin", "Xiao Wang", "Yin Fang", "Haolin Liu", "Yifan Yang", "Fangyuan Chen", "Zhixing Song", "Dengyu Wang", "Minjia Zhang", "Zhiyong Lu", "Aidong Zhang"], "abstract": "Retrieval-augmented generation (RAG) has shown great promise for knowledge-intensive tasks and recently advanced with agentic RAG, where language agents engage in multi-round interactions with external knowledge sources for adaptive information retrieval. However, existing agentic RAG methods often depend on ad-hoc prompt engineering and lack a unified optimization framework. We introduce RAG-Gym, a comprehensive platform that systematically explores three optimization dimensions: (1) prompt engineering, (2) actor tuning, and (3) critic training. For prompt engineering, we propose Re$^2$Search, a novel agent incorporating reasoning reflection that significantly outperforms standard prompts. In actor tuning, we evaluate three popular post-training algorithms with fine-grained process supervision and identify direct preference optimization as the most effective. We further demonstrate that a trained critic can enhance inference by selecting higher-quality intermediate reasoning steps. Together, these findings lead to the optimized Re$^2$Search++ agent, which surpasses most recent methods like Search-R1 by a relative increase of 3.2% to 11.6% in average F1. Finally, we examine the impact of different reward sources and analyze scaling properties in training and inference, offering practical insights for agentic RAG optimization. The project homepage is available at https://rag-gym.github.io.", "url": "http://arxiv.org/abs/2502.13957v2", "pdf_url": "https://arxiv.org/pdf/2502.13957v2", "source": "arxiv", "published_date": "2025-02-19T18:56:03Z", "score_bm25": 0.0}, {"paper_id": "2304.03679v1", "title": "T2Ranking: A large-scale Chinese Benchmark for Passage Ranking", "authors": ["Xiaohui Xie", "Qian Dong", "Bingning Wang", "Feiyang Lv", "Ting Yao", "Weinan Gan", "Zhijing Wu", "Xiangsheng Li", "Haitao Li", "Yiqun Liu", "Jin Ma"], "abstract": "Passage ranking involves two stages: passage retrieval and passage re-ranking, which are important and challenging topics for both academics and industries in the area of Information Retrieval (IR). However, the commonly-used datasets for passage ranking usually focus on the English language. For non-English scenarios, such as Chinese, the existing datasets are limited in terms of data scale, fine-grained relevance annotation and false negative issues. To address this problem, we introduce T2Ranking, a large-scale Chinese benchmark for passage ranking. T2Ranking comprises more than 300K queries and over 2M unique passages from real-world search engines. Expert annotators are recruited to provide 4-level graded relevance scores (fine-grained) for query-passage pairs instead of binary relevance judgments (coarse-grained). To ease the false negative issues, more passages with higher diversities are considered when performing relevance annotations, especially in the test set, to ensure a more accurate evaluation. Apart from the textual query and passage data, other auxiliary resources are also provided, such as query types and XML files of documents which passages are generated from, to facilitate further studies. To evaluate the dataset, commonly used ranking models are implemented and tested on T2Ranking as baselines. The experimental results show that T2Ranking is challenging and there is still scope for improvement. The full data and all codes are available at https://github.com/THUIR/T2Ranking/", "url": "http://arxiv.org/abs/2304.03679v1", "pdf_url": "https://arxiv.org/pdf/2304.03679v1", "source": "arxiv", "published_date": "2023-04-07T15:02:33Z", "score_bm25": 0.0}, {"paper_id": "1906.02083v1", "title": "A Passage-Based Approach to Learning to Rank Documents", "authors": ["Eilon Sheetrit", "Anna Shtok", "Oren Kurland"], "abstract": "According to common relevance-judgments regimes, such as TREC's, a document can be deemed relevant to a query even if it contains a very short passage of text with pertinent information. This fact has motivated work on passage-based document retrieval: document ranking methods that induce information from the document's passages. However, the main source of passage-based information utilized was passage-query similarities. We address the challenge of utilizing richer sources of passage-based information to improve document retrieval effectiveness. Specifically, we devise a suite of learning-to-rank-based document retrieval methods that utilize an effective ranking of passages produced in response to the query; the passage ranking is also induced using a learning-to-rank approach. Some of the methods quantify the ranking of the passages of a document. Others utilize the feature-based representation of passages used for learning a passage ranker. Empirical evaluation attests to the clear merits of our methods with respect to highly effective baselines. Our best performing method is based on learning a document ranking function using document-query features and passage-query features of the document's passage most highly ranked.", "url": "http://arxiv.org/abs/1906.02083v1", "pdf_url": "https://arxiv.org/pdf/1906.02083v1", "source": "arxiv", "published_date": "2019-06-05T15:44:02Z", "score_bm25": 0.0}, {"paper_id": "2207.01762", "title": "[2207.01762] PReGAN: Answer Oriented Passage Ranking with Weakly Supervised GAN", "authors": [], "abstract": "<H1>Computer Science > Computation and Language</H1><H1>PReGAN: Answer Oriented Passage Ranking with Weakly Supervised GAN</H1><P>Beyond topical relevance, passage ranking for open-domain factoid question answering also requires a passage to contain an answer (answerability). While a few recent studies have incorporated some reading capability into a ranker to account for answerability, the ranker is still hindered by the noisy nature of the training data typically available in this area, which considers any passage containing an answer entity as a positive sample. However, the answer entity in a passage is not necessarily mentioned in relation with the given question. To address the problem, we propose an approach called \\ttt {PReGAN} for Passage Reranking based on Generative Adversarial Neural networks, which incorporates a discriminator on answerability, in addition to a discriminator on topical relevance. The goal is to force the generator to rank higher a passage that is topically relevant and contains an answer. Experiments on five public datasets show that \\ttt {PReGAN} can better rank appropriate passages, which in turn, boosts the effectiveness of QA systems, and outperforms the existing approaches without using external data.</P><TABLE><TR><TD>Subjects:</TD><TD>Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)</TD></TR><TR><TD>Cite as:</TD><TD>arXiv:2207.01762 [cs.CL]</TD></TR><TR><TD> </TD><TD>(or arXiv:2207.01762v1 [cs.CL] for this version)</TD></TR><TR><TD> </TD><TD>https://doi.org/10.48550/arXiv.2207.01762\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer (What is the Explorer?)</P><P>Connected Papers (What is Connected Papers?)</P><P>Litmaps (What is Litmaps?)</P><P>scite Smart Citations (What are Smart Citations?)</P>", "url": "https://arxiv.org/abs/2207.01762", "pdf_url": "https://arxiv.org/pdf/2207.01762.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2207.01762v1", "title": "PReGAN: Answer Oriented Passage Ranking with Weakly Supervised GAN", "authors": ["Pan Du", "Jian-Yun Nie", "Yutao Zhu", "Hao Jiang", "Lixin Zou", "Xiaohui Yan"], "abstract": "Beyond topical relevance, passage ranking for open-domain factoid question answering also requires a passage to contain an answer (answerability). While a few recent studies have incorporated some reading capability into a ranker to account for answerability, the ranker is still hindered by the noisy nature of the training data typically available in this area, which considers any passage containing an answer entity as a positive sample. However, the answer entity in a passage is not necessarily mentioned in relation with the given question. To address the problem, we propose an approach called \\ttt{PReGAN} for Passage Reranking based on Generative Adversarial Neural networks, which incorporates a discriminator on answerability, in addition to a discriminator on topical relevance. The goal is to force the generator to rank higher a passage that is topically relevant and contains an answer. Experiments on five public datasets show that \\ttt{PReGAN} can better rank appropriate passages, which in turn, boosts the effectiveness of QA systems, and outperforms the existing approaches without using external data.", "url": "http://arxiv.org/abs/2207.01762v1", "pdf_url": "https://arxiv.org/pdf/2207.01762v1", "source": "arxiv", "published_date": "2022-07-05T01:43:35Z", "score_bm25": 0.0}, {"paper_id": "2305.18144v1", "title": "GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved  Passage Ranking", "authors": [], "abstract": "<H1>GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking</H1><P>Jiaqi Bai1,2, Hongcheng Guo2, Jiaheng Liu2, Jian Yang2, Xinnian Liang2, Zhao Yan3and Zhoujun Li2</P><P>1School of Cyber Science and Technology, Beihang University 2State Key Lab of Software Development Environment, Beihang University 3Tencent Cloud AI</P><P>Beijing, China</P><P>{bjq,hongchengguo,liujiaheng,jiaya,xnliang,lizj}@buaa.edu.cn</P><P>zhaoyan@tencent.com</P><H3>ABSTRACT</H3><P>Retrieval-enhanced text generation, which aims to leverage pas-sages retrieved from a large passage corpus for delivering a proper answer given the input query, has shown remarkable progress on knowledge-intensive language tasks such as open-domain question answering and knowledge-enhanced dialogue generation. How-ever, the retrieved passages are not ideal for guiding answer gener-ation because of the discrepancy between retrieval and generation, i.e., the candidate passages are all treated equally during the re-trieval procedure without considering their potential to generate the proper answers. This discrepancy makes a passage retriever deliver a sub-optimal collection of candidate passages to gener-ate answers. In this paper, we propose the GeneRative Knowledge Improved Passage Ranking (GripRank) approach, addressing the above challenge by distilling knowledge from a generative passage estimator (GPE) to a passage ranker, where the GPE is a generative language model used to measure how likely the candidate passages can generate the proper answer. We realize the distillation proce-dure by teaching the passage ranker learning to rank the passages ordered by the GPE. Furthermore, we improve the distillation qual-ity by devising a curriculum knowledge distillation mechanism, which allows the knowledge provided by the GPE can be progres-sively distilled to the ranker through an easy-to-hard curriculum, enabling the passage ranker to correctly recognize the provenance of the answer from many plausible candidates. We conduct exten-sive experiments on four datasets across three knowledge-intensive language tasks. Experimental results show advantages over the state-of-the-art methods for both passage ranking and answer gen-eration on the KILT benchmark.</P><H3>CCS CONCEPTS</H3><P>• Information systems → Retrieval models and ranking.</P><H3>KEYWORDS</H3><P>Knowledge-intensive language tasks, Retrieval-enhanced text gen-eration, Passage ranking, Knowledge distillation</P><H3>1 INTRODUCTION</H3><P>Knowledge-intensive language tasks, including open-domain ques-tion answering, knowledge-grounded conversation, and fact verifi-cation, pose a challenge for retrieving passages most likely to be the provenance of the target answer from a large passage corpus (e.g., Wikipedia). One of the most successful paradigms to deal with these tasks is retrieval-enhanced text generation [14, 20, 29]. It first trains a passage retriever (e.g., DPR [24] and GTR [34]) to c", "url": "https://arxiv.org/pdf/2305.18144v1", "pdf_url": "https://arxiv.org/pdf/2305.18144v1.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2405.20654v2", "title": "[2405.20654v2] Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models", "authors": [], "abstract": "<H1>Computer Science > Computation and Language</H1><H1>Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models</H1><P>Effective passage retrieval and reranking methods have been widely utilized to identify suitable candidates in open-domain question answering tasks, recent studies have resorted to LLMs for reranking the retrieved passages by the log-likelihood of the question conditioned on each passage. Although these methods have demonstrated promising results, the performance is notably sensitive to the human-written prompt (or hard prompt), and fine-tuning LLMs can be computationally intensive and time-consuming. Furthermore, this approach limits the leverage of question-passage relevance pairs and passage-specific knowledge to enhance the ranking capabilities of LLMs. In this paper, we propose passage-specific prompt tuning for reranking in open-domain question answering (PSPT): a parameter-efficient method that fine-tunes learnable passage-specific soft prompts, incorporating passage-specific knowledge from a limited set of question-passage relevance pairs. The method involves ranking retrieved passages based on the log-likelihood of the model generating the question conditioned on each passage and the learned soft prompt. We conducted extensive experiments utilizing the Llama-2-chat-7B model across three publicly available open-domain question answering datasets and the results demonstrate the effectiveness of the proposed approach.</P><TABLE><TR><TD>Comments:</TD><TD>Accepted at Gen-IR@SIGIR24</TD></TR><TR><TD>Subjects:</TD><TD>Computation and Language (cs.CL); Information Retrieval (cs.IR)</TD></TR><TR><TD>Cite as:</TD><TD>arXiv:2405.20654 [cs.CL]</TD></TR><TR><TD>(or arXiv:2405.20654v2 [cs.CL] for this version)</TD></TR><TR><TD>https://doi.org/10.48550/arXiv.2405.20654\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer (What is the Explorer?)</P><P>Connected Papers (What is Connected Papers?)</P><P>Litmaps (What is Litmaps?)</P><P>scite Smart Citations (What are Smart Citations?)</P>", "url": "https://arxiv.org/abs/2405.20654v2", "pdf_url": "https://arxiv.org/pdf/2405.20654v2.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2103.16669v3", "title": "An In-depth Analysis of Passage-Level Label Transfer for Contextual Document Ranking", "authors": ["Koustav Rudra", "Zeon Trevor Fernando", "Avishek Anand"], "abstract": "Pre-trained contextual language models such as BERT, GPT, and XLnet work quite well for document retrieval tasks. Such models are fine-tuned based on the query-document/query-passage level relevance labels to capture the ranking signals. However, the documents are longer than the passages and such document ranking models suffer from the token limitation (512) of BERT. Researchers proposed ranking strategies that either truncate the documents beyond the token limit or chunk the documents into units that can fit into the BERT. In the later case, the relevance labels are either directly transferred from the original query-document pair or learned through some external model. In this paper, we conduct a detailed study of the design decisions about splitting and label transfer on retrieval effectiveness and efficiency. We find that direct transfer of relevance labels from documents to passages introduces label noise that strongly affects retrieval effectiveness for large training datasets. We also find that query processing times are adversely affected by fine-grained splitting schemes. As a remedy, we propose a careful passage level labelling scheme using weak supervision that delivers improved performance (3-14% in terms of nDCG score) over most of the recently proposed models for ad-hoc retrieval while maintaining manageable computational complexity on four diverse document retrieval datasets.", "url": "http://arxiv.org/abs/2103.16669v3", "pdf_url": "https://arxiv.org/pdf/2103.16669v3", "source": "arxiv", "published_date": "2021-03-30T20:28:02Z", "score_bm25": 0.0}, {"paper_id": "2405.20654v2", "title": "Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models", "authors": [], "abstract": "<H1>Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models</H1><TABLE><TR><TD><P>Xuyang Wu∗</P></TD><TD><P>Zhiyuan Peng∗</P></TD><TD><P>Krishna Sravanthi Rajanala Sai</P></TD></TR><TR><TD><P>Santa Clara University</P></TD><TD><P>Santa Clara University</P></TD><TD><P>Walmart Global Tech</P></TD></TR><TR><TD><P>Santa Clara, USA</P></TD><TD><P>Santa Clara, USA</P></TD><TD><P>Sunnyvale, USA</P></TD></TR><TR><TD><P>xwu5@scu.edu</P></TD><TD><P>zpeng@scu.edu</P></TD><TD><P>sravanthi.rajanala@walmart.com</P></TD></TR></TABLE><H3>Hsin-Tai Wu</H3><P>Docomo Innovations Sunnyvale, USA hwu@docomoinnovations.com</P><H3>ABSTRACT</H3><P>Efective passage retrieval and reranking methods have been widely utilized to identify suitable candidates in open-domain question answering tasks, recent studies have resorted to LLMs for rerank-ing the retrieved passages by the log-likelihood of the question conditioned on each passage. Although these methods have demon-strated promising results, the performance is notably sensitive to the human-written prompt (or hard prompt), and fine-tuning LLMs can be computationally intensive and time-consuming. Further-more, this approach limits the leverage of question-passage rele-vance pairs and passage-specific knowledge to enhance the ranking capabilities of LLMs. In this paper, we propose passage-specific prompt tuning for reranking in open-domain question answer-ing (PSPT1): a parameter-eficient method that fine-tunes learn-able passage-specific soft prompts, incorporating passage-specific knowledge from a limited set of question-passage relevance pairs. The method involves ranking retrieved passages based on the log-likelihood of the model generating the question conditioned on each passage and the learned soft prompt. We conducted extensive experiments utilizing the Llama-2-chat-7B model across three pub-licly available open-domain question answering datasets and the results demonstrate the efectiveness of the proposed approach.</P><H3>1 INTRODUCTION</H3><P>Open-domain question answering (QA) involves to answer ques-tions from a vast collection of passages [41]. The existing works [5, 12, 44] have demonstrated that eficiently retrieving a small subset of passages, which contain the answer to the question, is a crucial part of enhancing the QA task. Typically, relevant pas-sages can be retrieved using keyword matching methods such as TF-IDF or BM25 [31], or through dense latent representations [5]. The results can be refined further by reranking the top- retrieved passages to ensure accuracy.</P><P>∗Both authors contributed equally to this research.</P><P>1Our code can be found at https://github.com/elviswxy/Gen-IR_PSPT.</P><P>Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation", "url": "https://arxiv.org/pdf/2405.20654v2", "pdf_url": "https://arxiv.org/pdf/2405.20654v2.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "1511.05806v1", "title": "Ranking library materials", "authors": ["Dirk Lewandowski"], "abstract": "Purpose: This paper discusses ranking factors suitable for library materials and shows that ranking in general is a complex process and that ranking for library materials requires a variety of techniques. Design/methodology/approach: The relevant literature is reviewed to provide a systematic overview of suitable ranking factors. The discussion is based on an overview of ranking factors used in Web search engines. Findings: While there are a wide variety of ranking factors applicable to library materials, todays library systems use only some of them. When designing a ranking component for the library catalogue, an individual weighting of applicable factors is necessary. Research limitations/applications: While this article discusses different factors, no particular ranking formula is given. However, this article presents the argument that such a formula must always be individual to a certain use case. Practical implications: The factors presented can be considered when designing a ranking component for a librarys search system or when discussing such a project with an ILS vendor. Originality/value: This paper is original in that it is the first to systematically discuss ranking of library materials based on the main factors used by Web search engines.", "url": "http://arxiv.org/abs/1511.05806v1", "pdf_url": "https://arxiv.org/pdf/1511.05806v1", "source": "arxiv", "published_date": "2015-11-18T14:36:20Z", "score_bm25": 0.0}, {"paper_id": "2008.02460v1", "title": "DeText: A Deep Text Ranking Framework with BERT", "authors": ["Weiwei Guo", "Xiaowei Liu", "Sida Wang", "Huiji Gao", "Ananth Sankar", "Zimeng Yang", "Qi Guo", "Liang Zhang", "Bo Long", "Bee-Chung Chen", "Deepak Agarwal"], "abstract": "Ranking is the most important component in a search system. Mostsearch systems deal with large amounts of natural language data,hence an effective ranking system requires a deep understandingof text semantics. Recently, deep learning based natural languageprocessing (deep NLP) models have generated promising results onranking systems. BERT is one of the most successful models thatlearn contextual embedding, which has been applied to capturecomplex query-document relations for search ranking. However,this is generally done by exhaustively interacting each query wordwith each document word, which is inefficient for online servingin search product systems. In this paper, we investigate how tobuild an efficient BERT-based ranking model for industry use cases.The solution is further extended to a general ranking framework,DeText, that is open sourced and can be applied to various rankingproductions. Offline and online experiments of DeText on threereal-world search systems present significant improvement overstate-of-the-art approaches.", "url": "http://arxiv.org/abs/2008.02460v1", "pdf_url": "https://arxiv.org/pdf/2008.02460v1", "source": "arxiv", "published_date": "2020-08-06T05:12:11Z", "score_bm25": 0.0}, {"paper_id": "2210.15133v1", "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval", "authors": ["Dingkun Long", "Yanzhao Zhang", "Guangwei Xu", "Pengjun Xie"], "abstract": "Pre-trained language model (PTM) has been shown to yield powerful text representations for dense passage retrieval task. The Masked Language Modeling (MLM) is a major sub-task of the pre-training process. However, we found that the conventional random masking strategy tend to select a large number of tokens that have limited effect on the passage retrieval task (e,g. stop-words and punctuation). By noticing the term importance weight can provide valuable information for passage retrieval, we hereby propose alternative retrieval oriented masking (dubbed as ROM) strategy where more important tokens will have a higher probability of being masked out, to capture this straightforward yet essential information to facilitate the language model pre-training process. Notably, the proposed new token masking method will not change the architecture and learning objective of original PTM. Our experiments verify that the proposed ROM enables term importance information to help language model pre-training thus achieving better performance on multiple passage retrieval benchmarks.", "url": "http://arxiv.org/abs/2210.15133v1", "pdf_url": "https://arxiv.org/pdf/2210.15133v1", "source": "arxiv", "published_date": "2022-10-27T02:43:48Z", "score_bm25": 0.0}, {"paper_id": "2204.07496v4", "title": "Improving Passage Retrieval with Zero-Shot Question Generation", "authors": ["Devendra Singh Sachan", "Mike Lewis", "Mandar Joshi", "Armen Aghajanyan", "Wen-tau Yih", "Joelle Pineau", "Luke Zettlemoyer"], "abstract": "We propose a simple and effective re-ranking method for improving passage retrieval in open question answering. The re-ranker re-scores retrieved passages with a zero-shot question generation model, which uses a pre-trained language model to compute the probability of the input question conditioned on a retrieved passage. This approach can be applied on top of any retrieval method (e.g. neural or keyword-based), does not require any domain- or task-specific training (and therefore is expected to generalize better to data distribution shifts), and provides rich cross-attention between query and passage (i.e. it must explain every token in the question). When evaluated on a number of open-domain retrieval datasets, our re-ranker improves strong unsupervised retrieval models by 6%-18% absolute and strong supervised models by up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new state-of-the-art results on full open-domain question answering by simply adding the new re-ranker to existing models with no further changes.", "url": "http://arxiv.org/abs/2204.07496v4", "pdf_url": "https://arxiv.org/pdf/2204.07496v4", "source": "arxiv", "published_date": "2022-04-15T14:51:41Z", "score_bm25": 0.0}, {"paper_id": null, "title": "Privacy-Preserving Important Passage Retrieval", "authors": ["Luis Marujo", "José Portêlo", "David Martins de Matos", "João P. Neto", "Anatole Gershman", "Jaime Carbonell", "Isabel Trancoso", "Bhiksha Raj"], "abstract": "State-of-the-art important passage retrieval methods obtain very good results, but do not take into account privacy issues. In this paper, we present a privacy preserving method that relies on creating secure representations of documents. Our approach allows for third parties to retrieve important passages from documents without learning anything regarding their content. We use a hashing scheme known as Secure Binary Embeddings to convert a key phrase and bag-of-words representation to bit strings in a way that allows the computation of approximate distances, instead of exact ones. Experiments show that our secure system yield similar results to its non-private counterpart on both clean text and noisy speech recognized text.", "url": "http://arxiv.org/abs/1407.5416v1", "pdf_url": "https://arxiv.org/pdf/1407.5416v1", "source": "arxiv", "published_date": "2014-07-21T08:56:09Z", "score_bm25": 0.0}, {"paper_id": "2210.07774v3", "title": "Learning To Rank Diversely At Airbnb", "authors": ["Malay Haldar", "Mustafa Abdool", "Liwei He", "Dillon Davis", "Huiji Gao", "Sanjeev Katariya"], "abstract": "Airbnb is a two-sided marketplace, bringing together hosts who own listings for rent, with prospective guests from around the globe. Applying neural network-based learning to rank techniques has led to significant improvements in matching guests with hosts. These improvements in ranking were driven by a core strategy: order the listings by their estimated booking probabilities, then iterate on techniques to make these booking probability estimates more and more accurate. Embedded implicitly in this strategy was an assumption that the booking probability of a listing could be determined independently of other listings in search results. In this paper we discuss how this assumption, pervasive throughout the commonly-used learning to rank frameworks, is false. We provide a theoretical foundation correcting this assumption, followed by efficient neural network architectures based on the theory. Explicitly accounting for possible similarities between listings, and reducing them to diversify the search results generated strong positive impact. We discuss these metric wins as part of the online A/B tests of the theory. Our method provides a practical way to diversify search results for large-scale production ranking systems.", "url": "http://arxiv.org/abs/2210.07774v3", "pdf_url": "https://arxiv.org/pdf/2210.07774v3", "source": "arxiv", "published_date": "2022-09-19T22:57:10Z", "score_bm25": 0.0}, {"paper_id": "2104.09630v2", "title": "Quaternion Generative Adversarial Networks", "authors": ["Eleonora Grassucci", "Edoardo Cicero", "Danilo Comminiello"], "abstract": "Latest Generative Adversarial Networks (GANs) are gathering outstanding results through a large-scale training, thus employing models composed of millions of parameters requiring extensive computational capabilities. Building such huge models undermines their replicability and increases the training instability. Moreover, multi-channel data, such as images or audio, are usually processed by realvalued convolutional networks that flatten and concatenate the input, often losing intra-channel spatial relations. To address these issues related to complexity and information loss, we propose a family of quaternion-valued generative adversarial networks (QGANs). QGANs exploit the properties of quaternion algebra, e.g., the Hamilton product, that allows to process channels as a single entity and capture internal latent relations, while reducing by a factor of 4 the overall number of parameters. We show how to design QGANs and to extend the proposed approach even to advanced models.We compare the proposed QGANs with real-valued counterparts on several image generation benchmarks. Results show that QGANs are able to obtain better FID scores than real-valued GANs and to generate visually pleasing images. Furthermore, QGANs save up to 75% of the training parameters. We believe these results may pave the way to novel, more accessible, GANs capable of improving performance and saving computational resources.", "url": "http://arxiv.org/abs/2104.09630v2", "pdf_url": "https://arxiv.org/pdf/2104.09630v2", "source": "arxiv", "published_date": "2021-04-19T20:46:18Z", "score_bm25": 0.0}, {"paper_id": null, "title": "First passage times and asymmetry of DNA translocation", "authors": ["Rhonald C. Lua", "Alexander Y. Grosberg"], "abstract": "Motivated by experiments in which single-stranded DNA with a short hairpin loop at one end undergoes unforced diffusion through a narrow pore, we study the first passage times for a particle, executing one-dimensional brownian motion in an asymmetric sawtooth potential, to exit one of the boundaries. We consider the first passage times for the case of classical diffusion, characterized by a mean-square displacement of the form $< (Δx)^2> \\sim t$, and for the case of anomalous diffusion or subdiffusion, characterized by a mean-square displacement of the form $< (Δx)^2> \\sim t^γ$ with $0<γ<1$. In the context of classical diffusion, we obtain an expression for the mean first passage time and show that this quantity changes when the direction of the sawtooth is reversed or, equivalently, when the reflecting and absorbing boundaries are exchanged. We discuss at which numbers of `teeth' $N$ (or number of DNA nucleotides) and at which heights of the sawtooth potential this difference becomes significant. For large $N$, it is well known that the mean first passage time scales as $N^2$. In the context of subdiffusion, the mean first passage time does not exist. Therefore we obtain instead the distribution of first passage times in the limit of long times. We show that the prefactor in the power relation for this distribution is simply the expression for the mean first passage time in classical diffusion. We also describe a hypothetical experiment to calculate the average of the first passage times for a fraction of passage events that each end within some time $t^*$. We show that this average first passage time scales as $N^{2/γ}$ in subdiffusion.", "url": "http://arxiv.org/abs/q-bio/0508010v2", "pdf_url": "https://arxiv.org/pdf/q-bio/0508010v2", "source": "arxiv", "published_date": "2005-08-10T23:57:28Z", "score_bm25": 0.0}, {"paper_id": null, "title": "First passage behaviour of multi-dimensional fractional Brownian motion and application to reaction phenomena", "authors": ["Jae-Hyung Jeon", "Aleksei V. Chechkin", "Ralf Metzler"], "abstract": "Fractional Brownian motion is a generalised Gaussian diffusive process that is found to describe numerous stochastic phenomena in physics and biology. Here we introduce a multi-dimensional fractional Brownian motion (FBM) defined as a superposition of conventional FBM for each coordinate in analogy to multi-dimensional Brownian motion, and study its first passage properties. Starting from the well-established first passage time statistics of one-dimensional FBM and the associated approximation schemes, we explore the first passage time behaviour of multi-dimensional FBM and compare these results with simulations. The asymptotic kinetic behaviour of diffusion-limited reactions of reactant particles performing FBM in a one- and multi-dimensional space is studied based on the corresponding first passage time statistics.", "url": "http://arxiv.org/abs/1306.1667v2", "pdf_url": "https://arxiv.org/pdf/1306.1667v2", "source": "arxiv", "published_date": "2013-06-07T09:28:14Z", "score_bm25": 0.0}, {"paper_id": null, "title": "Trajectory-to-trajectory fluctuations in first-passage phenomena in bounded domains", "authors": ["T. G. Mattos", "C. Mejía-Monasterio", "R. Metzler", "G. Oshanin", "G. Schehr"], "abstract": "We study the statistics of the first passage of a random walker to absorbing subsets of the boundary of compact domains in different spatial dimensions. We describe a novel diagnostic method to quantify the trajectory-to-trajectory fluctuations of the first passage, based on the distribution of the so-called uniformity index $ω$, measuring the similarity of the first passage times of two independent walkers starting at the same location. We show that the characteristic shape of $P(ω)$ exhibits a transition from unimodal to bimodal, depending on the starting point of the trajectories. From the study of different geometries in one, two and three dimensions, we conclude that this transition is a generic property of first passage phenomena in bounded domains. Our results show that, in general, the Mean First Passage Time (MFPT) is a meaningful characteristic measure of the first passage behaviour only when the Brownian walkers start sufficiently far from the absorbing boundary. Strikingly, in the opposite case, the first passage statistics exhibit large trajectory-to-trajectory fluctuations and the MFPT is not representative of the actual behaviour.", "url": "http://arxiv.org/abs/1305.0637v1", "pdf_url": "https://arxiv.org/pdf/1305.0637v1", "source": "arxiv", "published_date": "2013-05-03T08:16:03Z", "score_bm25": 0.0}, {"paper_id": null, "title": "First Passage Problems in Biology", "authors": ["Tom Chou", "Maria R. D'Orsogna"], "abstract": "Applications of first passage times in stochastic processes arise across a wide range of length and time scales in biological settings. After an initial technical overview, we survey representative applications and their corresponding models. Within models that are effectively Markovian, we discuss canonical examples of first passage problems spanning applications to molecular dissociation and self-assembly, molecular search, transcription and translation, neuronal spiking, cellular mutation and disease, and organismic evolution and population dynamics. In this last application, a simple model for stem-cell ageing is presented and some results derived. Various approximation methods and the physical and mathematical subtleties that arise in the chosen applications are also discussed.", "url": "http://arxiv.org/abs/1408.4518v1", "pdf_url": "https://arxiv.org/pdf/1408.4518v1", "source": "arxiv", "published_date": "2014-08-20T03:59:57Z", "score_bm25": 0.0}, {"paper_id": null, "title": "Mean first passage time for fission potentials having structure", "authors": ["H. Hofmann", "A. G. Magner"], "abstract": "A schematic model of over-damped motion is presented which permits one to calculate the mean first passage time for nuclear fission. Its asymptotic value may exceed considerably the lifetime suggested by Kramers rate formula, which applies only to very special, favorable potentials and temperatures. The additional time obtained in the more general case is seen to allow for a considerable increment in the emission of light particles.", "url": "http://arxiv.org/abs/nucl-th/0304022v1", "pdf_url": "https://arxiv.org/pdf/nucl-th/0304022v1", "source": "arxiv", "published_date": "2003-04-07T14:55:43Z", "score_bm25": 0.0}, {"paper_id": null, "title": "Monte-Carlo Simulations of the First Passage Time for Multivariate Jump-Diffusion Processes in Financial Applications", "authors": ["Di Zhang", "Roderick V. N. Melnik"], "abstract": "Many problems in finance require the information on the first passage time (FPT) of a stochastic process. Mathematically, such problems are often reduced to the evaluation of the probability density of the time for such a process to cross a certain level, a boundary, or to enter a certain region. While in other areas of applications the FPT problem can often be solved analytically, in finance we usually have to resort to the application of numerical procedures, in particular when we deal with jump-diffusion stochastic processes (JDP). In this paper, we propose a Monte-Carlo-based methodology for the solution of the first passage time problem in the context of multivariate (and correlated) jump-diffusion processes. The developed technique provide an efficient tool for a number of applications, including credit risk and option pricing. We demonstrate its applicability to the analysis of the default rates and default correlations of several different, but correlated firms via a set of empirical data.", "url": "http://arxiv.org/abs/cs/0702164v1", "pdf_url": "https://arxiv.org/pdf/cs/0702164v1", "source": "arxiv", "published_date": "2007-02-28T10:51:16Z", "score_bm25": 0.0}, {"paper_id": "2304.04616v1", "title": "https://arxiv.org/pdf/2304.04616v1", "authors": [], "abstract": "<H1>Automated Reading Passage Generation with OpenAI’s Large Language Model</H1><H1>Ummugul Bezirhan1 & Matthias von Davier1</H1><P>1Boston College, Chestnut Hill, MA, USA  </P><P>Corresponding Author: Ummugul Bezirhan  TIMSS &PIRLS International Study Center, Boston College, 188 Beacon Street, Chestnut Hill, MA 02467, USA. Email: bezirhan@bc.edu</P><H3>Abstract</H3><P>The widespread usage of computer-based assessments and individualized learning platforms has</P><P>resulted in an increased demand for the rapid production of high-quality items. Automated item</P><P>generation (AIG), the process of using item models to generate new items with the help of</P><P>computer technology, was proposed to reduce reliance on human subject experts at each step of</P><P>the process. AIG has been used in test development for some time. Still, the use of machine</P><P>learning algorithms has introduced the potential to improve the efficiency and effectiveness of</P><P>the process greatly. The approach presented in this paper utilizes OpenAI’s latest1 transformer-</P><P>based language model, GPT-3, to generate reading passages. Existing reading passages were</P><P>used in carefully engineered prompts to ensure the AI-generated text has similar content and</P><P>structure to a fourth-grade reading passage. For each prompt, we generated multiple passages,</P><P>the final passage was selected according to the Lexile score agreement with the original passage.</P><P>In the final round, the selected passage went through a simple revision by a human editor to</P><P>ensure the text was free of any grammatical and factual errors. All AI-generated passages, along</P><P>with original passages were evaluated by human judges according to their coherence,</P><P>appropriateness to fourth graders, and readability.</P><P>1 GPT-3 was the latest release as of 2022.</P><H3>Introduction</H3><P>Technological innovations in all aspects of test development facilitate efficient test practices and</P><P>a more well-rounded information retrieval from the data compared to traditional paper-pencil</P><P>assessments. Consequentially, the integration of technology in computer-based assessments</P><P>(CBA) increases the demand for more frequent administration and rapid and efficient production</P><P>of high-quality content-specific innovative items. The greater selection of item types presented to</P><P>groups of examinees with high frequency requires a more streamlined item development process.</P><P>Conventional item development is among the most expensive, time-consuming, and labor-</P><P>intensive parts of assessment development because the process heavily depends on human</P><P>content specialists. Human subject experts write each item individually, then each item is</P><P>reviewed, edited, and revised by a group of experts until it meets predefined quality control</P><P>standards (Haladyna & Rodriguez, 2013). Therefore, the subjectivity of traditional item writing</P><P>is often compromi", "url": "https://arxiv.org/pdf/2304.04616v1", "pdf_url": "https://arxiv.org/pdf/2304.04616v1.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2304.04616", "title": "https://arxiv.org/pdf/2304.04616", "authors": [], "abstract": "<H1>Automated Reading Passage Generation with OpenAI’s Large Language Model</H1><H1>Ummugul Bezirhan1 & Matthias von Davier1</H1><P>1Boston College, Chestnut Hill, MA, USA  </P><P>Corresponding Author: Ummugul Bezirhan  TIMSS &PIRLS International Study Center, Boston College, 188 Beacon Street, Chestnut Hill, MA 02467, USA. Email: bezirhan@bc.edu</P><H3>Abstract</H3><P>The widespread usage of computer-based assessments and individualized learning platforms has</P><P>resulted in an increased demand for the rapid production of high-quality items. Automated item</P><P>generation (AIG), the process of using item models to generate new items with the help of</P><P>computer technology, was proposed to reduce reliance on human subject experts at each step of</P><P>the process. AIG has been used in test development for some time. Still, the use of machine</P><P>learning algorithms has introduced the potential to improve the efficiency and effectiveness of</P><P>the process greatly. The approach presented in this paper utilizes OpenAI’s latest1 transformer-</P><P>based language model, GPT-3, to generate reading passages. Existing reading passages were</P><P>used in carefully engineered prompts to ensure the AI-generated text has similar content and</P><P>structure to a fourth-grade reading passage. For each prompt, we generated multiple passages,</P><P>the final passage was selected according to the Lexile score agreement with the original passage.</P><P>In the final round, the selected passage went through a simple revision by a human editor to</P><P>ensure the text was free of any grammatical and factual errors. All AI-generated passages, along</P><P>with original passages were evaluated by human judges according to their coherence,</P><P>appropriateness to fourth graders, and readability.</P><P>1 GPT-3 was the latest release as of 2022.</P><H3>Introduction</H3><P>Technological innovations in all aspects of test development facilitate efficient test practices and</P><P>a more well-rounded information retrieval from the data compared to traditional paper-pencil</P><P>assessments. Consequentially, the integration of technology in computer-based assessments</P><P>(CBA) increases the demand for more frequent administration and rapid and efficient production</P><P>of high-quality content-specific innovative items. The greater selection of item types presented to</P><P>groups of examinees with high frequency requires a more streamlined item development process.</P><P>Conventional item development is among the most expensive, time-consuming, and labor-</P><P>intensive parts of assessment development because the process heavily depends on human</P><P>content specialists. Human subject experts write each item individually, then each item is</P><P>reviewed, edited, and revised by a group of experts until it meets predefined quality control</P><P>standards (Haladyna & Rodriguez, 2013). Therefore, the subjectivity of traditional item writing</P><P>is often compromi", "url": "https://arxiv.org/pdf/2304.04616", "pdf_url": "https://arxiv.org/pdf/2304.04616.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2304.04616", "title": "https://arxiv.org/pdf/2304.04616.pdf", "authors": [], "abstract": "<H1>Automated Reading Passage Generation with OpenAI’s Large Language Model</H1><H1>Ummugul Bezirhan1 & Matthias von Davier1</H1><P>1Boston College, Chestnut Hill, MA, USA  </P><P>Corresponding Author: Ummugul Bezirhan  TIMSS &PIRLS International Study Center, Boston College, 188 Beacon Street, Chestnut Hill, MA 02467, USA. Email: bezirhan@bc.edu</P><H3>Abstract</H3><P>The widespread usage of computer-based assessments and individualized learning platforms has</P><P>resulted in an increased demand for the rapid production of high-quality items. Automated item</P><P>generation (AIG), the process of using item models to generate new items with the help of</P><P>computer technology, was proposed to reduce reliance on human subject experts at each step of</P><P>the process. AIG has been used in test development for some time. Still, the use of machine</P><P>learning algorithms has introduced the potential to improve the efficiency and effectiveness of</P><P>the process greatly. The approach presented in this paper utilizes OpenAI’s latest1 transformer-</P><P>based language model, GPT-3, to generate reading passages. Existing reading passages were</P><P>used in carefully engineered prompts to ensure the AI-generated text has similar content and</P><P>structure to a fourth-grade reading passage. For each prompt, we generated multiple passages,</P><P>the final passage was selected according to the Lexile score agreement with the original passage.</P><P>In the final round, the selected passage went through a simple revision by a human editor to</P><P>ensure the text was free of any grammatical and factual errors. All AI-generated passages, along</P><P>with original passages were evaluated by human judges according to their coherence,</P><P>appropriateness to fourth graders, and readability.</P><P>1 GPT-3 was the latest release as of 2022.</P><H3>Introduction</H3><P>Technological innovations in all aspects of test development facilitate efficient test practices and</P><P>a more well-rounded information retrieval from the data compared to traditional paper-pencil</P><P>assessments. Consequentially, the integration of technology in computer-based assessments</P><P>(CBA) increases the demand for more frequent administration and rapid and efficient production</P><P>of high-quality content-specific innovative items. The greater selection of item types presented to</P><P>groups of examinees with high frequency requires a more streamlined item development process.</P><P>Conventional item development is among the most expensive, time-consuming, and labor-</P><P>intensive parts of assessment development because the process heavily depends on human</P><P>content specialists. Human subject experts write each item individually, then each item is</P><P>reviewed, edited, and revised by a group of experts until it meets predefined quality control</P><P>standards (Haladyna & Rodriguez, 2013). Therefore, the subjectivity of traditional item writing</P><P>is often compromi", "url": "https://arxiv.org/pdf/2304.04616.pdf", "pdf_url": "https://arxiv.org/pdf/2304.04616.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2502.12921", "title": "[2502.12921] Q-STRUM Debate: Query-Driven Contrastive Summarization for Recommendation Comparison", "authors": [], "abstract": "<H1>Computer Science > Computation and Language</H1><H1>Q-STRUM Debate: Query-Driven Contrastive Summarization for Recommendation Comparison</H1><P>Query-driven recommendation with unknown items poses a challenge for users to understand why certain items are appropriate for their needs. Query-driven Contrastive Summarization (QCS) is a methodology designed to address this issue by leveraging language-based item descriptions to clarify contrasts between them. However, existing state-of-the-art contrastive summarization methods such as STRUM-LLM fall short of this goal. To overcome these limitations, we introduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs debate-style prompting to generate focused and contrastive summarizations of item aspects relevant to a query. Leveraging modern large language models (LLMs) as powerful tools for generating debates, Q-STRUM Debate provides enhanced contrastive summaries. Experiments across three datasets demonstrate that Q-STRUM Debate yields significant performance improvements over existing methods on key contrastive summarization criteria, thus introducing a novel and performant debate prompting methodology for QCS.</P><TABLE><TR><TD>Subjects:</TD><TD>Computation and Language (cs.CL)</TD></TR><TR><TD>Cite as:</TD><TD>arXiv:2502.12921 [cs.CL]</TD></TR><TR><TD> </TD><TD>(or arXiv:2502.12921v2 [cs.CL] for this version)</TD></TR><TR><TD> </TD><TD>https://doi.org/10.48550/arXiv.2502.12921\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer (What is the Explorer?)</P><P>Connected Papers (What is Connected Papers?)</P><P>Litmaps (What is Litmaps?)</P><P>scite Smart Citations (What are Smart Citations?)</P>", "url": "https://arxiv.org/abs/2502.12921", "pdf_url": "https://arxiv.org/pdf/2502.12921.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2508.20559", "title": "Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search", "authors": [], "abstract": "<H1>Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search</H1><H6>Abstract.</H6><P>In the dynamic landscape of large-scale web search, Query-Driven Text Summarization (QDTS) aims to generate concise and informative summaries from textual documents based on a given query, which is essential for improving user engagement and facilitating rapid decision-making. Traditional extractive summarization models, based primarily on ranking candidate summary segments, have been the dominant approach in industrial applications. However, these approaches suffer from two key limitations: 1) The multi-stage pipeline often introduces cumulative information loss and architectural bottlenecks due to its weakest component; 2) Traditional models lack sufficient semantic understanding of both user queries and documents, particularly when dealing with complex search intents. In this study, we propose a novel framework to pioneer the application of generative models to address real-time QDTS in industrial web search. Our approach integrates large model distillation, supervised fine-tuning, direct preference optimization, and lookahead decoding to transform a lightweight model with only 0.1B parameters into a domain-specialized QDTS expert. Evaluated on multiple industry-relevant metrics, our model outperforms the production baseline and achieves a new state of the art. Furthermore, it demonstrates excellent deployment efficiency, requiring only 334 NVIDIA L20 GPUs to handle ~50,000 queries per second under 55 ms average latency per query.</P><H3>Generative Model, Query-Driven Text Summarization, Web Search</H3><H2>1. Introduction</H2><P>To enhance user experience and facilitate rapid decision-making, large-scale web search systems such as Google and Baidu are increasingly focusing on Query-Driven Text Summarization (QDTS) — a task aimed at generating concise and informative summaries tailored to user queries (Dang, 2006; Park and Ko, 2022; Deng et al., 2020; Peng et al., 2016; Yao et al., 2017).</P><P>As shown in Figure 1 (a), the traditional summary generation process typically follows a multi-stage pipeline: extracting content from the landing page, generating snippet candidates, applying pre-ranking, ranking, and finally post-processing. Each stage makes independent selection decisions based on partial information, forming a funnel-like filtering mechanism. This design often leads to the early elimination of relevant content, resulting in cumulative information loss throughout the pipeline (Xu and Cohen, 2023). Moreover, due to its fragmented structure, such a pipeline is inherently difficult to optimize for the global objective of user intent alignment. Existing summarization methods usually rely on computing the relevance score between the query and the document, selecting the most relevant sentence or paragraph as the summary (Bast and Celikik, 2014; Turpin et al., 2007; Mohamed et al., 2024). Many of these systems ad", "url": "https://arxiv.org/html/2508.20559", "pdf_url": "https://arxiv.org/pdf/2508.20559.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2508.20559v1", "title": "[2508.20559v1] Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search", "authors": [], "abstract": "<H1>Computer Science > Computation and Language</H1><H1>Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search</H1><P>In the dynamic landscape of large-scale web search, Query-Driven Text Summarization (QDTS) aims to generate concise and informative summaries from textual documents based on a given query, which is essential for improving user engagement and facilitating rapid decision-making. Traditional extractive summarization models, based primarily on ranking candidate summary segments, have been the dominant approach in industrial applications. However, these approaches suffer from two key limitations: 1) The multi-stage pipeline often introduces cumulative information loss and architectural bottlenecks due to its weakest component; 2) Traditional models lack sufficient semantic understanding of both user queries and documents, particularly when dealing with complex search intents. In this study, we propose a novel framework to pioneer the application of generative models to address real-time QDTS in industrial web search. Our approach integrates large model distillation, supervised fine-tuning, direct preference optimization, and lookahead decoding to transform a lightweight model with only 0.1B parameters into a domain-specialized QDTS expert. Evaluated on multiple industry-relevant metrics, our model outperforms the production baseline and achieves a new state of the art. Furthermore, it demonstrates excellent deployment efficiency, requiring only 334 NVIDIA L20 GPUs to handle \\textasciitilde50,000 queries per second under 55~ms average latency per query.</P><TABLE><TR><TD>Comments:</TD><TD>CIKM'25</TD></TR><TR><TD>Subjects:</TD><TD>Computation and Language (cs.CL); Information Retrieval (cs.IR)</TD></TR><TR><TD>Cite as:</TD><TD>arXiv:2508.20559 [cs.CL]</TD></TR><TR><TD> </TD><TD>(or arXiv:2508.20559v1 [cs.CL] for this version)</TD></TR><TR><TD> </TD><TD>https://doi.org/10.48550/arXiv.2508.20559\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer (What is the Explorer?)</P><P>Connected Papers (What is Connected Papers?)</P><P>Litmaps (What is Litmaps?)</P><P>scite Smart Citations (What are Smart Citations?)</P>", "url": "https://arxiv.org/abs/2508.20559v1", "pdf_url": "https://arxiv.org/pdf/2508.20559v1.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2408.00025v3", "title": "Need of AI in Modern Education: in the Eyes of Explainable AI (xAI)", "authors": ["Supriya Manna", "Niladri Sett"], "abstract": "Modern Education is not \\textit{Modern} without AI. However, AI's complex nature makes understanding and fixing problems challenging. Research worldwide shows that a parent's income greatly influences a child's education. This led us to explore how AI, especially complex models, makes important decisions using Explainable AI tools. Our research uncovered many complexities linked to parental income and offered reasonable explanations for these decisions. However, we also found biases in AI that go against what we want from AI in education: clear transparency and equal access for everyone. These biases can impact families and children's schooling, highlighting the need for better AI solutions that offer fair opportunities to all. This chapter tries to shed light on the complex ways AI operates, especially concerning biases. These are the foundational steps towards better educational policies, which include using AI in ways that are more reliable, accountable, and beneficial for everyone involved.", "url": "http://arxiv.org/abs/2408.00025v3", "pdf_url": "https://arxiv.org/pdf/2408.00025v3", "source": "arxiv", "published_date": "2024-07-31T08:11:33Z", "score_bm25": 0.0}, {"paper_id": "2504.08817v2", "title": "Exploring utilization of generative AI for research and education in data-driven materials science", "authors": ["Takahiro Misawa", "Ai Koizumi", "Ryo Tamura", "Kazuyoshi Yoshimi"], "abstract": "Generative AI has recently had a profound impact on various fields, including daily life, research, and education. To explore its efficient utilization in data-driven materials science, we organized a hackathon -- AIMHack2024 -- in July 2024. In this hackathon, researchers from fields such as materials science, information science, bioinformatics, and condensed matter physics worked together to explore how generative AI can facilitate research and education. Based on the results of the hackathon, this paper presents topics related to (1) conducting AI-assisted software trials, (2) building AI tutors for software, and (3) developing GUI applications for software. While generative AI continues to evolve rapidly, this paper provides an early record of its application in data-driven materials science and highlights strategies for integrating AI into research and education.", "url": "http://arxiv.org/abs/2504.08817v2", "pdf_url": "https://arxiv.org/pdf/2504.08817v2", "source": "arxiv", "published_date": "2025-04-09T11:15:21Z", "score_bm25": 0.0}, {"paper_id": "2504.16770v1", "title": "DeBiasMe: De-biasing Human-AI Interactions with Metacognitive AIED (AI in Education) Interventions", "authors": ["Chaeyeon Lim"], "abstract": "While generative artificial intelligence (Gen AI) increasingly transforms academic environments, a critical gap exists in understanding and mitigating human biases in AI interactions, such as anchoring and confirmation bias. This position paper advocates for metacognitive AI literacy interventions to help university students critically engage with AI and address biases across the Human-AI interaction workflows. The paper presents the importance of considering (1) metacognitive support with deliberate friction focusing on human bias; (2) bi-directional Human-AI interaction intervention addressing both input formulation and output interpretation; and (3) adaptive scaffolding that responds to diverse user engagement patterns. These frameworks are illustrated through ongoing work on \"DeBiasMe,\" AIED (AI in Education) interventions designed to enhance awareness of cognitive biases while empowering user agency in AI interactions. The paper invites multiple stakeholders to engage in discussions on design and evaluation methods for scaffolding mechanisms, bias visualization, and analysis frameworks. This position contributes to the emerging field of AI-augmented learning by emphasizing the critical role of metacognition in helping students navigate the complex interaction between human, statistical, and systemic biases in AI use while highlighting how cognitive adaptation to AI systems must be explicitly integrated into comprehensive AI literacy frameworks.", "url": "http://arxiv.org/abs/2504.16770v1", "pdf_url": "https://arxiv.org/pdf/2504.16770v1", "source": "arxiv", "published_date": "2025-04-23T14:41:31Z", "score_bm25": 0.0}, {"paper_id": "2401.15284v6", "title": "Beyond principlism: Practical strategies for ethical AI use in research practices", "authors": ["Zhicheng Lin"], "abstract": "The rapid adoption of generative artificial intelligence (AI) in scientific research, particularly large language models (LLMs), has outpaced the development of ethical guidelines, leading to a \"Triple-Too\" problem: too many high-level ethical initiatives, too abstract principles lacking contextual and practical relevance, and too much focus on restrictions and risks over benefits and utilities. Existing approaches--principlism (reliance on abstract ethical principles), formalism (rigid application of rules), and technological solutionism (overemphasis on technological fixes)--offer little practical guidance for addressing ethical challenges of AI in scientific research practices. To bridge the gap between abstract principles and day-to-day research practices, a user-centered, realism-inspired approach is proposed here. It outlines five specific goals for ethical AI use: 1) understanding model training and output, including bias mitigation strategies; 2) respecting privacy, confidentiality, and copyright; 3) avoiding plagiarism and policy violations; 4) applying AI beneficially compared to alternatives; and 5) using AI transparently and reproducibly. Each goal is accompanied by actionable strategies and realistic cases of misuse and corrective measures. I argue that ethical AI application requires evaluating its utility against existing alternatives rather than isolated performance metrics. Additionally, I propose documentation guidelines to enhance transparency and reproducibility in AI-assisted research. Moving forward, we need targeted professional development, training programs, and balanced enforcement mechanisms to promote responsible AI use while fostering innovation. By refining these ethical guidelines and adapting them to emerging AI capabilities, we can accelerate scientific progress without compromising research integrity.", "url": "http://arxiv.org/abs/2401.15284v6", "pdf_url": "https://arxiv.org/pdf/2401.15284v6", "source": "arxiv", "published_date": "2024-01-27T03:53:25Z", "score_bm25": 0.0}, {"paper_id": "2501.02842v1", "title": "Foundations of GenIR", "authors": ["Qingyao Ai", "Jingtao Zhan", "Yiqun Liu"], "abstract": "The chapter discusses the foundational impact of modern generative AI models on information access (IA) systems. In contrast to traditional AI, the large-scale training and superior data modeling of generative AI models enable them to produce high-quality, human-like responses, which brings brand new opportunities for the development of IA paradigms. In this chapter, we identify and introduce two of them in details, i.e., information generation and information synthesis. Information generation allows AI to create tailored content addressing user needs directly, enhancing user experience with immediate, relevant outputs. Information synthesis leverages the ability of generative AI to integrate and reorganize existing information, providing grounded responses and mitigating issues like model hallucination, which is particularly valuable in scenarios requiring precision and external knowledge. This chapter delves into the foundational aspects of generative models, including architecture, scaling, and training, and discusses their applications in multi-modal scenarios. Additionally, it examines the retrieval-augmented generation paradigm and other methods for corpus modeling and understanding, demonstrating how generative AI can enhance information access systems. It also summarizes potential challenges and fruitful directions for future studies.", "url": "http://arxiv.org/abs/2501.02842v1", "pdf_url": "https://arxiv.org/pdf/2501.02842v1", "source": "arxiv", "published_date": "2025-01-06T08:38:29Z", "score_bm25": 0.0}, {"paper_id": "2112.01298v2", "title": "Meaningful human control: actionable properties for AI system development", "authors": ["Luciano Cavalcante Siebert", "Maria Luce Lupetti", "Evgeni Aizenberg", "Niek Beckers", "Arkady Zgonnikov", "Herman Veluwenkamp", "David Abbink", "Elisa Giaccardi", "Geert-Jan Houben", "Catholijn M. Jonker", "Jeroen van den Hoven", "Deborah Forster", "Reginald L. Lagendijk"], "abstract": "How can humans remain in control of artificial intelligence (AI)-based systems designed to perform tasks autonomously? Such systems are increasingly ubiquitous, creating benefits - but also undesirable situations where moral responsibility for their actions cannot be properly attributed to any particular person or group. The concept of meaningful human control has been proposed to address responsibility gaps and mitigate them by establishing conditions that enable a proper attribution of responsibility for humans; however, clear requirements for researchers, designers, and engineers are yet inexistent, making the development of AI-based systems that remain under meaningful human control challenging. In this paper, we address the gap between philosophical theory and engineering practice by identifying, through an iterative process of abductive thinking, four actionable properties for AI-based systems under meaningful human control, which we discuss making use of two applications scenarios: automated vehicles and AI-based hiring. First, a system in which humans and AI algorithms interact should have an explicitly defined domain of morally loaded situations within which the system ought to operate. Second, humans and AI agents within the system should have appropriate and mutually compatible representations. Third, responsibility attributed to a human should be commensurate with that human's ability and authority to control the system. Fourth, there should be explicit links between the actions of the AI agents and actions of humans who are aware of their moral responsibility. We argue that these four properties will support practically-minded professionals to take concrete steps toward designing and engineering for AI systems that facilitate meaningful human control.", "url": "http://arxiv.org/abs/2112.01298v2", "pdf_url": "https://arxiv.org/pdf/2112.01298v2", "source": "arxiv", "published_date": "2021-11-25T11:05:37Z", "score_bm25": 0.0}, {"paper_id": "2211.12434v1", "title": "Expansive Participatory AI: Supporting Dreaming within Inequitable Institutions", "authors": ["Michael Alan Chang", "Shiran Dudy"], "abstract": "Participatory Artificial Intelligence (PAI) has recently gained interest by researchers as means to inform the design of technology through collective's lived experience. PAI has a greater promise than that of providing useful input to developers, it can contribute to the process of democratizing the design of technology, setting the focus on what should be designed. However, in the process of PAI there existing institutional power dynamics that hinder the realization of expansive dreams and aspirations of the relevant stakeholders. In this work we propose co-design principals for AI that address institutional power dynamics focusing on Participatory AI with youth.", "url": "http://arxiv.org/abs/2211.12434v1", "pdf_url": "https://arxiv.org/pdf/2211.12434v1", "source": "arxiv", "published_date": "2022-11-22T17:44:03Z", "score_bm25": 0.0}, {"paper_id": "2308.12400v1", "title": "Towards The Ultimate Brain: Exploring Scientific Discovery with ChatGPT AI", "authors": ["Gerardo Adesso"], "abstract": "This paper presents a novel approach to scientific discovery using an artificial intelligence (AI) environment known as ChatGPT, developed by OpenAI. This is the first paper entirely generated with outputs from ChatGPT. We demonstrate how ChatGPT can be instructed through a gamification environment to define and benchmark hypothetical physical theories. Through this environment, ChatGPT successfully simulates the creation of a new improved model, called GPT$^4$, which combines the concepts of GPT in AI (generative pretrained transformer) and GPT in physics (generalized probabilistic theory). We show that GPT$^4$ can use its built-in mathematical and statistical capabilities to simulate and analyze physical laws and phenomena. As a demonstration of its language capabilities, GPT$^4$ also generates a limerick about itself. Overall, our results demonstrate the promising potential for human-AI collaboration in scientific discovery, as well as the importance of designing systems that effectively integrate AI's capabilities with human intelligence.", "url": "http://arxiv.org/abs/2308.12400v1", "pdf_url": "https://arxiv.org/pdf/2308.12400v1", "source": "arxiv", "published_date": "2023-07-08T09:59:22Z", "score_bm25": 0.0}, {"paper_id": "2509.00961v1", "title": "Ultra Strong Machine Learning: Teaching Humans Active Learning Strategies via Automated AI Explanations", "authors": ["Lun Ai", "Johannes Langer", "Ute Schmid", "Stephen Muggleton"], "abstract": "Ultra Strong Machine Learning (USML) refers to symbolic learning systems that not only improve their own performance but can also teach their acquired knowledge to quantifiably improve human performance. In this work, we present LENS (Logic Programming Explanation via Neural Summarisation), a neuro-symbolic method that combines symbolic program synthesis with large language models (LLMs) to automate the explanation of machine-learned logic programs in natural language. LENS addresses a key limitation of prior USML approaches by replacing hand-crafted explanation templates with scalable automated generation. Through systematic evaluation using multiple LLM judges and human validation, we demonstrate that LENS generates superior explanations compared to direct LLM prompting and hand-crafted templates. To investigate whether LENS can teach transferable active learning strategies, we carried out a human learning experiment across three related domains. Our results show no significant human performance improvements, suggesting that comprehensive LLM responses may overwhelm users for simpler problems rather than providing learning support. Our work provides a solid foundation for building effective USML systems to support human learning. The source code is available on: https://github.com/lun-ai/LENS.git.", "url": "http://arxiv.org/abs/2509.00961v1", "pdf_url": "https://arxiv.org/pdf/2509.00961v1", "source": "arxiv", "published_date": "2025-08-31T19:04:31Z", "score_bm25": 0.0}, {"paper_id": "2310.10981v3", "title": "Instructive Dialogue Summarization with Query Aggregations", "authors": ["Bin Wang", "Zhengyuan Liu", "Nancy F. Chen"], "abstract": "Conventional dialogue summarization methods directly generate summaries and do not consider user's specific interests. This poses challenges in cases where the users are more focused on particular topics or aspects. With the advancement of instruction-finetuned language models, we introduce instruction-tuning to dialogues to expand the capability set of dialogue summarization models. To overcome the scarcity of instructive dialogue summarization data, we propose a three-step approach to synthesize high-quality query-based summarization triples. This process involves summary-anchored query generation, query filtering, and query-based summary generation. By training a unified model called InstructDS (Instructive Dialogue Summarization) on three summarization datasets with multi-purpose instructive triples, we expand the capability of dialogue summarization models. We evaluate our method on four datasets, including dialogue summarization and dialogue reading comprehension. Experimental results show that our approach outperforms the state-of-the-art models and even models with larger sizes. Additionally, our model exhibits higher generalizability and faithfulness, as confirmed by human subjective evaluations.", "url": "http://arxiv.org/abs/2310.10981v3", "pdf_url": "https://arxiv.org/pdf/2310.10981v3", "source": "arxiv", "published_date": "2023-10-17T04:03:00Z", "score_bm25": 0.0}, {"paper_id": "2407.10486", "title": "[2407.10486] IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused Summarization", "authors": [], "abstract": "<H1>Computer Science > Artificial Intelligence</H1><P>Query-focused summarization (QFS) aims to produce summaries that answer particular questions of interest, enabling greater user control and personalization. With the advent of large language models (LLMs), shows their impressive capability of textual understanding through large-scale pretraining, which implies the great potential of extractive snippet generation. In this paper, we systematically investigated two indispensable characteristics that the LLMs-based QFS models should be harnessed, Lengthy Document Summarization and Efficiently Fine-grained Query-LLM Alignment, respectively. Correspondingly, we propose two modules called Query-aware HyperExpert and Query-focused Infini-attention to access the aforementioned characteristics. These innovations pave the way for broader application and accessibility in the field of QFS technology. Extensive experiments conducted on existing QFS benchmarks indicate the effectiveness and generalizability of the proposed approach. Our code is publicly available at this https URL.</P><TABLE><TR><TD>Subjects:</TD><TD>Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</TD></TR><TR><TD>Cite as:</TD><TD>arXiv:2407.10486 [cs.AI]</TD></TR><TR><TD> </TD><TD>(or arXiv:2407.10486v2 [cs.AI] for this version)</TD></TR><TR><TD> </TD><TD>https://doi.org/10.48550/arXiv.2407.10486\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer (What is the Explorer?)</P><P>Connected Papers (What is Connected Papers?)</P><P>Litmaps (What is Litmaps?)</P><P>scite Smart Citations (What are Smart Citations?)</P>", "url": "https://arxiv.org/abs/2407.10486", "pdf_url": "https://arxiv.org/pdf/2407.10486.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2503.05935v1", "title": "[2503.05935v1] DETQUS: Decomposition-Enhanced Transformers for QUery-focused Summarization", "authors": [], "abstract": "<H1>Computer Science > Computation and Language</H1><H1>DETQUS: Decomposition-Enhanced Transformers for QUery-focused Summarization</H1><P>Query-focused tabular summarization is an emerging task in table-to-text generation that synthesizes a summary response from tabular data based on user queries. Traditional transformer-based approaches face challenges due to token limitations and the complexity of reasoning over large tables. To address these challenges, we introduce DETQUS (Decomposition-Enhanced Transformers for QUery-focused Summarization), a system designed to improve summarization accuracy by leveraging tabular decomposition alongside a fine-tuned encoder-decoder model. DETQUS employs a large language model to selectively reduce table size, retaining only query-relevant columns while preserving essential information. This strategy enables more efficient processing of large tables and enhances summary quality. Our approach, equipped with table-based QA model Omnitab, achieves a ROUGE-L score of 0.4437, outperforming the previous state-of-the-art REFACTOR model (ROUGE-L: 0.422). These results highlight DETQUS as a scalable and effective solution for query-focused tabular summarization, offering a structured alternative to more complex architectures.</P><TABLE><TR><TD>Comments:</TD><TD>12 pages, 2 figures, Accepted to NAACL 2025 main conference</TD></TR><TR><TD>Subjects:</TD><TD>Computation and Language (cs.CL)</TD></TR><TR><TD>Cite as:</TD><TD>arXiv:2503.05935 [cs.CL]</TD></TR><TR><TD>(or arXiv:2503.05935v1 [cs.CL] for this version)</TD></TR><TR><TD>https://doi.org/10.48550/arXiv.2503.05935\nFocus to learn more\narXiv-issued DOI via DataCite</TD></TR></TABLE><H2>Submission history</H2><H1>Bibliographic and Citation Tools</H1><P>Bibliographic Explorer (What is the Explorer?)</P><P>Connected Papers (What is Connected Papers?)</P><P>Litmaps (What is Litmaps?)</P><P>scite Smart Citations (What are Smart Citations?)</P>", "url": "https://arxiv.org/abs/2503.05935v1", "pdf_url": "https://arxiv.org/pdf/2503.05935v1.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2407.10486", "title": "arXiv:2407.10486v2 [cs.AI] 7 Jan 2025", "authors": [], "abstract": "<H1>arXiv:2407.10486v2 [cs.AI] 7 Jan 2025\nIDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language </H1><P>Jie Cao1, Dian Jiao 1, Qiang Yan 2, Wenqiao Zhang* 1, Siliang Tang 1, Yueting Zhuang 1</P><H3>1 Zhejiang University, 2 Tencent</H3><P>caojie@zju.edu.cn, jd_dcd@zju.edu.cn, rolanyan@qq.com, wenqiaozhang@zju.edu.cn, siliang@zju.edu.cn, yzhuang@zju.edu.cn</P><H3>Abstract</H3><P>Query-focused summarization (QFS) aims to produce summaries that answer particular ques-tions of interest, enabling greater user con-trol and personalization. With the advent of large language models (LLMs), shows their impressive capability of textual understanding through large-scale pretraining, which implies the great potential of extractive snippet gener-ation. In this paper, we systematically inves-tigated two indispensable characteristics that the LLMs-based QFS models should be har-nessed, Lengthy Document Summarization and Efficiently Fine-grained Query-LLM Alignment, respectively. Correspondingly, we propose two modules called Query-aware HyperExpert and Query-focused Infini-attention to access the aforementioned characteristics. These innova-tions pave the way for broader application and accessibility in the field of QFS technology. Extensive experiments conducted on existing QFS benchmarks indicate the effectiveness and generalizability of the proposed approach. Our code is publicly available at https://github. com/DCDmllm/IDEAL_Summary.</P><H3>1 Introduction</H3><P>In today’s world, where we are constantly bom-barded with vast amounts of text, the ability to efficiently summarize information has become cru-cial. Textual summarization (Gambhir and Gupta, 2017), the process of condensing a lengthy docu-ment into a succinct and digestible version while preserving the most crucial information, enabling quicker understanding and better management of information. As everyone has unique needs and interests in real-life scenarios, necessitating sum-marizers that succinctly address the information needed for a specific query by extracting essential information from documents, i.e., Query-Focused Summarization (QFS) (Daumé III, 2009). This</P><P>*Corresponding author</P><P>task involves analyzing the content to identify key themes and then highlighting these in the summary, which draws increasing attention in the textual sum-marization community. Traditionally, QFS has used extract-then-summarize methods (Zhong et al., 2021; Wang et al., 2022; Amar et al., 2023) that rely on the most relevant spans of text from a candidate document-based on the prevalence of query terms. Further onwards, the triumph of Large Language Models (LLMs) such as the GPT series (Achiam et al., 2023), LLaMA (Touvron et al., 2023) and other open-source LLMs showcased the power of large-scale pretraining in understanding, reasoning and generating intricate textual patterns, the great po-tential of LLMs offering new opportunities for QFS. However, there has been relatively littl", "url": "https://arxiv.org/pdf/2407.10486", "pdf_url": "https://arxiv.org/pdf/2407.10486.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}, {"paper_id": "2412.08970", "title": "arXiv:2412.08970v1 [cs.CL] 12 Dec 2024", "authors": [], "abstract": "<H1>arXiv:2412.08970v1 [cs.CL] 12 Dec 2024\nReasoning-Aware Query-Focu</H1><H3>Xiaochuan Lin, Xiangyong Chen</H3><P>Henan Polytechnic University</P><H3>Abstract</H3><P>Query-focused summarization over multi-table data is a challenging yet critical task for ex-tracting precise and relevant information from structured data. Existing methods often rely on complex preprocessing steps and struggle to generalize across domains or handle the logi-cal reasoning required for multi-table queries. In this paper, we propose QueryTableSumma-rizer++, an end-to-end generative framework leveraging large language models (LLMs) en-hanced with table-aware pre-training, query-aligned fine-tuning, and reinforcement learn-ing with feedback. Our method eliminates the need for intermediate serialization steps and directly generates query-relevant summaries. Experiments on a benchmark dataset demon-strate that QueryTableSummarizer++ signifi-cantly outperforms state-of-the-art baselines in terms of BLEU, ROUGE, and F1-score. Addi-tional analyses highlight its scalability, general-ization across domains, and robust handling of complex queries. Human evaluation further val-idates the superior quality and practical applica-bility of the generated summaries, establishing QueryTableSummarizer++ as a highly effective solution for multi-table summarization tasks.</P><H3>1 Introduction</H3><P>The rapid growth of structured data in the form of tables across diverse domains, such as finance, healthcare, and public datasets, has created an in-creasing demand for automated methods to extract meaningful insights from this data. Tables often contain vast amounts of information, and in many real-world scenarios, users seek answers to spe-cific queries that require synthesizing information from multiple tables. Query-focused summariza-tion over multi-table data thus emerges as a critical task, enabling users to receive concise and rele-vant summaries tailored to their information needs. This task goes beyond traditional table summariza-tion or information retrieval, as it involves complex reasoning across multiple tables to provide contex-tually relevant answers [Zhang et al., 2024, Zhou et al., 2023a].</P><P>Despite the significance of this task, existing approaches face notable challenges. Firstly, most prior methods rely on heuristic-based preprocess-ing steps, such as table serialization, which can lead to information loss or distortions in the data representation [Zhang et al., 2024]. These methods often treat tables as static entities and fail to cap-ture inter-table relationships effectively. Secondly, while some advanced models attempt to incorpo-rate structured information, they struggle with gen-eralization to diverse table formats and query con-texts. Finally, the absence of robust and scalable training frameworks for query-focused summariza-tion hinders progress in this field.</P><P>Motivated by these challenges, we propose a novel approach that leverages the power of large la", "url": "https://arxiv.org/pdf/2412.08970", "pdf_url": "https://arxiv.org/pdf/2412.08970.pdf", "source": "web", "published_date": "N/A", "score_bm25": 0.0}]}