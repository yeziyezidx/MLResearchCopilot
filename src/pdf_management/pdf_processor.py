"""
PDF å¤„ç†å™¨ - æ•´åˆä¸‹è½½ã€ç¼“å­˜ã€è§£æçš„å®Œæ•´å·¥ä½œæµ
"""
from typing import Dict, Optional, List
from .downloader import PDFDownloader
from .parser import PDFParser, ExtractedInfo
from .cache_manager import CacheManager


class PDFProcessor:
    """PDF å¤„ç†å™¨ - å®Œæ•´çš„ PDF ç®¡ç†å·¥ä½œæµ"""
    
    def __init__(
        self,
        cache_dir: str = "./cache/pdfs",
        llm_client=None,
        max_workers: int = 4,
    ):
        """
        åˆå§‹åŒ– PDF å¤„ç†å™¨
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•
            llm_client: LLM å®¢æˆ·ç«¯
            max_workers: æœ€å¤§å¹¶å‘ä¸‹è½½æ•°
        """
        self.downloader = PDFDownloader(cache_dir=cache_dir, max_workers=max_workers)
        self.parser = PDFParser(llm_client=llm_client)
        self.cache_manager = CacheManager(cache_dir=cache_dir)
        self.llm_client = llm_client
    
    def process_paper(
        self,
        paper: Dict,
        force_reprocess: bool = False,
    ) -> Dict:
        """
        å¤„ç†å•ç¯‡è®ºæ–‡ï¼šä¸‹è½½ -> è§£æ -> æå–
        
        Args:
            paper: è®ºæ–‡å­—å…¸
                {
                    "paper_id": "...",
                    "url": "...",
                    "title": "...",
                    ...
                }
            force_reprocess: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
            
        Returns:
            Dict: å¤„ç†ç»“æœ
                {
                    "success": bool,
                    "paper_id": str,
                    "pdf_path": str,
                    "extracted_info": ExtractedInfo,
                    "error": str,
                }
        """
        paper_id = paper.get("paper_id", "")
        url = paper.get("url", "")
        
        if not paper_id or not url:
            return {
                "success": False,
                "paper_id": paper_id,
                "pdf_path": None,
                "extracted_info": None,
                "error": "ç¼ºå°‘ paper_id æˆ– url",
            }
        
        try:
            # 1. æ£€æŸ¥ç¼“å­˜
            if not force_reprocess and self.cache_manager.has_cached_pdf(paper_id):
                cached_path = str(self.cache_manager.get_cache_path(paper_id))
                metadata = self.cache_manager.get_metadata(paper_id)
                
                # å¦‚æœå·²ç»æå–è¿‡ï¼Œç›´æ¥è¿”å›
                if metadata and metadata.status == "extracted":
                    return {
                        "success": True,
                        "paper_id": paper_id,
                        "pdf_path": cached_path,
                        "extracted_info": metadata.metadata.get("extracted_info"),
                        "error": None,
                    }
            
            # 2. ä¸‹è½½ PDF
            print(f"ğŸ“¥ ä¸‹è½½è®ºæ–‡: {paper_id}")
            download_result = self.downloader.download_paper(url)
            
            if not download_result["success"]:
                return {
                    "success": False,
                    "paper_id": paper_id,
                    "pdf_path": None,
                    "extracted_info": None,
                    "error": f"ä¸‹è½½å¤±è´¥: {download_result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                }
            
            pdf_path = download_result["file_path"]
            
            # 3. æ³¨å†Œåˆ°ç¼“å­˜
            self.cache_manager.register_pdf(
                paper_id=paper_id,
                url=url,
                file_path=pdf_path,
                file_size=download_result["file_size"],
            )
            self.cache_manager.update_metadata(paper_id, status="processing")
            
            # 4. è§£æ PDF
            print(f"ğŸ“– è§£æ PDF: {paper_id}")
            pages = self.parser.extract_text(pdf_path)
            
            if not pages:
                return {
                    "success": False,
                    "paper_id": paper_id,
                    "pdf_path": pdf_path,
                    "extracted_info": None,
                    "error": "æ— æ³•æå– PDF æ–‡æœ¬",
                }
            
            # 5. æå–å…³é”®ä¿¡æ¯
            print(f"ğŸ” æå–ä¿¡æ¯: {paper_id}")
            sections = self.parser.parse_structure(pages)
            extracted_info = self.parser.extract_key_information(pdf_path, sections)
            
            # 6. æå–å¼•ç”¨
            citations = self.parser.extract_citations(pages)
            
            # 7. æ›´æ–°ç¼“å­˜å…ƒæ•°æ®
            from datetime import datetime
            self.cache_manager.update_metadata(
                paper_id,
                status="extracted",
                metadata={
                    "extracted_info": self._convert_to_dict(extracted_info),
                    "citations": citations,
                    "page_count": len(pages),
                    "extracted_sections": len(sections),
                },
                extraction_date=datetime.now().isoformat(),
            )
            
            return {
                "success": True,
                "paper_id": paper_id,
                "pdf_path": pdf_path,
                "extracted_info": extracted_info,
                "citations": citations,
                "error": None,
            }
        
        except Exception as e:
            return {
                "success": False,
                "paper_id": paper_id,
                "pdf_path": None,
                "extracted_info": None,
                "error": f"å¤„ç†å¤±è´¥: {str(e)}",
            }
    
    def process_papers_batch(
        self,
        papers: List[Dict],
        force_reprocess: bool = False,
    ) -> Dict:
        """
        æ‰¹é‡å¤„ç†è®ºæ–‡
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            force_reprocess: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†
            
        Returns:
            Dict: å¤„ç†ç»“æœç»Ÿè®¡
        """
        results = {
            "total": len(papers),
            "successful": 0,
            "failed": 0,
            "papers": {},
        }
        
        for i, paper in enumerate(papers, 1):
            print(f"\n[{i}/{len(papers)}] å¤„ç†è®ºæ–‡...")
            result = self.process_paper(paper, force_reprocess)
            
            paper_id = paper.get("paper_id", "")
            results["papers"][paper_id] = result
            
            if result["success"]:
                results["successful"] += 1
            else:
                results["failed"] += 1
        
        return results
    
    @staticmethod
    def _convert_to_dict(obj):
        """å°†å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸ï¼ˆç”¨äº JSON åºåˆ—åŒ–ï¼‰"""
        if isinstance(obj, dict):
            return obj
        
        # å¦‚æœæ˜¯ dataclass
        if hasattr(obj, '__dataclass_fields__'):
            return {
                field: getattr(obj, field)
                for field in obj.__dataclass_fields__
            }
        
        return str(obj)
    
    def get_cache_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        return self.cache_manager.get_cache_stats()
    
    def cleanup_cache(self, max_age_days: int = 30, max_size_mb: int = 5000):
        """æ¸…ç†ç¼“å­˜"""
        self.cache_manager.cleanup(max_age_days, max_size_mb)
